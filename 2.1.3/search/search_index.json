{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"<p>A deep learning library focused on graph representation learning for real-world chemical tasks.</p> <ul> <li>\u2705 State-of-the-art GNN architectures.</li> <li>\ud83d\udc0d Extensible API: build your own GNN model and train it with ease.</li> <li>\u2697\ufe0f Rich featurization: powerful and flexible built-in molecular featurization.</li> <li>\ud83e\udde0 Pretrained models: for fast and easy inference or transfer learning.</li> <li>\u2b94 Read-to-use training loop based on Pytorch Lightning.</li> <li>\ud83d\udd0c Have a new dataset? Graphium provides a simple plug-and-play interface. Change the path, the name of the columns to predict, the atomic featurization, and you\u2019re ready to play!</li> </ul>"},{"location":"index.html#installation","title":"Installation","text":"<p>Use <code>mamba</code>:</p> <pre><code># Install Graphium\nmamba install -c conda-forge graphium\n</code></pre> <p>or pip:</p> <pre><code>pip install graphium\n</code></pre>"},{"location":"index.html#ipu-installation","title":"IPU installation","text":"<pre><code>mkdir ~/.venv                               # Create the folder for the environment\npython3 -m venv ~/.venv/graphium_ipu        # Create the environment\nsource ~/.venv/graphium_ipu/bin/activate    # Activate the environment\n\n# Install the PopTorch wheel\npip install PATH_TO_SDK/poptorch-3.2.0+109946_bb50ce43ab_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\n\n# Enable Poplar SDK (including Poplar and PopART)\nsource PATH_TO_SDK/enable\n\n# Install the IPU specific and graphium requirements\nPACKAGE_NAME=pytorch pip install -r requirements_ipu.txt\npip install -r lightning.txt\n\n# Install Graphium in dev mode\npip install --no-deps -e .\n</code></pre>"},{"location":"baseline.html","title":"ToyMix Baseline","text":"<p>From the paper to be released soon. Below, you can see the baselines for the <code>ToyMix</code> dataset, a multitasking dataset comprising of <code>QM9</code>, <code>Zinc12k</code> and <code>Tox21</code>. The datasets and their splits are available on this link.</p> <p>One can observe that the smaller datasets (<code>Zinc12k</code> and <code>Tox21</code>) beneficiate from adding another unrelated task (<code>QM9</code>), where the labels are computed from DFT simulations.</p> Dataset Model MAE \u2193 Pearson \u2191 R\u00b2 \u2191 MAE \u2193 Pearson \u2191 R\u00b2 \u2191 Single-Task Model Multi-Task Model QM9 GCN 0.102 \u00b1 0.0003 0.958 \u00b1 0.0007 0.920 \u00b1 0.002 0.119 \u00b1 0.01 0.955 \u00b1 0.001 0.915 \u00b1 0.001 GIN 0.0976 \u00b1 0.0006 0.959 \u00b1 0.0002 0.922 \u00b1 0.0004 0.117 \u00b1 0.01 0.950 \u00b1 0.002 0.908 \u00b1 0.003 GINE 0.0959 \u00b1 0.0002 0.955 \u00b1 0.002 0.918 \u00b1 0.004 0.102 \u00b1 0.01 0.956 \u00b1 0.0009 0.918 \u00b1 0.002 Zinc12k GCN 0.348 \u00b1 0.02 0.941 \u00b1 0.002 0.863 \u00b1 0.01 0.226 \u00b1 0.004 0.973 \u00b1 0.0005 0.940 \u00b1 0.003 GIN 0.303 \u00b1 0.007 0.950 \u00b1 0.003 0.889 \u00b1 0.003 0.189 \u00b1 0.004 0.978 \u00b1 0.006 0.953 \u00b1 0.002 GINE 0.266 \u00b1 0.02 0.961 \u00b1 0.003 0.915 \u00b1 0.01 0.147 \u00b1 0.009 0.987 \u00b1 0.001 0.971 \u00b1 0.003 BCE \u2193 AUROC \u2191 AP \u2191 BCE \u2193 AUROC \u2191 AP \u2191 Single-Task Model Multi-Task Model Tox21 GCN 0.202 \u00b1 0.005 0.773 \u00b1 0.006 0.334 \u00b1 0.03 0.176 \u00b1 0.001 0.850 \u00b1 0.006 0.446 \u00b1 0.01 GIN 0.200 \u00b1 0.002 0.789 \u00b1 0.009 0.350 \u00b1 0.01 0.176 \u00b1 0.001 0.841 \u00b1 0.005 0.454 \u00b1 0.009 GINE 0.201 \u00b1 0.007 0.783 \u00b1 0.007 0.345 \u00b1 0.02 0.177 \u00b1 0.0008 0.836 \u00b1 0.004 0.455 \u00b1 0.008"},{"location":"cli_references.html","title":"CLI Reference","text":"<p>This page provides documentation for our command line tools.</p>"},{"location":"cli_references.html#data","title":"data","text":"<p>Graphium datasets.</p> <p>Usage:</p> <pre><code>data [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli_references.html#download","title":"download","text":"<p>Download a Graphium dataset.</p> <p>Usage:</p> <pre><code>data download [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -n, --name TEXT    Name of the graphium dataset to download.  [required]\n  -o, --output TEXT  Where to download the Graphium dataset.  [required]\n  --progress         Whether to extract the dataset if it's a zip file.\n  --help             Show this message and exit.\n</code></pre>"},{"location":"cli_references.html#list","title":"list","text":"<p>List available Graphium dataset.</p> <p>Usage:</p> <pre><code>data list [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"contribute.html","title":"Contribute","text":"<p>The below documents the development lifecycle of Graphium.</p>"},{"location":"contribute.html#setup-a-dev-environment","title":"Setup a dev environment","text":"<pre><code>mamba env create -n graphium -f env.yml\nmamba activate graphium\n\npip install --no-deps -e .\n</code></pre>"},{"location":"contribute.html#run-tests","title":"Run tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"contribute.html#build-the-documentation","title":"Build the documentation","text":"<p>You can build and serve the documentation locally with:</p> <pre><code># Build and serve the doc\nmike serve\n</code></pre>"},{"location":"datasets.html","title":"Graphium Datasets","text":"<p>Graphium datasets are hosted at on Google Cloud Storage at <code>gs://graphium-public/datasets</code>. Graphium provides a convenient utility functions to list and download those datasets:</p> <pre><code>import graphium\n\ndataset_dir = \"/my/path\"\ndata_path = graphium.data.utils.download_graphium_dataset(\"graphium-zinc-micro\", output_path=dataset_dir)\nprint(data_path)\n# /my/path/graphium-zinc-micro\n</code></pre>"},{"location":"datasets.html#graphium-zinc-micro","title":"<code>graphium-zinc-micro</code>","text":"<p>ADD DESCRIPTION.</p> <ul> <li>Number of molecules: xxx</li> <li>Label columns: xxx</li> <li>Split informations.</li> </ul>"},{"location":"datasets.html#graphium-zinc-bench-gnn","title":"<code>graphium-zinc-bench-gnn</code>","text":"<p>ADD DESCRIPTION.</p> <ul> <li>Number of molecules: xxx</li> <li>Label columns: xxx- Split informations.</li> </ul>"},{"location":"design.html","title":"Graphium Library Design","text":""},{"location":"design.html#diagram-for-data-processing-in-molgps","title":"Diagram for data processing in molGPS.","text":""},{"location":"design.html#diagram-for-muti-task-network-in-molgps","title":"Diagram for Muti-task network in molGPS","text":"<p>Section from the previous README:</p>"},{"location":"design.html#data-setup","title":"Data setup","text":"<p>Then, you need to download the data needed to run the code. Right now, we have 2 sets of data folders, present in the link here.</p> <ul> <li>micro_ZINC (Synthetic dataset)</li> <li>A small subset (1000 mols) of the ZINC dataset</li> <li>The score is the subtraction of the computed LogP and the synthetic accessibility score SA</li> <li> <p>The data must be downloaded to the folder <code>./graphium/data/micro_ZINC/</code></p> </li> <li> <p>ZINC_bench_gnn (Synthetic dataset)</p> </li> <li>A subset (12000 mols) of the ZINC dataset</li> <li>The score is the subtraction of the computed LogP and the synthetic accessibility score SA</li> <li>These are the same 12k molecules provided by the Benchmarking-gnn repository.<ul> <li>We provide the pre-processed graphs in <code>ZINC_bench_gnn/data_from_benchmark</code></li> <li>We provide the SMILES in <code>ZINC_bench_gnn/smiles_score.csv</code>, with the train-val-test indexes in the file <code>indexes_train_val_test.csv</code>.</li> <li>The first 10k elements are the training set</li> <li>The next 1k the valid set</li> <li>The last 1k the test set.</li> </ul> </li> <li>The data must be downloaded to the folder <code>./graphium/data/ZINC_bench_gnn/</code></li> </ul> <p>Then, you can run the main file to make sure that all the dependancies are correctly installed and that the code works as expected.</p> <pre><code>python expts/main_micro_zinc.py\n</code></pre> <p>TODO: explain the internal design of Graphium so people can contribute to it more easily.</p>"},{"location":"design.html#structure-of-the-code","title":"Structure of the code","text":"<p>The code is built to rapidly iterate on different architectures of neural networks (NN) and graph neural networks (GNN) with Pytorch. The main focus of this work is molecular tasks, and we use the package <code>rdkit</code> to transform molecular SMILES into graphs.</p>"},{"location":"design.html#data_parser","title":"data_parser","text":"<p>This folder contains tools that allow tdependenciesrent kind of molecular data files, such as <code>.csv</code> or <code>.xlsx</code> with SMILES data, or <code>.sdf</code> files with 3D data.</p>"},{"location":"design.html#features","title":"features","text":"<p>Different utilities for molecules, such as Smiles to adjacency graph transformer, molecular property extraction, atomic properties, bond properties, ...</p> <p>The MolecularTransformer and AdjGraphTransformer come from ivbase, but I don't like them. I think we should replace them with something simpler and give more flexibility for combining one-hot embedding with physical properties embedding..</p>"},{"location":"design.html#trainer","title":"trainer","text":"<p>The trainer contains the interface to the <code>pytorch-lightning</code> library, with <code>PredictorModule</code> being the main class used for any NN model, either for regression or classification. It also contains some modifications to the logger from <code>pytorch-lightning</code> to enable more flexibility.</p>"},{"location":"design.html#utils","title":"utils","text":"<p>Any kind of utilities that can be used anywhere, including argument checkers and configuration loader</p>"},{"location":"design.html#visualization","title":"visualization","text":"<p>Plot visualization tools</p>"},{"location":"design.html#modifying-the-code","title":"Modifying the code","text":""},{"location":"design.html#adding-a-new-gnn-layer","title":"Adding a new GNN layer","text":"<p>Any new GNN layer must inherit from the class <code>graphium.nn.base_graph_layer.BaseGraphLayer</code> and be implemented in the folder <code>graphium/nn/pyg_layers</code>, imported in the file <code>graphium/nn/architectures.py</code>, and in the same file, added to the function <code>FeedForwardGraph._parse_gnn_layer</code>.</p> <p>To be used in the configuration file as a <code>graphium.model.layer_name</code>, it must also be implemented with some variable parameters in the file <code>expts/config_gnns.yaml</code>.</p>"},{"location":"design.html#adding-a-new-nn-architecture","title":"Adding a new NN architecture","text":"<p>All NN and GNN architectures compatible with the <code>pyg</code> library are provided in the file <code>graphium/nn/global_architectures.py</code>. When implementing a new architecture, it is highly recommended to inherit from <code>graphium.nn.architectures.FeedForwardNN</code> for regular neural networks, from <code>graphium.nn.global_architectures.FeedForwardGraph</code> for pyg neural network, or from any of their sub-classes.</p>"},{"location":"design.html#changing-the-predictormodule-and-loss-function","title":"Changing the PredictorModule and loss function","text":"<p>The <code>PredictorModule</code> is a general pytorch-lightning module that should work with any kind of <code>pytorch.nn.Module</code> or <code>pl.LightningModule</code>. The class defines a structure of including models, loss functions, batch sizes, collate functions, metrics...</p> <p>Some loss functions are already implemented in the PredictorModule, including <code>mse, bce, mae, cosine</code>, but some tasks will require more complex loss functions. One can add any new function in <code>graphium.trainer.predictor.PredictorModule._parse_loss_fun</code>.</p>"},{"location":"design.html#changing-the-metrics-used","title":"Changing the metrics used","text":"<p>!WARNING! The metrics implementation was done for pytorch-lightning v0.8. There has been major changes to how the metrics are used and defined, so the whole implementation must change.</p> <p>Our current code is compatible with the metrics defined by pytorch-lightning, which include a great set of metrics. We also added the PearsonR and SpearmanR as they are important correlation metrics. You can define any new metric in the file <code>graphium/trainer/metrics.py</code>. The metric must inherit from <code>TensorMetric</code> and must be added to the dictionary <code>graphium.trainer.metrics.METRICS_DICT</code>.</p> <p>To use the metric, you can easily add it's name from <code>METRICS_DICT</code> in the yaml configuration file, at the address <code>metrics.metrics_dict</code>. Each metric has an underlying dictionnary with a mandatory <code>threshold</code> key containing information on how to threshold the prediction/target before computing the metric. Any <code>kwargs</code> arguments of the metric must also be added.</p>"},{"location":"license.html","title":"License","text":"<pre><code>Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2021 Valence\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n</code></pre>"},{"location":"pretrained_models.html","title":"Graphium pretrained models","text":"<p>Graphium provides a set of pretrained models that you can use for inference or transfer learning. The models are available on Google Cloud Storage at <code>gs://graphium-public/pretrained-models</code>.</p> <p>You can load a pretrained models using the Graphium API:</p> <pre><code>import graphium\n\npredictor = graphium.trainer.PredictorModule.load_pretrained_models(\"graphium-zinc-micro-dummy-test\")\n</code></pre>"},{"location":"pretrained_models.html#graphium-zinc-micro-dummy-test","title":"<code>graphium-zinc-micro-dummy-test</code>","text":"<p>Dummy model used for testing purposes (probably to delete in the future).</p>"},{"location":"api/graphium.config.html","title":"graphium.config","text":"<p>helper functions to load and convert config files</p>"},{"location":"api/graphium.config.html#graphium.config._loader","title":"<code>graphium.config._loader</code>","text":""},{"location":"api/graphium.config.html#graphium.config._loader.get_accelerator","title":"<code>get_accelerator(config_acc)</code>","text":"<p>Get the accelerator from the config file, and ensure that they are consistant.</p>"},{"location":"api/graphium.config.html#graphium.config._loader.load_architecture","title":"<code>load_architecture(config, in_dims)</code>","text":"<p>Loading the architecture used for training.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[omegaconf.DictConfig, Dict[str, Any]]</code> <p>The config file, with key <code>architecture</code></p> required <code>in_dims</code> <code>Dict[str, int]</code> <p>Dictionary of the input dimensions for various</p> required <p>Returns:</p> Name Type Description <code>architecture</code> <code>Union[FullGraphMultiTaskNetwork, torch.nn.Module]</code> <p>The datamodule used to process and load the data</p>"},{"location":"api/graphium.config.html#graphium.config._loader.load_datamodule","title":"<code>load_datamodule(config, accelerator_type)</code>","text":"<p>Load the datamodule from the specified configurations at the key <code>datamodule: args</code>. If the accelerator is IPU, load the IPU options as well.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[omegaconf.DictConfig, Dict[str, Any]]</code> <p>The config file, with key <code>datamodule: args</code></p> required <code>accelerator_type</code> <code>str</code> <p>The accelerator type, e.g. \"cpu\", \"gpu\", \"ipu\"</p> required <p>Returns:</p> Name Type Description <code>datamodule</code> <code>BaseDataModule</code> <p>The datamodule used to process and load the data</p>"},{"location":"api/graphium.config.html#graphium.config._loader.load_metrics","title":"<code>load_metrics(config)</code>","text":"<p>Loading the metrics to be tracked.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[omegaconf.DictConfig, Dict[str, Any]]</code> <p>The config file, with key <code>metrics</code></p> required <p>Returns:</p> Name Type Description <code>metrics</code> <code>Dict[str, MetricWrapper]</code> <p>A dictionary of all the metrics</p>"},{"location":"api/graphium.config.html#graphium.config._loader.load_mup","title":"<code>load_mup(mup_base_path, predictor)</code>","text":"<p>Load the base shapes for the mup, based either on a <code>.ckpt</code> or <code>.yaml</code> file. If <code>.yaml</code>, it should be generated by <code>mup.save_base_shapes</code></p>"},{"location":"api/graphium.config.html#graphium.config._loader.load_predictor","title":"<code>load_predictor(config, model_class, model_kwargs, metrics, accelerator_type, task_norms=None)</code>","text":"<p>Defining the predictor module, which handles the training logic from <code>lightning.LighningModule</code></p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>Type[torch.nn.Module]</code> <p>The torch Module containing the main forward function</p> required <code>accelerator_type</code> <code>str</code> <p>The accelerator type, e.g. \"cpu\", \"gpu\", \"ipu\"</p> required <p>Returns:</p> Name Type Description <code>predictor</code> <code>PredictorModule</code> <p>The predictor module</p>"},{"location":"api/graphium.config.html#graphium.config._loader.load_trainer","title":"<code>load_trainer(config, run_name, accelerator_type, date_time_suffix='')</code>","text":"<p>Defining the pytorch-lightning Trainer module.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Union[omegaconf.DictConfig, Dict[str, Any]]</code> <p>The config file, with key <code>trainer</code></p> required <code>run_name</code> <code>str</code> <p>The name of the current run. To be used for logging.</p> required <code>accelerator_type</code> <code>str</code> <p>The accelerator type, e.g. \"cpu\", \"gpu\", \"ipu\"</p> required <code>date_time_suffix</code> <code>str</code> <p>The date and time of the current run. To be used for logging.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>trainer</code> <code>Trainer</code> <p>the trainer module</p>"},{"location":"api/graphium.config.html#graphium.config._loader.load_yaml_config","title":"<code>load_yaml_config(config_path, main_dir=None, unknown_args=None)</code>","text":"<p>Load a YAML config file and return it as a dictionary. Also returns the anchors <code>&amp;</code> and aliases <code>*</code> of the YAML file. Then, update the config with the unknown arguments. Finally, update the config with the config override file specified in <code>constants.config_override</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Union[str, os.PathLike]</code> <p>The path to the YAML config file</p> required <code>main_dir</code> <code>Optional[Union[str, os.PathLike]]</code> <p>The main directory of the project. If specified, the config override file will be loaded from this directory</p> <code>None</code> <code>unknown_args</code> <p>The unknown arguments to update the config with, taken from <code>argparse.parse_known_args</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>config</code> <code>Dict[str, Any]</code> <p>The config dictionary</p>"},{"location":"api/graphium.config.html#graphium.config._loader.merge_dicts","title":"<code>merge_dicts(dict_a, dict_b, previous_dict_path='', on_exist='raise')</code>","text":"<p>Recursively merges dict_b into dict_a. If a key is missing from dict_a, it is added from dict_b. If a key exists in both, an error is raised. <code>dict_a</code> is modified in-place.</p> <p>Parameters:</p> Name Type Description Default <code>dict_a</code> <code>Dict[str, Any]</code> <p>The dictionary to merge into. Modified in-place.</p> required <code>dict_b</code> <code>Dict[str, Any]</code> <p>The dictionary to merge from.</p> required <code>previous_dict_path</code> <code>str</code> <p>The key path of the parent dictionary,</p> <code>''</code> <code>on_exist</code> <code>str</code> <p>What to do if a key already exists in dict_a. Options are \"raise\", \"overwrite\", \"ignore\".</p> <code>'raise'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a key path already exists in dict_a.</p>"},{"location":"api/graphium.config.html#graphium.config._loader.save_params_to_wandb","title":"<code>save_params_to_wandb(logger, config, predictor, datamodule)</code>","text":"<p>Save a few stuff to weights-and-biases WandB</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The object used to log the training. Usually WandbLogger</p> required <code>config</code> <code>Union[omegaconf.DictConfig, Dict[str, Any]]</code> <p>The config file, with key <code>trainer</code></p> required <code>predictor</code> <code>PredictorModule</code> <p>The predictor used to handle the train/val/test steps logic</p> required <code>datamodule</code> <code>MultitaskFromSmilesDataModule</code> <p>The datamodule used to load the data into training</p> required"},{"location":"api/graphium.config.html#graphium.config.config_convert","title":"<code>graphium.config.config_convert</code>","text":""},{"location":"api/graphium.config.html#graphium.config.config_convert.recursive_config_reformating","title":"<code>recursive_config_reformating(configs)</code>","text":"<p>For a given configuration file, convert all <code>DictConfig</code> to <code>dict</code>, all <code>ListConfig</code> to <code>list</code>, and all <code>byte</code> to <code>str</code>.</p> <p>This helps avoid errors when dumping a yaml file.</p>"},{"location":"api/graphium.config.html#graphium.config._load","title":"<code>graphium.config._load</code>","text":""},{"location":"api/graphium.config.html#graphium.config._load.load_config","title":"<code>load_config(name)</code>","text":"<p>Load a default config file by its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the config to load.</p> required"},{"location":"api/graphium.data.html","title":"graphium.data","text":"<p>module for loading datasets and collating batches</p> Contents <ul> <li>Data Module</li> <li>Collate Module</li> <li>Util Functions</li> </ul>"},{"location":"api/graphium.data.html#data-module","title":"Data Module","text":""},{"location":"api/graphium.data.html#graphium.data.datamodule","title":"<code>graphium.data.datamodule</code>","text":""},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule","title":"<code>BaseDataModule</code>","text":"<p>         Bases: <code>lightning.LightningDataModule</code></p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.get_num_workers","title":"<code>get_num_workers</code>  <code>property</code>","text":"<p>get the number of workers to use</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.predict_ds","title":"<code>predict_ds</code>  <code>writable</code> <code>property</code>","text":"<p>Get the dataset used for the prediction</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.__init__","title":"<code>__init__(batch_size_training=16, batch_size_inference=16, batch_size_per_pack=None, num_workers=0, pin_memory=True, persistent_workers=False, collate_fn=None)</code>","text":"<p>base dataset module for all datasets (to be inherented)</p> <p>Parameters:</p> Name Type Description Default <code>batch_size_training</code> <code>int</code> <p>batch size for training</p> <code>16</code> <code>batch_size_inference</code> <code>int</code> <p>batch size for inference</p> <code>16</code> <code>num_workers</code> <code>int</code> <p>number of workers for data loading</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>whether to pin memory</p> <code>True</code> <code>persistent_workers</code> <code>bool</code> <p>whether to use persistent workers</p> <code>False</code> <code>collate_fn</code> <code>Optional[Callable]</code> <p>collate function for batching</p> <code>None</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.get_dataloader","title":"<code>get_dataloader(dataset, shuffle, stage)</code>","text":"<p>Get the dataloader for a given dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset from which to load the data</p> required <code>shuffle</code> <code>bool</code> <p>set to <code>True</code> to have the data reshuffled at every epoch.</p> required <code>stage</code> <code>RunningStage</code> <p>Whether in Training, Validating, Testing, Sanity-checking, Predicting, or Tuning phase.</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>The dataloader to sample from</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.get_dataloader_kwargs","title":"<code>get_dataloader_kwargs(stage, shuffle, **kwargs)</code>","text":"<p>Get the options for the dataloader depending on the current stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>RunningStage</code> <p>Whether in Training, Validating, Testing, Sanity-checking, Predicting, or Tuning phase.</p> required <code>shuffle</code> <code>bool</code> <p>set to <code>True</code> to have the data reshuffled at every epoch.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Arguments to pass to the <code>DataLoader</code> during initialization</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.get_max_num_edges_datamodule","title":"<code>get_max_num_edges_datamodule(stages=None)</code>","text":"<p>Get the maximum number of edges across all datasets from the datamodule</p> <p>Parameters:</p> Name Type Description Default <code>datamodule</code> <p>The datamodule from which to extract the maximum number of nodes</p> required <code>stages</code> <code>Optional[List[str]]</code> <p>The stages from which to extract the max num nodes. Possible values are [\"train\", \"val\", \"test\", \"predict\"]. If None, all stages are considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>max_num_edges</code> <code>int</code> <p>The maximum number of edges across all datasets from the datamodule</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.get_max_num_nodes_datamodule","title":"<code>get_max_num_nodes_datamodule(stages=None)</code>","text":"<p>Get the maximum number of nodes across all datasets from the datamodule</p> <p>Parameters:</p> Name Type Description Default <code>datamodule</code> <p>The datamodule from which to extract the maximum number of nodes</p> required <code>stages</code> <code>Optional[List[str]]</code> <p>The stages from which to extract the max num nodes. Possible values are [\"train\", \"val\", \"test\", \"predict\"]. If None, all stages are considered.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>max_num_nodes</code> <code>int</code> <p>The maximum number of nodes across all datasets from the datamodule</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.predict_dataloader","title":"<code>predict_dataloader(**kwargs)</code>","text":"<p>return the dataloader for prediction</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.test_dataloader","title":"<code>test_dataloader(**kwargs)</code>","text":"<p>return the test dataloader</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.train_dataloader","title":"<code>train_dataloader(**kwargs)</code>","text":"<p>return the training dataloader</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.BaseDataModule.val_dataloader","title":"<code>val_dataloader(**kwargs)</code>","text":"<p>return the validation dataloader</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.DatasetProcessingParams","title":"<code>DatasetProcessingParams</code>","text":""},{"location":"api/graphium.data.html#graphium.data.datamodule.DatasetProcessingParams.__init__","title":"<code>__init__(task_level=None, df=None, df_path=None, smiles_col=None, label_cols=None, weights_col=None, weights_type=None, idx_col=None, sample_size=None, split_val=0.2, split_test=0.2, seed=None, splits_path=None, split_names=['train', 'val', 'test'], label_normalization=None)</code>","text":"<p>object to store the parameters for the dataset processing</p> <p>Parameters:</p> Name Type Description Default <code>task_level</code> <code>str</code> <p>The task level, wether it is graph, node, edge or nodepair</p> <code>None</code> <code>df</code> <code>pd.DataFrame</code> <p>The dataframe containing the data</p> <code>None</code> <code>df_path</code> <code>Optional[Union[str, os.PathLike]]</code> <p>The path to the dataframe containing the data</p> <code>None</code> <code>smiles_col</code> <code>str</code> <p>The column name of the smiles</p> <code>None</code> <code>label_cols</code> <code>List[str]</code> <p>The column names of the labels</p> <code>None</code> <code>weights_col</code> <code>str</code> <p>The column name of the weights</p> <code>None</code> <code>weights_type</code> <code>str</code> <p>The type of weights</p> <code>None</code> <code>idx_col</code> <code>str</code> <p>The column name of the indices</p> <code>None</code> <code>sample_size</code> <code>Union[int, float, Type[None]]</code> <p>The size of the sample</p> <code>None</code> <code>split_val</code> <code>float</code> <p>The fraction of the data to use for validation</p> <code>0.2</code> <code>split_test</code> <code>float</code> <p>The fraction of the data to use for testing</p> <code>0.2</code> <code>seed</code> <code>int</code> <p>The seed to use for the splits and subsampling</p> <code>None</code> <code>splits_path</code> <code>Optional[Union[str, os.PathLike]]</code> <p>The path to the splits</p> <code>None</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.FakeDataModule","title":"<code>FakeDataModule</code>","text":"<p>         Bases: <code>MultitaskFromSmilesDataModule</code></p> <p>A fake datamodule that generates artificial data by mimicking the true data coming from the provided dataset. It is useful to test the speed and performance of the model on a dataset without having to featurize it and wait for the workers to load it.</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.FakeDataModule.generate_data","title":"<code>generate_data(label_cols, smiles_col)</code>","text":"<p>Returns:</p> Type Description <p>pd.DataFrame</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.FakeDataModule.get_fake_graph","title":"<code>get_fake_graph()</code>","text":"<p>Low memory footprint method to get the first datapoint DGL graph. The first 10 rows of the data are read in case the first one has a featurization error. If all 20 first element, then <code>None</code> is returned, otherwise the first graph to not fail is returned.</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.FakeDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Called only from a single process in distributed settings. Steps:</p> <ul> <li>If each cache is set and exists, reload from cache and return. Otherwise,</li> <li>For each single-task dataset:<ul> <li>Load its dataframe from a path (if provided)</li> <li>Subsample the dataframe</li> <li>Extract the smiles, labels from the dataframe</li> </ul> </li> <li>In the previous step, we were also able to get the unique smiles, which we use to compute the features</li> <li>For each single-task dataframe and associated data (smiles, labels, etc.):<ul> <li>Filter out the data corresponding to molecules which failed featurization.</li> <li>Create a corresponding SingletaskDataset</li> <li>Split the SingletaskDataset according to the task-specific splits for train, val and test</li> </ul> </li> </ul>"},{"location":"api/graphium.data.html#graphium.data.datamodule.FakeDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Prepare the torch dataset. Called on every GPUs. Setting state here is ok.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either 'fit', 'test', or None.</p> <code>None</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.GraphOGBDataModule","title":"<code>GraphOGBDataModule</code>","text":"<p>         Bases: <code>MultitaskFromSmilesDataModule</code></p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.GraphOGBDataModule.__init__","title":"<code>__init__(task_specific_args, cache_data_path=None, featurization=None, batch_size_training=16, batch_size_inference=16, batch_size_per_pack=None, num_workers=0, pin_memory=True, persistent_workers=False, featurization_n_jobs=-1, featurization_progress=False, featurization_backend='loky', collate_fn=None, prepare_dict_or_graph='pyg:graph', **kwargs)</code>","text":"<p>Load an OGB (Open-graph-benchmark) GraphProp dataset.</p> <p>Parameters:</p> Name Type Description Default <code>task_specific_args</code> <code>Dict[str, Dict[str, Any]]</code> <p>Arguments related to each task, with the task-name being the key, and the specific arguments being the values. The arguments must be a Dict containing the following keys:</p> <ul> <li>\"dataset_name\": Name of the OGB dataset to load. Examples of possible datasets are \"ogbg-molhiv\", \"ogbg-molpcba\", \"ogbg-moltox21\", \"ogbg-molfreesolv\".</li> <li>\"sample_size\": The number of molecules to sample from the dataset. Default=None, meaning that all molecules will be considered.</li> </ul> required <code>cache_data_path</code> <code>Optional[Union[str, os.PathLike]]</code> <p>path where to save or reload the cached data. The path can be remote (S3, GS, etc).</p> <code>None</code> <code>featurization</code> <code>Optional[Union[Dict[str, Any], omegaconf.DictConfig]]</code> <p>args to apply to the SMILES to Graph featurizer.</p> <code>None</code> <code>batch_size_training</code> <code>int</code> <p>batch size for training and val dataset.</p> <code>16</code> <code>batch_size_inference</code> <code>int</code> <p>batch size for test dataset.</p> <code>16</code> <code>num_workers</code> <code>int</code> <p>Number of workers for the dataloader. Use -1 to use all available cores.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin on paginated CPU memory for the dataloader.</p> <code>True</code> <code>featurization_n_jobs</code> <code>int</code> <p>Number of cores to use for the featurization.</p> <code>-1</code> <code>featurization_progress</code> <code>bool</code> <p>whether to show a progress bar during featurization.</p> <code>False</code> <code>featurization_backend</code> <code>str</code> <p>The backend to use for the molecular featurization.</p> <ul> <li>\"multiprocessing\": Found to cause less memory issues.</li> <li>\"loky\": joblib's Default. Found to cause memory leaks.</li> <li>\"threading\": Found to be slow.</li> </ul> <code>'loky'</code> <code>collate_fn</code> <code>Optional[Callable]</code> <p>A custom torch collate function. Default is to <code>graphium.data.graphium_collate_fn</code></p> <code>None</code> <code>sample_size</code> <ul> <li><code>int</code>: The maximum number of elements to take from the dataset.</li> <li><code>float</code>: Value between 0 and 1 representing the fraction of the dataset to consider</li> <li><code>None</code>: all elements are considered.</li> </ul> required"},{"location":"api/graphium.data.html#graphium.data.datamodule.GraphOGBDataModule.to_dict","title":"<code>to_dict()</code>","text":"<p>geenrate a dictionary representation of the class</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>dictionary representation of the class</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.IPUDataModuleModifier","title":"<code>IPUDataModuleModifier</code>","text":""},{"location":"api/graphium.data.html#graphium.data.datamodule.IPUDataModuleModifier.__init__","title":"<code>__init__(ipu_inference_opts=None, ipu_training_opts=None, ipu_dataloader_training_opts=None, ipu_dataloader_inference_opts=None, *args, **kwargs)</code>","text":"<p>wrapper functions from the a <code>DataModule</code> to support IPU and IPU options To be used in dual inheritance, for example: <pre><code>IPUDataModule(BaseDataModule, IPUDataModuleModifier):\n    def __init__(self, **kwargs):\n        BaseDataModule.__init__(self, **kwargs)\n        IPUDataModuleModifier.__init__(self, **kwargs)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>ipu_inference_opts</code> <code>Optional[poptorch.Options]</code> <p>Options for the IPU in inference mode. Ignore if not using IPUs</p> <code>None</code> <code>ipu_training_opts</code> <code>Optional[poptorch.Options]</code> <p>Options for the IPU in training mode. Ignore if not using IPUs</p> <code>None</code> <code>ipu_dataloader_kwargs_train_val</code> <p>Options for the dataloader for the IPU. Ignore if not using IPUs</p> required <code>ipu_dataloader_kwargs_test</code> <p>Options for the dataloader for the IPU. Ignore if not using IPUs</p> required <code>args</code> <p>Arguments for the <code>DataModule</code></p> <code>()</code> <code>kwargs</code> <p>Keyword arguments for the <code>DataModule</code></p> <code>{}</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule","title":"<code>MultitaskFromSmilesDataModule</code>","text":"<p>         Bases: <code>BaseDataModule</code>, <code>IPUDataModuleModifier</code></p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.in_dims","title":"<code>in_dims</code>  <code>property</code>","text":"<p>Return all input dimensions for the set of graphs. Including node/edge features, and raw positional encoding dimensions such eigval, eigvec, rwse and more</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.num_edge_feats","title":"<code>num_edge_feats</code>  <code>property</code>","text":"<p>Return the number of edge features in the first graph</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.num_node_feats","title":"<code>num_node_feats</code>  <code>property</code>","text":"<p>Return the number of node features in the first graph</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.__init__","title":"<code>__init__(task_specific_args, cache_data_path=None, processed_graph_data_path=None, featurization=None, batch_size_training=16, batch_size_inference=16, batch_size_per_pack=None, num_workers=0, pin_memory=True, persistent_workers=False, featurization_n_jobs=-1, featurization_progress=False, featurization_backend='loky', featurization_batch_size=1000, collate_fn=None, prepare_dict_or_graph='pyg:graph', **kwargs)</code>","text":"<p>only for parameters beginning with task_*, we have a dictionary where the key is the task name and the value is specified below.</p> <p>Parameters:</p> Name Type Description Default <code>task_df</code> <p>(value) a dataframe</p> required <code>task_df_path</code> <p>(value) a path to a dataframe to load (CSV file). <code>df</code> takes precedence over <code>df_path</code>.</p> required <code>task_smiles_col</code> <p>(value) Name of the SMILES column. If set to <code>None</code>, it will look for a column with the word \"smile\" (case insensitive) in it. If no such column is found, an error will be raised.</p> required <code>task_label_cols</code> <p>(value) Name of the columns to use as labels, with different options.</p> <ul> <li><code>list</code>: A list of all column names to use</li> <li><code>None</code>: All the columns are used except the SMILES one.</li> <li><code>str</code>: The name of the single column to use</li> <li><code>*str</code>: A string starting by a <code>*</code> means all columns whose name   ends with the specified <code>str</code></li> <li><code>str*</code>: A string ending by a <code>*</code> means all columns whose name   starts with the specified <code>str</code></li> </ul> required <code>task_weights_col</code> <p>(value) Name of the column to use as sample weights. If <code>None</code>, no weights are used. This parameter cannot be used together with <code>weights_type</code>.</p> required <code>task_weights_type</code> <p>(value) The type of weights to use. This parameter cannot be used together with <code>weights_col</code>. It only supports multi-label binary classification.</p> <p>Supported types:</p> <ul> <li><code>None</code>: No weights are used.</li> <li><code>\"sample_balanced\"</code>: A weight is assigned to each sample inversely     proportional to the number of positive value. If there are multiple     labels, the product of the weights is used.</li> <li><code>\"sample_label_balanced\"</code>: Similar to the <code>\"sample_balanced\"</code> weights,     but the weights are applied to each element individually, without     computing the product of the weights for a given sample.</li> </ul> required <code>task_idx_col</code> <p>(value) Name of the columns to use as indices. Unused if set to None.</p> required <code>task_sample_size</code> <p>(value)</p> <ul> <li><code>int</code>: The maximum number of elements to take from the dataset.</li> <li><code>float</code>: Value between 0 and 1 representing the fraction of the dataset to consider</li> <li><code>None</code>: all elements are considered.</li> </ul> required <code>task_split_val</code> <p>(value) Ratio for the validation split.</p> required <code>task_split_test</code> <p>(value) Ratio for the test split.</p> required <code>task_seed</code> <p>(value) Seed to use for the random split and subsampling. More complex splitting strategy should be implemented.</p> required <code>task_splits_path</code> <p>(value) A path a CSV file containing indices for the splits. The file must contains 3 columns \"train\", \"val\" and \"test\". It takes precedence over <code>split_val</code> and <code>split_test</code>.</p> required <code>cache_data_path</code> <code>Optional[Union[str, os.PathLike]]</code> <p>path where to save or reload the cached data. The path can be remote (S3, GS, etc).</p> <code>None</code> <code>featurization</code> <code>Optional[Union[Dict[str, Any], omegaconf.DictConfig]]</code> <p>args to apply to the SMILES to Graph featurizer.</p> <code>None</code> <code>batch_size_training</code> <code>int</code> <p>batch size for training and val dataset.</p> <code>16</code> <code>batch_size_inference</code> <code>int</code> <p>batch size for test dataset.</p> <code>16</code> <code>num_workers</code> <code>int</code> <p>Number of workers for the dataloader. Use -1 to use all available cores.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>Whether to pin on paginated CPU memory for the dataloader.</p> <code>True</code> <code>featurization_n_jobs</code> <code>int</code> <p>Number of cores to use for the featurization.</p> <code>-1</code> <code>featurization_progress</code> <code>bool</code> <p>whether to show a progress bar during featurization.</p> <code>False</code> <code>featurization_backend</code> <code>str</code> <p>The backend to use for the molecular featurization.</p> <ul> <li>\"multiprocessing\": Found to cause less memory issues.</li> <li>\"loky\": joblib's Default. Found to cause memory leaks.</li> <li>\"threading\": Found to be slow.</li> </ul> <code>'loky'</code> <code>featurization_batch_size</code> <code>int</code> <p>Batch size to use for the featurization.</p> <code>1000</code> <code>collate_fn</code> <code>Optional[Callable]</code> <p>A custom torch collate function. Default is to <code>graphium.data.graphium_collate_fn</code></p> <code>None</code> <code>prepare_dict_or_graph</code> <code>str</code> <p>Whether to preprocess all molecules as Graph dict or PyG graphs. Possible options:</p> <ul> <li>\"pyg:dict\": Process molecules as a <code>dict</code>. It's faster and requires less RAM during   pre-processing. It is slower during training with with <code>num_workers=0</code> since   pyg <code>Data</code> will be created during data-loading, but faster with large   <code>num_workers</code>, and less likely to cause memory issues with the parallelization.</li> <li>\"pyg:graph\": Process molecules as <code>pyg.data.Data</code>.</li> </ul> <code>'pyg:graph'</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of elements of the current DataModule, which is the combined size of all single-task datasets given.</p> <p>Returns:</p> Name Type Description <code>num_elements</code> <code>int</code> <p>Number of elements in the current DataModule</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.calculate_statistics","title":"<code>calculate_statistics(dataset, train=False)</code>","text":"<p>Calculate the statistics of the labels for each task, and overwrites the <code>self.task_norms</code> attribute.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Datasets.MultitaskDataset</code> <p>the dataset to calculate the statistics from</p> required <code>train</code> <code>bool</code> <p>whether the dataset is the training set</p> <code>False</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.get_data_cache_fullname","title":"<code>get_data_cache_fullname(compress=False)</code>","text":"<p>Create a hash for the dataset, and use it to generate a file name</p> <p>Parameters:</p> Name Type Description Default <code>compress</code> <code>bool</code> <p>Whether to compress the data</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>full path to the data cache file</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.get_data_hash","title":"<code>get_data_hash()</code>","text":"<p>Get a hash specific to a dataset and smiles_transformer. Useful to cache the pre-processed data.</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.get_dataloader","title":"<code>get_dataloader(dataset, shuffle, stage)</code>","text":"<p>Get the poptorch dataloader for a given dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset from which to load the data</p> required <code>shuffle</code> <code>bool</code> <p>set to <code>True</code> to have the data reshuffled at every epoch.</p> required <code>stage</code> <code>RunningStage</code> <p>Whether in Training, Validating, Testing, Sanity-checking, Predicting, or Tuning phase.</p> required <p>Returns:</p> Type Description <code>Union[DataLoader, poptorch.DataLoader]</code> <p>The poptorch dataloader to sample from</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.get_dataloader_kwargs","title":"<code>get_dataloader_kwargs(stage, shuffle, **kwargs)</code>","text":"<p>Get the options for the dataloader depending on the current stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>RunningStage</code> <p>Whether in Training, Validating, Testing, Sanity-checking, Predicting, or Tuning phase.</p> required <code>shuffle</code> <code>bool</code> <p>set to <code>True</code> to have the data reshuffled at every epoch.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Arguments to pass to the <code>DataLoader</code> during initialization</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.get_fake_graph","title":"<code>get_fake_graph()</code>","text":"<p>Low memory footprint method to get the featurization of a fake graph without reading the dataset. Useful for getting the number of node/edge features.</p> <p>Returns:</p> Name Type Description <code>graph</code> <p>A fake graph with the right featurization</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.get_label_statistics","title":"<code>get_label_statistics(data_path, data_hash, dataset, train=False)</code>","text":"<p>Get the label statistics from the dataset, and save them to file, if needed. <code>self.task_norms</code> will be modified in-place with the label statistics.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Union[str, os.PathLike]</code> <p>the path to save and load the label statistics to. If None, no saving and loading will be done.</p> required <code>data_hash</code> <code>str</code> <p>the hash of the dataset generated by <code>get_data_hash()</code></p> required <code>dataset</code> <code>Datasets.MultitaskDataset</code> <p>the dataset to calculate the statistics from</p> required <code>train</code> <code>bool</code> <p>whether the dataset is the training set</p> <code>False</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.get_subsets_of_datasets","title":"<code>get_subsets_of_datasets(single_task_datasets, task_train_indices, task_val_indices, task_test_indices)</code>","text":"<p>From a dictionary of datasets and their associated indices, subset the train/val/test sets</p> <p>Parameters:</p> Name Type Description Default <code>single_task_datasets</code> <code>Dict[str, Datasets.SingleTaskDataset]</code> <p>Dictionary of datasets</p> required <code>task_train_indices</code> <code>Dict[str, Iterable]</code> <p>Dictionary of train indices</p> required <code>task_val_indices</code> <code>Dict[str, Iterable]</code> <p>Dictionary of val indices</p> required <code>task_test_indices</code> <code>Dict[str, Iterable]</code> <p>Dictionary of test indices</p> required <p>Returns:</p> Name Type Description <code>train_singletask_datasets</code> <code>Subset</code> <p>Dictionary of train subsets</p> <code>val_singletask_datasets</code> <code>Subset</code> <p>Dictionary of val subsets</p> <code>test_singletask_datasets</code> <code>Subset</code> <p>Dictionary of test subsets</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.load_data_from_cache","title":"<code>load_data_from_cache(verbose=True, compress=False)</code>","text":"<p>Load the datasets from cache. First create a hash for the dataset, and verify if that hash is available at the path given by <code>self.cache_data_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Whether to print the progress</p> <code>True</code> <code>compress</code> <code>bool</code> <p>Whether to compress the data</p> <code>False</code> <p>Returns:</p> Name Type Description <code>cache_data_exists</code> <code>bool</code> <p>Whether the cache exists (if the hash matches) and the loading succeeded</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.normalize_label","title":"<code>normalize_label(dataset, stage)</code>","text":"<p>Normalize the labels in the dataset using the statistics in <code>self.task_norms</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Datasets.MultitaskDataset</code> <p>the dataset to normalize the labels from</p> required <p>Returns:</p> Type Description <code>Datasets.MultitaskDataset</code> <p>the dataset with normalized labels</p>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Called only from a single process in distributed settings. Steps:</p> <ul> <li>If each cache is set and exists, reload from cache and return. Otherwise,</li> <li>For each single-task dataset:<ul> <li>Load its dataframe from a path (if provided)</li> <li>Subsample the dataframe</li> <li>Extract the smiles, labels from the dataframe</li> </ul> </li> <li>In the previous step, we were also able to get the unique smiles, which we use to compute the features</li> <li>For each single-task dataframe and associated data (smiles, labels, etc.):<ul> <li>Filter out the data corresponding to molecules which failed featurization.</li> <li>Create a corresponding SingletaskDataset</li> <li>Split the SingletaskDataset according to the task-specific splits for train, val and test</li> </ul> </li> </ul>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.save_data_to_cache","title":"<code>save_data_to_cache(verbose=True, compress=False)</code>","text":"<p>Save the datasets from cache. First create a hash for the dataset, use it to generate a file name. Then save to the path given by <code>self.cache_data_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Whether to print the progress</p> <code>True</code> <code>compress</code> <code>bool</code> <p>Whether to compress the data</p> <code>False</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.setup","title":"<code>setup(stage=None, save_smiles_and_ids=False)</code>","text":"<p>Prepare the torch dataset. Called on every GPUs. Setting state here is ok.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either 'fit', 'test', or None.</p> <code>None</code>"},{"location":"api/graphium.data.html#graphium.data.datamodule.MultitaskFromSmilesDataModule.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns a dictionary representation of the current DataModule</p> <p>Returns:</p> Name Type Description <code>obj_repr</code> <code>Dict[str, Any]</code> <p>Dictionary representation of the current DataModule</p>"},{"location":"api/graphium.data.html#collate-module","title":"Collate Module","text":""},{"location":"api/graphium.data.html#graphium.data.collate","title":"<code>graphium.data.collate</code>","text":""},{"location":"api/graphium.data.html#graphium.data.collate.collage_pyg_graph","title":"<code>collage_pyg_graph(pyg_graphs, batch_size_per_pack=None)</code>","text":"<p>Function to collate pytorch geometric graphs. Convert all numpy types to torch Convert edge indices to int64</p> <p>Parameters:</p> Name Type Description Default <code>pyg_graphs</code> <code>Iterable[Union[Data, Dict]]</code> <p>Iterable of PyG graphs</p> required <code>batch_size_per_pack</code> <code>Optional[int]</code> <p>The number of graphs to pack together. This is useful for using packing with the Transformer,</p> <code>None</code>"},{"location":"api/graphium.data.html#graphium.data.collate.collate_labels","title":"<code>collate_labels(labels, labels_size_dict=None, labels_dtype_dict=None)</code>","text":"<p>Collate labels for multitask learning.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>List[Data]</code> <p>List of labels</p> required <code>labels_size_dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dict of the form Dict[tasks, sizes] which has task names as keys and the size of the label tensor as value. The size of the tensor corresponds to how many labels/values there are to predict for that task.</p> <code>None</code> <code>labels_dtype_dict</code> <code>Optional[Dict[str, Any]]</code> <p>(Note): This is an attribute of the <code>MultitaskDataset</code>. A dictionary of the form Dict[tasks, dtypes] which has task names as keys and the dtype of the label tensor as value. This is necessary to ensure the missing labels are added with NaNs of the right dtype</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the form Dict[tasks, labels] where tasks is the name of the task and labels</p> <p>is a tensor of shape (batch_size, *labels_size_dict[task]).</p>"},{"location":"api/graphium.data.html#graphium.data.collate.collate_pyg_graph_labels","title":"<code>collate_pyg_graph_labels(pyg_labels)</code>","text":"<p>Function to collate pytorch geometric labels. Convert all numpy types to torch</p> <p>Parameters:</p> Name Type Description Default <code>pyg_labels</code> <code>List[Data]</code> <p>Iterable of PyG label Data objects</p> required"},{"location":"api/graphium.data.html#graphium.data.collate.get_expected_label_size","title":"<code>get_expected_label_size(label_data, task, label_size)</code>","text":"<p>Determines expected label size based on the specfic graph properties and the number of targets in the task-dataset.</p>"},{"location":"api/graphium.data.html#graphium.data.collate.graphium_collate_fn","title":"<code>graphium_collate_fn(elements, labels_size_dict=None, labels_dtype_dict=None, mask_nan='raise', do_not_collate_keys=[], batch_size_per_pack=None)</code>","text":"<p>This collate function is identical to the default pytorch collate function but add support for <code>pyg.data.Data</code> to batch graphs.</p> <p>Beside pyg graph collate, other objects are processed the same way as the original torch collate function. See https://pytorch.org/docs/stable/data.html#dataloader-collate-fn for more details.</p> Note <p>If graphium needs to manipulate other tricky-to-batch objects. Support for them should be added to this single collate function.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>Union[List[Any], Dict[str, List[Any]]]</code> <p>The elements to batch. See <code>torch.utils.data.dataloader.default_collate</code>.</p> required <code>labels_size_dict</code> <code>Optional[Dict[str, Any]]</code> <p>(Note): This is an attribute of the <code>MultitaskDataset</code>. A dictionary of the form Dict[tasks, sizes] which has task names as keys and the size of the label tensor as value. The size of the tensor corresponds to how many labels/values there are to predict for that task.</p> <code>None</code> <code>labels_dtype_dict</code> <code>Optional[Dict[str, Any]]</code> <p>(Note): This is an attribute of the <code>MultitaskDataset</code>. A dictionary of the form Dict[tasks, dtypes] which has task names as keys and the dtype of the label tensor as value. This is necessary to ensure the missing labels are added with NaNs of the right dtype</p> <code>None</code> <code>mask_nan</code> <code>Union[str, float, Type[None]]</code> <p>Deal with the NaN/Inf when calling the function <code>make_pyg_graph</code>. Some values become <code>Inf</code> when changing data type. This allows to deal with that.</p> <ul> <li>\"raise\": Raise an error when there is a nan or inf in the featurization</li> <li>\"warn\": Raise a warning when there is a nan or inf in the featurization</li> <li>\"None\": DEFAULT. Don't do anything</li> <li>\"Floating value\": Replace nans or inf by the specified value</li> </ul> <code>'raise'</code> <code>do_not_batch_keys</code> <p>Keys to ignore for the collate</p> required <code>batch_size_per_pack</code> <code>Optional[int]</code> <p>The number of graphs to pack together. This is useful for using packing with the Transformer. If None, no packing is done. Otherwise, indices are generated to map the nodes to the pack they belong to under the key <code>\"pack_from_node_idx\"</code>, with an additional mask to indicate which nodes are from the same graph under the key <code>\"pack_attn_mask\"</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Any, Dict[str, Any]]</code> <p>The batched elements. See <code>torch.utils.data.dataloader.default_collate</code>.</p>"},{"location":"api/graphium.data.html#graphium.data.collate.pad_nodepairs","title":"<code>pad_nodepairs(pe, num_nodes, max_num_nodes_per_graph)</code>","text":"<p>This function zero-pads nodepair-level positional encodings to conform with the batching logic.</p> <p>Parameters:</p> Name Type Description Default <code>pe</code> <code>torch.Tensor, [num_nodes, num_nodes, num_feat]</code> <p>Nodepair pe</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes of processed graph</p> required <code>max_num_nodes_per_graph</code> <code>int</code> <p>Maximum number of nodes among graphs in current batch</p> required <p>Returns:</p> Name Type Description <code>padded_pe</code> <code>torch.Tensor, [num_nodes, max_num_nodes_per_graph, num_feat]</code> <p>padded nodepair pe tensor</p>"},{"location":"api/graphium.data.html#graphium.data.collate.pad_to_expected_label_size","title":"<code>pad_to_expected_label_size(labels, label_size)</code>","text":"<p>Determine difference of <code>labels</code> shape to expected shape <code>label_size</code> and pad with <code>torch.nan</code> accordingly.</p>"},{"location":"api/graphium.data.html#util-functions","title":"Util Functions","text":""},{"location":"api/graphium.data.html#graphium.data.utils","title":"<code>graphium.data.utils</code>","text":""},{"location":"api/graphium.data.html#graphium.data.utils.download_graphium_dataset","title":"<code>download_graphium_dataset(name, output_path, extract_zip=True, progress=False)</code>","text":"<p>Download a Graphium dataset to a specified location.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the Graphium dataset from <code>graphium.data.utils.get_graphium_datasets()</code>.</p> required <code>output_path</code> <code>str</code> <p>Directory path where to download the dataset to.</p> required <code>extract_zip</code> <code>bool</code> <p>Whether to extract the dataset if it's a zip file.</p> <code>True</code> <code>progress</code> <code>bool</code> <p>Whether to show a progress bar during download.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the downloaded dataset.</p>"},{"location":"api/graphium.data.html#graphium.data.utils.graphium_package_path","title":"<code>graphium_package_path(graphium_path)</code>","text":"<p>Return the path of a graphium file in the package.</p>"},{"location":"api/graphium.data.html#graphium.data.utils.list_graphium_datasets","title":"<code>list_graphium_datasets()</code>","text":"<p>List Graphium datasets available to download.</p> <p>Returns:</p> Name Type Description <code>set</code> <code>set</code> <p>A set of Graphium dataset names.</p>"},{"location":"api/graphium.data.html#graphium.data.utils.load_micro_zinc","title":"<code>load_micro_zinc()</code>","text":"<p>Return a dataframe of micro ZINC (1000 data points).</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: A dataframe of micro ZINC.</p>"},{"location":"api/graphium.data.html#graphium.data.utils.load_tiny_zinc","title":"<code>load_tiny_zinc()</code>","text":"<p>Return a dataframe of tiny ZINC (100 data points).</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: A dataframe of tiny ZINC.</p>"},{"location":"api/graphium.features.html","title":"graphium.features","text":"<p>Feature extraction and manipulation</p> Contents <ul> <li>Featurizer</li> <li>Positional Encoding</li> <li>Properties</li> <li>Spectral PE</li> <li>Random Walk PE</li> <li>NMP</li> </ul>"},{"location":"api/graphium.features.html#featurizer","title":"Featurizer","text":""},{"location":"api/graphium.features.html#graphium.features.featurizer","title":"<code>graphium.features.featurizer</code>","text":""},{"location":"api/graphium.features.html#graphium.features.featurizer.GraphDict","title":"<code>GraphDict</code>","text":"<p>         Bases: <code>dict</code></p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.GraphDict.__init__","title":"<code>__init__(dic)</code>","text":"<p>Store the parameters required to initialize a <code>pyg.data.Data</code>, but as a dictionary to reduce memory consumption.</p> <p>Possible keys for the dictionary:</p> <ul> <li> <p>adj: A sparse Tensor containing the adjacency matrix</p> </li> <li> <p>ndata: A dictionnary containing different keys and Tensors     associated to the node features.</p> </li> <li> <p>edata: A dictionnary containing different keys and Tensors     associated to the edge features.</p> </li> <li> <p>dtype: The dtype for the floating data.</p> </li> <li> <p>mask_nan:     Deal with molecules that fail a part of the featurization.     NaNs can happen when taking the of a noble gas,     or other properties that are not measured for specific atoms.</p> <ul> <li>\"raise\": Raise an error when there is a nan or inf in the featurization</li> <li>\"warn\": Raise a warning when there is a nan or inf in the featurization</li> <li>\"None\": DEFAULT. Don't do anything</li> <li>\"Floating value\": Replace nans or inf by the specified value</li> </ul> </li> </ul>"},{"location":"api/graphium.features.html#graphium.features.featurizer.GraphDict.make_pyg_graph","title":"<code>make_pyg_graph(**kwargs)</code>","text":"<p>Convert the current dictionary of parameters, containing an adjacency matrix with node/edge data into a <code>pyg.data.Data</code> of torch Tensors.</p> <p><code>**kwargs</code> can be used to overwrite any parameter from the current dictionary. See <code>GraphDict.__init__</code> for a list of parameters</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.get_estimated_bond_length","title":"<code>get_estimated_bond_length(bond, mol)</code>","text":"<p>Estimate the bond length between atoms by looking at the estimated atomic radius that depends both on the atom type and the bond type. The resulting bond-length is then the sum of the radius.</p> <p>Keep in mind that this function only provides an estimate of the bond length and not the true one based on a conformer. The vast majority od estimated bond lengths will have an error below 5% while some bonds can have an error up to 20%. This function is mostly useful when conformer generation fails for some molecules, or for increased computation speed.</p> <p>Parameters:</p> Name Type Description Default <code>bond</code> <code>Chem.rdchem.Bond</code> <p>The bond to measure its lenght</p> required <code>mol</code> <code>dm.Mol</code> <p>The molecule containing the bond (used to get neighbouring atoms)</p> required <p>Returns:</p> Name Type Description <code>bond_length</code> <code>float</code> <p>The bond length in Angstrom, typically a value around 1-2.</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.get_mol_atomic_features_float","title":"<code>get_mol_atomic_features_float(mol, property_list, offset_carbon=True, mask_nan='raise')</code>","text":"<p>Get a dictionary of floating-point arrays of atomic properties. To ensure all properties are at a similar scale, some of the properties are divided by a constant.</p> <p>There is also the possibility of offseting by the carbon value using the <code>offset_carbon</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>dm.Mol</code> <p>molecule from which to extract the properties</p> required <code>property_list</code> <code>Union[List[str], List[Callable]]</code> <p>A list of atomic properties to get from the molecule, such as 'atomic-number', 'mass', 'valence', 'degree', 'electronegativity'. Some elements are divided by a factor to avoid feature explosion.</p> <p>Accepted properties are:</p> <ul> <li>\"atomic-number\"</li> <li>\"mass\", \"weight\"</li> <li>\"valence\", \"total-valence\"</li> <li>\"implicit-valence\"</li> <li>\"hybridization\"</li> <li>\"chirality\"</li> <li>\"hybridization\"</li> <li>\"aromatic\"</li> <li>\"ring\", \"in-ring\"</li> <li>\"min-ring\"</li> <li>\"max-ring\"</li> <li>\"num-ring\"</li> <li>\"degree\"</li> <li>\"radical-electron\"</li> <li>\"formal-charge\"</li> <li>\"vdw-radius\"</li> <li>\"covalent-radius\"</li> <li>\"electronegativity\"</li> <li>\"ionization\", \"first-ionization\"</li> <li>\"melting-point\"</li> <li>\"metal\"</li> <li>\"single-bond\"</li> <li>\"aromatic-bond\"</li> <li>\"double-bond\"</li> <li>\"triple-bond\"</li> <li>\"is-carbon\"</li> <li>\"group\"</li> <li>\"period\"</li> </ul> required <code>offset_carbon</code> <code>bool</code> <p>Whether to subract the Carbon property from the desired atomic property. For example, if we want the mass of the Lithium (6.941), the mass of the Carbon (12.0107) will be subracted, resulting in a value of -5.0697</p> <code>True</code> <code>mask_nan</code> <code>Union[str, float, type(None)]</code> <p>Deal with molecules that fail a part of the featurization. NaNs can happen when taking the of a noble gas, or other properties that are not measured for specific atoms.</p> <ul> <li>\"raise\": Raise an error when there is a nan or inf in the featurization</li> <li>\"warn\": Raise a warning when there is a nan or inf in the featurization</li> <li>\"None\": DEFAULT. Don't do anything</li> <li>\"Floating value\": Replace nans or inf by the specified value</li> </ul> <code>'raise'</code> <p>Returns:</p> Name Type Description <code>prop_dict</code> <code>Dict[str, np.ndarray]</code> <p>A dictionnary where the element of <code>property_list</code> are the keys and the values are np.ndarray of shape (N,). N is the number of atoms in <code>mol</code>.</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.get_mol_atomic_features_onehot","title":"<code>get_mol_atomic_features_onehot(mol, property_list)</code>","text":"<p>Get the following set of features for any given atom</p> <ul> <li>One-hot representation of the atom</li> <li>One-hot representation of the atom degree</li> <li>One-hot representation of the atom implicit valence</li> <li>One-hot representation of the the atom hybridization</li> <li>Whether the atom is aromatic</li> <li>The atom's formal charge</li> <li>The atom's number of radical electrons</li> </ul> <p>Additionally, the following features can be set, depending on the value of input Parameters</p> <ul> <li>One-hot representation of the number of hydrogen atom in the the current atom neighborhood if <code>explicit_H</code> is false</li> <li>One-hot encoding of the atom chirality, and whether such configuration is even possible</li> </ul> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>dm.Mol</code> <p>molecule from which to extract the properties</p> required <code>property_list</code> <code>List[str]</code> <p>A list of integer atomic properties to get from the molecule. The integer values are converted to a one-hot vector. Callables are not supported by this function.</p> <p>Accepted properties are:</p> <ul> <li>\"atomic-number\"</li> <li>\"degree\"</li> <li>\"valence\", \"total-valence\"</li> <li>\"implicit-valence\"</li> <li>\"hybridization\"</li> <li>\"chirality\"</li> <li>\"phase\"</li> <li>\"type\"</li> <li>\"group\"</li> <li>\"period\"</li> </ul> required <p>Returns:</p> Name Type Description <code>prop_dict</code> <code>Dict[str, Tensor]</code> <p>A dictionnary where the element of <code>property_list</code> are the keys and the values are np.ndarray of shape (N, OH). N is the number of atoms in <code>mol</code> and OH the lenght of the one-hot encoding.</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.get_mol_conformer_features","title":"<code>get_mol_conformer_features(mol, property_list, mask_nan=None)</code>","text":"<p>obtain the conformer features of a molecule</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>dm.Mol</code> <p>molecule from which to extract the properties</p> required <code>property_list</code> <code>Union[List[str], List[Callable]]</code> <p>A list of conformer property to get from the molecule Accepted properties are: - \"positions_3d\"</p> required <p>Returns:</p> Name Type Description <code>prop_dict</code> <code>Dict[str, np.ndarray]</code> <p>a dictionary where the element of <code>property_list</code> are the keys</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.get_mol_edge_features","title":"<code>get_mol_edge_features(mol, property_list, mask_nan='raise')</code>","text":"<p>Get the following set of features for any given bond See <code>graphium.features.nmp</code> for allowed values in one hot encoding</p> <ul> <li>One-hot representation of the bond type. Note that you should not kekulize your     molecules, if you expect this to take aromatic bond into account.</li> <li>Bond stereo type, following CIP classification</li> <li>Whether the bond is conjugated</li> <li>Whether the bond is in a ring</li> </ul> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>dm.Mol</code> <p>rdkit.Chem.Molecule the molecule of interest</p> required <code>property_list</code> <code>List[str]</code> <p>A list of edge properties to return for the given molecule. Accepted properties are:</p> <ul> <li>\"bond-type-onehot\"</li> <li>\"bond-type-float\"</li> <li>\"stereo\"</li> <li>\"in-ring\"</li> <li>\"conjugated\"</li> <li>\"conformer-bond-length\" (might cause problems with complex molecules)</li> <li>\"estimated-bond-length\"</li> </ul> required <p>Returns:</p> Name Type Description <code>prop_dict</code> <code>Dict[str, np.ndarray]</code> <p>A dictionnary where the element of <code>property_list</code> are the keys and the values are np.ndarray of shape (N,). N is the number of atoms in <code>mol</code>.</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.get_simple_mol_conformer","title":"<code>get_simple_mol_conformer(mol)</code>","text":"<p>If the molecule has a conformer, then it will return the conformer at idx <code>0</code>. Otherwise, it generates a simple molecule conformer using <code>rdkit.Chem.rdDistGeom.EmbedMolecule</code> and returns it. This is meant to be used in simple functions like <code>GetBondLength</code>, not in functions requiring complex 3D structure.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>dm.Mol</code> <p>Rdkit Molecule</p> required <p>Returns:</p> Name Type Description <code>conf</code> <code>Union[Chem.rdchem.Conformer, None]</code> <p>A conformer of the molecule, or <code>None</code> if it fails</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.mol_to_adj_and_features","title":"<code>mol_to_adj_and_features(mol, atom_property_list_onehot=[], atom_property_list_float=[], conformer_property_list=[], edge_property_list=[], add_self_loop=False, explicit_H=False, use_bonds_weights=False, pos_encoding_as_features=None, dtype=np.float16, mask_nan='raise')</code>","text":"<p>Transforms a molecule into an adjacency matrix representing the molecular graph and a set of atom and bond features.</p> <p>It also returns the positional encodings associated to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Union[str, dm.Mol]</code> <p>The molecule to be converted</p> required <code>atom_property_list_onehot</code> <code>List[str]</code> <p>List of the properties used to get one-hot encoding of the atom type, such as the atom index represented as a one-hot vector. See function <code>get_mol_atomic_features_onehot</code></p> <code>[]</code> <code>atom_property_list_float</code> <code>List[Union[str, Callable]]</code> <p>List of the properties used to get floating-point encoding of the atom type, such as the atomic mass or electronegativity. See function <code>get_mol_atomic_features_float</code></p> <code>[]</code> <code>conformer_property_list</code> <code>List[str]</code> <p>list of properties used to encode the conformer information, outside of atom properties, currently support \"positions_3d\"</p> <code>[]</code> <code>edge_property_list</code> <code>List[str]</code> <p>List of the properties used to encode the edges, such as the edge type and the stereo type.</p> <code>[]</code> <code>add_self_loop</code> <code>bool</code> <p>Whether to add a value of <code>1</code> on the diagonal of the adjacency matrix.</p> <code>False</code> <code>explicit_H</code> <code>bool</code> <p>Whether to consider the Hydrogens explicitely. If <code>False</code>, the hydrogens are implicit.</p> <code>False</code> <code>use_bonds_weights</code> <code>bool</code> <p>Whether to use the floating-point value of the bonds in the adjacency matrix, such that single bonds are represented by 1, double bonds 2, triple 3, aromatic 1.5</p> <code>False</code> <code>pos_encoding_as_features</code> <code>Dict[str, Any]</code> <p>keyword arguments for function <code>graph_positional_encoder</code> to generate positional encoding for node features.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>The torch data type used to build the graph</p> <code>np.float16</code> <code>mask_nan</code> <code>Union[str, float, type(None)]</code> <p>Deal with molecules that fail a part of the featurization. NaNs can happen when taking the of a noble gas, or other properties that are not measured for specific atoms.</p> <ul> <li>\"raise\": Raise an error when there is a nan or inf in the featurization</li> <li>\"warn\": Raise a warning when there is a nan or inf in the featurization</li> <li>\"None\": DEFAULT. Don't do anything</li> <li>\"Floating value\": Replace nans or inf by the specified value</li> </ul> <code>'raise'</code> <p>Returns:</p> Name Type Description <code>adj</code> <code>Union[coo_matrix, Union[Tensor, None], Union[Tensor, None], Dict[str, Tensor], Union[Tensor, None], Dict[str, Tensor]]</code> <p>torch coo sparse adjacency matrix of the molecule</p> <code>ndata</code> <code>Union[coo_matrix, Union[Tensor, None], Union[Tensor, None], Dict[str, Tensor], Union[Tensor, None], Dict[str, Tensor]]</code> <p>Concatenated node data of the atoms, based on the properties from <code>atom_property_list_onehot</code> and <code>atom_property_list_float</code>. If no properties are given, it returns <code>None</code></p> <code>edata</code> <code>Union[coo_matrix, Union[Tensor, None], Union[Tensor, None], Dict[str, Tensor], Union[Tensor, None], Dict[str, Tensor]]</code> <p>Concatenated node edge of the molecule, based on the properties from <code>edge_property_list</code>. If no properties are given, it returns <code>None</code></p> <code>pe_dict</code> <code>Union[coo_matrix, Union[Tensor, None], Union[Tensor, None], Dict[str, Tensor], Union[Tensor, None], Dict[str, Tensor]]</code> <p>Dictionary of all positional encodings. Current supported keys:</p> <ul> <li> <p>\"pos_enc_feats_sign_flip\":     Node positional encoding that requires augmentation via sign-flip.     For example, eigenvectors of the Laplacian are ambiguous to the     sign and are returned here.</p> </li> <li> <p>\"pos_enc_feats_no_flip\":     Node positional encoding that requires does not use sign-flip.     For example, distance from centroid are returned here.</p> </li> <li> <p>\"rwse\":     Node structural encoding corresponding to the diagonal of the random     walk matrix</p> </li> </ul> <code>conf_dict</code> <code>Union[coo_matrix, Union[Tensor, None], Union[Tensor, None], Dict[str, Tensor], Union[Tensor, None], Dict[str, Tensor]]</code> <p>contains the 3d positions of a conformer of the molecule or 0s if none is found</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.mol_to_adjacency_matrix","title":"<code>mol_to_adjacency_matrix(mol, use_bonds_weights=False, add_self_loop=False, dtype=np.float32)</code>","text":"<p>Convert a molecule to a sparse adjacency matrix, as a torch Tensor. Instead of using the Rdkit <code>GetAdjacencyMatrix()</code> method, this method uses the bond ordering from the molecule object, which is the same as the bond ordering in the bond features.</p> Warning <p>Do not use <code>Tensor.coalesce()</code> on the returned adjacency matrix, as it will change the ordering of the bonds.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>dm.Mol</code> <p>A molecule in the form of a SMILES string or an RDKit molecule object.</p> required <code>use_bonds_weights</code> <code>bool</code> <p>If <code>True</code>, the adjacency matrix will contain the bond type as the value of the edge. If <code>False</code>, the adjacency matrix will contain <code>1</code> as the value of the edge.</p> <code>False</code> <code>add_self_loop</code> <code>bool</code> <p>If <code>True</code>, the adjacency matrix will contain a self-loop for each node.</p> <code>False</code> <code>dtype</code> <code>np.dtype</code> <p>The data type used to build the graph</p> <code>np.float32</code> <p>Returns:</p> Name Type Description <code>adj</code> <code>coo_matrix</code> <p>coo sparse adjacency matrix of the molecule</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.mol_to_graph_dict","title":"<code>mol_to_graph_dict(mol, atom_property_list_onehot=[], atom_property_list_float=[], conformer_property_list=[], edge_property_list=[], add_self_loop=False, explicit_H=False, use_bonds_weights=False, pos_encoding_as_features=None, dtype=np.float16, on_error='ignore', mask_nan='raise', max_num_atoms=None)</code>","text":"<p>Transforms a molecule into an adjacency matrix representing the molecular graph and a set of atom and bond features, and re-organizes them into a dictionary that allows to build a <code>pyg.data.Data</code> object.</p> <p>Compared to <code>mol_to_pyggraph</code>, this function does not build the graph directly, and is thus faster, less memory heavy, and compatible with other frameworks.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>dm.Mol</code> <p>The molecule to be converted</p> required <code>atom_property_list_onehot</code> <code>List[str]</code> <p>List of the properties used to get one-hot encoding of the atom type, such as the atom index represented as a one-hot vector. See function <code>get_mol_atomic_features_onehot</code></p> <code>[]</code> <code>atom_property_list_float</code> <code>List[Union[str, Callable]]</code> <p>List of the properties used to get floating-point encoding of the atom type, such as the atomic mass or electronegativity. See function <code>get_mol_atomic_features_float</code></p> <code>[]</code> <code>conformer_property_list</code> <code>List[str]</code> <p>list of properties used to encode the conformer information, outside of atom properties, currently support \"positions_3d\"</p> <code>[]</code> <code>edge_property_list</code> <code>List[str]</code> <p>List of the properties used to encode the edges, such as the edge type and the stereo type.</p> <code>[]</code> <code>add_self_loop</code> <code>bool</code> <p>Whether to add a value of <code>1</code> on the diagonal of the adjacency matrix.</p> <code>False</code> <code>explicit_H</code> <code>bool</code> <p>Whether to consider the Hydrogens explicitely. If <code>False</code>, the hydrogens are implicit.</p> <code>False</code> <code>use_bonds_weights</code> <code>bool</code> <p>Whether to use the floating-point value of the bonds in the adjacency matrix, such that single bonds are represented by 1, double bonds 2, triple 3, aromatic 1.5</p> <code>False</code> <code>pos_encoding_as_features</code> <code>Dict[str, Any]</code> <p>keyword arguments for function <code>graph_positional_encoder</code> to generate positional encoding for node features.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>The numpy data type used to build the graph</p> <code>np.float16</code> <code>on_error</code> <code>str</code> <p>What to do when the featurization fails. This can change the behavior of <code>mask_nan</code>.</p> <ul> <li>\"raise\": Raise an error</li> <li>\"warn\": Raise a warning and return a string of the error</li> <li>\"ignore\": Ignore the error and return a string of the error</li> </ul> <code>'ignore'</code> <code>mask_nan</code> <code>Union[str, float, type(None)]</code> <p>Deal with molecules that fail a part of the featurization. NaNs can happen when taking the of a noble gas, or other properties that are not measured for specific atoms.</p> <ul> <li>\"raise\": Raise an error when there is a nan or inf in the featurization</li> <li>\"warn\": Raise a warning when there is a nan or inf in the featurization</li> <li>\"None\": DEFAULT. Don't do anything</li> <li>\"Floating value\": Replace nans or inf by the specified value</li> </ul> <code>'raise'</code> <code>max_num_atoms</code> <code>Optional[int]</code> <p>Maximum number of atoms for a given molecule. If a molecule with more atoms is give, an error is raised, but catpured according to the rules of <code>on_error</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>graph_dict</code> <code>Union[GraphDict, str]</code> <p>A dictionary <code>GraphDict</code> containing the keys required to build a graph, and which can be used to build a PyG graph. If it fails to featurize the molecule, it returns a string with the error.</p> <ul> <li> <p>\"adj\": A sparse int-array containing the adjacency matrix</p> </li> <li> <p>\"data\": A dictionnary containing different keys and numpy   arrays associated to the (node, edge &amp; graph) features.</p> </li> <li> <p>\"dtype\": The numpy dtype for the floating data.</p> </li> </ul>"},{"location":"api/graphium.features.html#graphium.features.featurizer.mol_to_graph_signature","title":"<code>mol_to_graph_signature(featurizer_args=None)</code>","text":"<p>Get the default arguments of <code>mol_to_graph_dict</code> and update it with a provided dict of arguments in order to get a fulle signature of the featurizer args actually used for the features computation.</p> <p>Parameters:</p> Name Type Description Default <code>featurizer_args</code> <code>Dict[str, Any]</code> <p>A dictionary of featurizer arguments to update</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of featurizer arguments</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.mol_to_pyggraph","title":"<code>mol_to_pyggraph(mol, atom_property_list_onehot=[], atom_property_list_float=[], conformer_property_list=[], edge_property_list=[], add_self_loop=False, explicit_H=False, use_bonds_weights=False, pos_encoding_as_features=None, dtype=np.float16, on_error='ignore', mask_nan='raise', max_num_atoms=None)</code>","text":"<p>Transforms a molecule into an adjacency matrix representing the molecular graph and a set of atom and bond features.</p> <p>Then, the adjacency matrix and node/edge features are used to build a <code>pyg.data.Data</code> with pytorch Tensors.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>dm.Mol</code> <p>The molecule to be converted</p> required <code>atom_property_list_onehot</code> <code>List[str]</code> <p>List of the properties used to get one-hot encoding of the atom type, such as the atom index represented as a one-hot vector. See function <code>get_mol_atomic_features_onehot</code></p> <code>[]</code> <code>atom_property_list_float</code> <code>List[Union[str, Callable]]</code> <p>List of the properties used to get floating-point encoding of the atom type, such as the atomic mass or electronegativity. See function <code>get_mol_atomic_features_float</code></p> <code>[]</code> <code>conformer_property_list</code> <code>List[str]</code> <p>list of properties used to encode the conformer information, outside of atom properties, currently support \"positions_3d\"</p> <code>[]</code> <code>edge_property_list</code> <code>List[str]</code> <p>List of the properties used to encode the edges, such as the edge type and the stereo type.</p> <code>[]</code> <code>add_self_loop</code> <code>bool</code> <p>Whether to add a value of <code>1</code> on the diagonal of the adjacency matrix.</p> <code>False</code> <code>explicit_H</code> <code>bool</code> <p>Whether to consider the Hydrogens explicitely. If <code>False</code>, the hydrogens are implicit.</p> <code>False</code> <code>use_bonds_weights</code> <code>bool</code> <p>Whether to use the floating-point value of the bonds in the adjacency matrix, such that single bonds are represented by 1, double bonds 2, triple 3, aromatic 1.5</p> <code>False</code> <code>pos_encoding_as_features</code> <code>Dict[str, Any]</code> <p>keyword arguments for function <code>graph_positional_encoder</code> to generate positional encoding for node features.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>The numpy data type used to build the graph</p> <code>np.float16</code> <code>on_error</code> <code>str</code> <p>What to do when the featurization fails. This can change the behavior of <code>mask_nan</code>.</p> <ul> <li>\"raise\": Raise an error</li> <li>\"warn\": Raise a warning and return a string of the error</li> <li>\"ignore\": Ignore the error and return a string of the error</li> </ul> <code>'ignore'</code> <code>mask_nan</code> <code>Union[str, float, type(None)]</code> <p>Deal with molecules that fail a part of the featurization. NaNs can happen when taking the of a noble gas, or other properties that are not measured for specific atoms.</p> <ul> <li>\"raise\": Raise an error when there is a nan in the featurization</li> <li>\"warn\": Raise a warning when there is a nan in the featurization</li> <li>\"None\": DEFAULT. Don't do anything</li> <li>\"Floating value\": Replace nans by the specified value</li> </ul> <code>'raise'</code> <code>max_num_atoms</code> <code>Optional[int]</code> <p>Maximum number of atoms for a given molecule. If a molecule with more atoms is give, an error is raised, but catpured according to the rules of <code>on_error</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>graph</code> <code>Union[Data, str]</code> <p>Pyg graph, with <code>graph['feat']</code> corresponding to the concatenated node data from <code>atom_property_list_onehot</code> and <code>atom_property_list_float</code>, <code>graph['edge_feat']</code> corresponding to the concatenated edge data from <code>edge_property_list</code>. There are also additional entries for the positional encodings.</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.to_dense_array","title":"<code>to_dense_array(array, dtype=None)</code>","text":"<p>Assign the node data</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>np.ndarray</code> <p>The array to convert to dense</p> required <code>dtype</code> <code>str</code> <p>The dtype of the array</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The dense array</p>"},{"location":"api/graphium.features.html#graphium.features.featurizer.to_dense_tensor","title":"<code>to_dense_tensor(tensor, dtype=None)</code>","text":"<p>Assign the node data</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <p>The array to convert to dense</p> required <code>dtype</code> <code>str</code> <p>The dtype of the array</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The dense array</p>"},{"location":"api/graphium.features.html#positional-encoding","title":"Positional Encoding","text":""},{"location":"api/graphium.features.html#graphium.features.positional_encoding","title":"<code>graphium.features.positional_encoding</code>","text":""},{"location":"api/graphium.features.html#graphium.features.positional_encoding.get_all_positional_encodings","title":"<code>get_all_positional_encodings(adj, num_nodes, pos_kwargs=None)</code>","text":"<p>Get features positional encoding.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>[num_nodes, num_nodes]</code> <p>Adjacency matrix of the graph</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes in the graph</p> required <code>pos_encoding_as_features</code> <p>keyword arguments for function <code>graph_positional_encoder</code> to generate positional encoding for node features.</p> required <p>Returns:</p> Name Type Description <code>pe_dict</code> <code>Tuple[OrderedDict[str, np.ndarray]]</code> <p>Dictionary of positional and structural encodings</p>"},{"location":"api/graphium.features.html#graphium.features.positional_encoding.graph_positional_encoder","title":"<code>graph_positional_encoder(adj, num_nodes, pos_type=None, pos_level=None, pos_kwargs=None, cache=None)</code>","text":"<p>Get a positional encoding that depends on the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>[num_nodes, num_nodes]</code> <p>Adjacency matrix of the graph</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes in the graph</p> required <code>pos_type</code> <code>Optional[str]</code> <p>The type of positional encoding to use. If None, it must be provided by <code>pos_kwargs[\"pos_type\"]</code>. Supported types are: - laplacian_eigvec              \\ - laplacian_eigval               \\  -&gt; cache connected comps. &amp; eigendecomp. - rwse - electrostatic                 \\ - commute                        \\  -&gt; cache pinvL - graphormer</p> <code>None</code> <code>pos_level</code> <code>Optional[str]</code> <p>Positional level to output. If None, it must be provided by <code>pos_kwargs[\"pos_level\"]</code>. - node - edge - nodepair - graph</p> <code>None</code> <code>pos_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Extra keyword arguments for the positional encoding. Can include the keys pos_type and pos_level.</p> <code>None</code> <code>cache</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of cached objects</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pe</code> <code>Dict[str, np.ndarray]</code> <p>Positional or structural encoding</p> <code>cache</code> <code>Dict[str, Any]</code> <p>Updated dictionary of cached objects</p>"},{"location":"api/graphium.features.html#properties","title":"Properties","text":""},{"location":"api/graphium.features.html#graphium.features.properties","title":"<code>graphium.features.properties</code>","text":""},{"location":"api/graphium.features.html#graphium.features.properties.get_prop_or_none","title":"<code>get_prop_or_none(prop, n, *args, **kwargs)</code>","text":"<p>return properties. If error, return list of <code>None</code> with lenght <code>n</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>Callable</code> <p>The property to compute.</p> required <code>n</code> <code>int</code> <p>The number of elements in the property.</p> required <code>*args</code> <code>Union[dm.Mol, str]</code> <p>The arguments to pass to the property.</p> <code>()</code> <code>**kwargs</code> <code>Union[dm.Mol, str]</code> <p>The keyword arguments to pass to the property.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], List[None]]</code> <p>The property or a list of <code>None</code> with lenght <code>n</code>.</p>"},{"location":"api/graphium.features.html#graphium.features.properties.get_props_from_mol","title":"<code>get_props_from_mol(mol, properties='autocorr3d')</code>","text":"<p>Function to get a given set of desired properties from a molecule, and output a property list.</p> <p>Parameters:</p> Name Type Description Default <code>mol</code> <code>Union[dm.Mol, str]</code> <p>The molecule from which to compute the properties.</p> required <code>properties</code> <code>Union[List[str], str]</code> <p>The list of properties to compute for each molecule. It can be the following:</p> <ul> <li>'descriptors'</li> <li>'autocorr3d'</li> <li>'rdf'</li> <li>'morse'</li> <li>'whim'</li> <li>'all'</li> </ul> <code>'autocorr3d'</code> <p>Returns:</p> Name Type Description <code>props</code> <code>np.ndarray</code> <p>np.array(float) The array of properties for the desired molecule</p> <code>classes_start_idx</code> <code>np.ndarray</code> <p>list(int) The list of index specifying the start of each new class of descriptor or property. For example, if props has 20 elements, the first 5 are rotatable bonds, the next 8 are morse, and the rest are whim, then <code>classes_start_idx = [0, 5, 13]</code>. This will mainly be useful to normalize the features of each class.</p> <code>classes_names</code> <code>np.ndarray</code> <p>list(str) The name of the classes associated to each starting index. Will be usefull to understand what property is the network learning.</p>"},{"location":"api/graphium.features.html#spectral-pe","title":"Spectral PE","text":""},{"location":"api/graphium.features.html#graphium.features.spectral","title":"<code>graphium.features.spectral</code>","text":""},{"location":"api/graphium.features.html#graphium.features.spectral.compute_laplacian_pe","title":"<code>compute_laplacian_pe(adj, num_pos, cache, disconnected_comp=True, normalization='none')</code>","text":"<p>Compute the Laplacian eigenvalues and eigenvectors of the Laplacian of the graph.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>[num_nodes, num_nodes]</code> <p>Adjacency matrix of the graph</p> required <code>num_pos</code> <code>int</code> <p>Number of Laplacian eigenvectors to compute</p> required <code>cache</code> <code>Dict[str, Any]</code> <p>Dictionary of cached objects</p> required <code>disconnected_comp</code> <code>bool</code> <p>Whether to compute the eigenvectors for each connected component</p> <code>True</code> <code>normalization</code> <code>str</code> <p>Normalization to apply to the Laplacian</p> <code>'none'</code> <p>Returns:</p> Name Type Description <code>np.ndarray</code> <p>Two possible outputs: eigvals [num_nodes, num_pos]: Eigenvalues of the Laplacian repeated for each node.     This repetition is necessary in case of disconnected components, where     the eigenvalues of the Laplacian are not the same for each node. eigvecs [num_nodes, num_pos]: Eigenvectors of the Laplacian</p> <code>base_level</code> <code>str</code> <p>Indicator of the output pos_level (node, edge, nodepair, graph) -&gt; here node</p> <code>cache</code> <code>Dict[str, Any]</code> <p>Updated dictionary of cached objects</p>"},{"location":"api/graphium.features.html#graphium.features.spectral.normalize_matrix","title":"<code>normalize_matrix(matrix, degree_vector=None, normalization=None)</code>","text":"<p>Normalize a given matrix using its degree vector</p>"},{"location":"api/graphium.features.html#graphium.features.spectral.normalize_matrix--parameters","title":"Parameters","text":"<pre><code>matrix: torch.tensor(N, N) or scipy.sparse.spmatrix(N, N)\n    A square matrix representing either an Adjacency matrix or a Laplacian.\n\ndegree_vector: torch.tensor(N) or np.ndarray(N) or None\n    A vector representing the degree of ``matrix``.\n    ``None`` is only accepted if ``normalization==None``\n\nnormalization: str or None, Default='none'\n    Normalization to use on the eig_matrix\n\n    - 'none' or ``None``: no normalization\n\n    - 'sym': Symmetric normalization ``D^-0.5 L D^-0.5``\n\n    - 'inv': Inverse normalization ``D^-1 L``\n</code></pre>"},{"location":"api/graphium.features.html#graphium.features.spectral.normalize_matrix--returns","title":"Returns","text":"<pre><code>matrix: torch.tensor(N, N) or scipy.sparse.spmatrix(N, N)\n    The normalized matrix\n</code></pre>"},{"location":"api/graphium.features.html#random-walk-pe","title":"Random Walk PE","text":""},{"location":"api/graphium.features.html#graphium.features.rw","title":"<code>graphium.features.rw</code>","text":""},{"location":"api/graphium.features.html#graphium.features.rw.compute_rwse","title":"<code>compute_rwse(adj, ksteps, num_nodes, cache, pos_type='rw_return_probs' or 'rw_transition_probs', space_dim=0)</code>","text":"<p>Compute Random Walk Spectral Embedding (RWSE) for given list of K steps.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>[num_nodes, num_nodes]</code> <p>Adjacency matrix</p> required <code>ksteps</code> <code>Union[int, List[int]]</code> <p>List of numbers of steps for the random walks. If int, a list is generated from 1 to ksteps.</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes in the graph</p> required <code>cache</code> <code>Dict[str, Any]</code> <p>Dictionary of cached objects</p> required <code>pos_type</code> <code>str</code> <p>Desired output</p> <code>'rw_return_probs' or 'rw_transition_probs'</code> <code>space_dim</code> <code>int</code> <p>Estimated dimensionality of the space. Used to correct the random-walk diagonal by a factor <code>k^(space_dim/2)</code>. In euclidean space, this correction means that the height of the gaussian distribution stays almost constant across the number of steps, if <code>space_dim</code> is the dimension of the euclidean space.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>np.ndarray</code> <p>Two possible outputs: rw_return_probs [num_nodes, len(ksteps)]: Random-Walk k-step landing probabilities rw_transition_probs [num_nodes, num_nodes, len(ksteps)]:  Random-Walk k-step transition probabilities</p> <code>base_level</code> <code>str</code> <p>Indicator of the output pos_level (node, edge, nodepair, graph) -&gt; here either node or nodepair</p> <code>cache</code> <code>Dict[str, Any]</code> <p>Updated dictionary of cached objects</p>"},{"location":"api/graphium.features.html#graphium.features.rw.get_Pks","title":"<code>get_Pks(ksteps, edge_index, edge_weight=None, num_nodes=None, start_Pk=None, start_k=None)</code>","text":"<p>Compute Random Walk landing probabilities for given list of K steps.</p> <p>Parameters:</p> Name Type Description Default <code>ksteps</code> <code>List[int]</code> <p>List of numbers of k-steps for which to compute the RW landings</p> required <code>edge_index</code> <code>Tuple[torch.Tensor, torch.Tensor]</code> <p>PyG sparse representation of the graph</p> required <code>edge_weight</code> <code>Optional[torch.Tensor]</code> <p>Edge weights</p> <code>None</code> <code>num_nodes</code> <code>Optional[int]</code> <p>Number of nodes in the graph</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[int, np.ndarray]</code> <p>2D Tensor with shape (num_nodes, len(ksteps)) with RW landing probs</p>"},{"location":"api/graphium.features.html#nmp","title":"NMP","text":""},{"location":"api/graphium.features.html#graphium.features.nmp","title":"<code>graphium.features.nmp</code>","text":""},{"location":"api/graphium.features.html#graphium.features.nmp.float_or_none","title":"<code>float_or_none(string)</code>","text":"<p>check if a string can be converted to float, return none if it can't</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>str</p> required <p>Returns:</p> Name Type Description <code>val</code> <code>Union[float, None]</code> <p>float or None</p>"},{"location":"api/graphium.ipu.html","title":"graphium.ipu","text":"<p>Code for adapting to run on IPU</p> Contents <ul> <li>IPU Dataloader</li> <li>IPU Losses</li> <li>IPU Metrics</li> <li>IPU Simple Lightning</li> <li>IPU Utils</li> <li>IPU Wrapper</li> <li>To Dense Batch</li> </ul>"},{"location":"api/graphium.ipu.html#ipu-dataloader","title":"IPU Dataloader","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader","title":"<code>graphium.ipu.ipu_dataloader</code>","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader.CombinedBatchingCollator","title":"<code>CombinedBatchingCollator</code>","text":"Collator object that manages the combined batch size defined as <p>combined_batch_size = batch_size * device_iterations                      * replication_factor * gradient_accumulation</p> <p>This is intended to be used in combination with the poptorch.DataLoader</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader.CombinedBatchingCollator.__call__","title":"<code>__call__(batch)</code>","text":"<p>Stack tensors, batch the pyg graphs, and pad each tensor to be same size.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Dict[str, Union[Data, Dict[str, Tensor]]]]</code> <p>The batch of data, including pyg-graphs <code>Data</code> and labels <code>Dict[str, Tensor]</code> to be padded</p> required <p>Returns:</p> Name Type Description <code>out_batch</code> <code>Dict[str, Union[Batch, Dict[str, Tensor], Any]]</code> <p>A dictionary where the graphs are batched and the labels or other Tensors are stacked</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader.CombinedBatchingCollator.__init__","title":"<code>__init__(batch_size, max_num_nodes, max_num_edges, dataset_max_nodes_per_graph, dataset_max_edges_per_graph, collate_fn=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>mini batch size used by the model</p> required <code>max_num_nodes</code> <code>int</code> <p>Maximum number of nodes in the batched padded graph</p> required <code>max_num_edges</code> <code>int</code> <p>Maximum number of edges in the batched padded graph</p> required <code>dataset_max_nodes_per_graph</code> <code>int</code> <p>Maximum number of nodes per graph in the full dataset</p> required <code>dataset_max_edges_per_graph</code> <code>int</code> <p>Maximum number of edges per graph in the full dataset</p> required <code>collate_fn</code> <code>Optional[Callable]</code> <p>Function used to collate (or batch) the single data or graphs together</p> <code>None</code>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader.IPUDataloaderOptions","title":"<code>IPUDataloaderOptions</code>  <code>dataclass</code>","text":"<p>This data class stores the arguments necessary to instantiate a model for the Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <p>pytorch module used to create a model</p> required <code>model_kwargs</code> <p>Key-word arguments used to initialize the model from <code>model_class</code>.</p> required"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader.Pad","title":"<code>Pad</code>","text":"<p>         Bases: <code>BaseTransform</code></p> <p>Data transform that applies padding to enforce consistent tensor shapes.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader.Pad.__init__","title":"<code>__init__(max_num_nodes, dataset_max_nodes_per_graph, dataset_max_edges_per_graph, max_num_edges=None, node_value=0, edge_value=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>max_num_nodes</code> <code>int</code> <p>The maximum number of nodes for the total padded graph</p> required <code>dataset_max_nodes_per_graph</code> <p>the maximum number of nodes per graph in the dataset</p> required <code>dataset_max_edges_per_graph</code> <p>the maximum number of edges per graph in the dataset</p> required <code>max_num_edges</code> <code>Optional[int]</code> <p>The maximum number of edges for the total padded graph</p> <code>None</code> <code>node_value</code> <code>float</code> <p>Value to add to the node padding</p> <code>0</code> <code>edge_value</code> <code>float</code> <p>Value to add to the edge padding</p> <code>0</code>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader.Pad.validate","title":"<code>validate(data)</code>","text":"Validates that the input graph does not exceed the constraints that <ul> <li>the number of nodes must be &lt;= max_num_nodes</li> <li>the number of edges must be &lt;= max_num_edges</li> </ul> <p>Returns:</p> Type Description <p>Tuple containing the number nodes and the number of edges</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_dataloader.create_ipu_dataloader","title":"<code>create_ipu_dataloader(dataset, ipu_dataloader_options, ipu_options=None, batch_size=1, collate_fn=None, num_workers=0, **kwargs)</code>","text":"<p>Creates a poptorch.DataLoader for graph datasets Applies the mini-batching method of concatenating multiple graphs into a single graph with multiple disconnected subgraphs. See: https://pytorch-geometric.readthedocs.io/en/2.0.2/notes/batching.html</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The torch_geometric.data.Dataset instance from which to load the graph examples for the IPU.</p> required <code>ipu_dataloader_options</code> <code>IPUDataloaderOptions</code> <p>The options to initialize the Dataloader for IPU</p> required <code>ipu_options</code> <code>Optional[poptorch.Options]</code> <p>The poptorch.Options used by the poptorch.DataLoader. Will use the default options if not provided.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>How many graph examples to load in each batch (default: 1).</p> <code>1</code> <code>collate_fn</code> <p>The function used to collate batches</p> <code>None</code> <code>**kwargs</code> <code>optional</code> <p>Additional arguments of :class:<code>poptorch.DataLoader</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>poptorch.DataLoader</code> <p>The dataloader</p>"},{"location":"api/graphium.ipu.html#ipu-losses","title":"IPU Losses","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_losses","title":"<code>graphium.ipu.ipu_losses</code>","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_losses.BCELossIPU","title":"<code>BCELossIPU</code>","text":"<p>         Bases: <code>BCELoss</code></p> <p>A modified version of the <code>torch.nn.BCELoss</code> that can ignore NaNs by giving them a weight of <code>0</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_losses.HybridCELossIPU","title":"<code>HybridCELossIPU</code>","text":"<p>         Bases: <code>HybridCELoss</code></p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_losses.HybridCELossIPU.__init__","title":"<code>__init__(n_brackets, alpha=0.5)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n_brackets</code> <p>the number of brackets that will be used to group the regression targets. Expected to have the same size as the number of classes in the transformed regression task.</p> required"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_losses.HybridCELossIPU.forward","title":"<code>forward(input, target)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>(batch_size x n_classes) tensor of logits predicted for each bracket.</p> required <code>target</code> <code>Tensor</code> <p>(batch_size) or (batch_size, 1) tensor of target brackets in {0, 1, ..., self.n_brackets}.</p> required"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_losses.L1LossIPU","title":"<code>L1LossIPU</code>","text":"<p>         Bases: <code>L1Loss</code></p> <p>A modified version of the <code>torch.nn.L1Loss</code> that can ignore NaNs by giving them the same value for both <code>input</code> and <code>target</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_losses.MSELossIPU","title":"<code>MSELossIPU</code>","text":"<p>         Bases: <code>MSELoss</code></p> <p>A modified version of the <code>torch.nn.MSELoss</code> that can ignore NaNs by giving them the same value for both <code>input</code> and <code>target</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p>"},{"location":"api/graphium.ipu.html#ipu-metrics","title":"IPU Metrics","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics","title":"<code>graphium.ipu.ipu_metrics</code>","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor","title":"<code>NaNTensor</code>","text":"<p>         Bases: <code>Tensor</code></p> <p>Class to create and manage a NaN tensor along it's properties</p> <p>The goal of the class is to override the regular tensor such that the basic operations (sum, mean, max, etc) ignore the NaNs in the input. It also supports NaNs in integer tensors (as the lowest integer possible).</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.get_nans","title":"<code>get_nans: BoolTensor</code>  <code>property</code>","text":"<p>Gets the boolean Tensor containing the location of NaNs. In the case of an integer tensor, this returns where the tensor is equal to its minimal value In the case of a boolean tensor, this returns a Tensor filled with <code>False</code></p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Stupid fix that allows the code to work with <code>r2_score</code>, since it requires the size to be &gt; 2. But since <code>self.size</code> now returns a Tensor instead of a value, we check that all elements are &gt; 2.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.__torch_function__","title":"<code>__torch_function__(func, types, args=(), kwargs=None)</code>  <code>classmethod</code>","text":"<p>This torch_function implementation wraps subclasses such that methods called on subclasses return a subclass instance instead of a <code>torch.Tensor</code> instance.</p> <p>One corollary to this is that you need coverage for torch.Tensor methods if implementing torch_function for subclasses.</p> <p>Affects the call torch.sum() as to behave the same way as NaNTensor.sum()</p> <p>We recommend always calling <code>super().__torch_function__</code> as the base case when doing the above.</p> <p>While not mandatory, we recommend making <code>__torch_function__</code> a classmethod.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.argsort","title":"<code>argsort(dim=-1, descending=False)</code>","text":"<p>Return the indices that sort the tensor, while putting all the NaNs to the end of the sorting.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.max","title":"<code>max(*args, **kwargs)</code>","text":"<p>Returns the max vale of a tensor whitout NaNs</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.mean","title":"<code>mean(*args, **kwargs)</code>","text":"<p>Overloads the traditional mean to ignore the NaNs</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.min","title":"<code>min(*args, **kwargs)</code>","text":"<p>Returns the min vale of a tensor whitout NaNs</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.numel","title":"<code>numel()</code>","text":"<p>Returns the number of non-NaN elements.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.size","title":"<code>size(dim)</code>","text":"<p>Instead of returning the size, return the number of non-NaN elements in a specific dimension. Useful for the <code>r2_score</code> metric.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.NaNTensor.sum","title":"<code>sum(*args, **kwargs)</code>","text":"<p>Overloads the traditional sum to ignore the NaNs</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.accuracy_ipu","title":"<code>accuracy_ipu(preds, target, average='micro', mdmc_average='global', threshold=0.5, top_k=None, subset_accuracy=False, num_classes=None, multiclass=None, ignore_index=None)</code>","text":"<p>A modified version of the <code>torchmetrics.functional.accuracy</code> that can ignore NaNs by giving them the same value for both <code>preds</code> and <code>target</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predictions from model (probabilities, logits or labels)</p> required <code>target</code> <code>Tensor</code> <p>Ground truth labels</p> required <code>average</code> <code>Optional[str]</code> <p>Defines the reduction that is applied. Should be one of the following:</p> <ul> <li><code>'micro'</code> [default]: Calculate the metric globally, across all samples and classes.</li> <li><code>'macro'</code>: Calculate the metric for each class separately, and average the   metrics across classes (with equal weights for each class).</li> <li><code>'weighted'</code>: Calculate the metric for each class separately, and average the   metrics across classes, weighting each class by its support (<code>tp + fn</code>).</li> <li><code>'none'</code> or <code>None</code>: Calculate the metric for each class separately, and return   the metric for every class.</li> <li><code>'samples'</code>: Calculate the metric for each sample, and average the metrics   across samples (with equal weights for each sample).</li> </ul> <p>.. note:: What is considered a sample in the multi-dimensional multi-class case     depends on the value of <code>mdmc_average</code>.</p> <p>.. note:: If <code>'none'</code> and a given class doesn't occur in the <code>preds</code> or <code>target</code>,     the value for the class will be <code>nan</code>.</p> <code>'micro'</code> <code>mdmc_average</code> <code>Optional[str]</code> <p>Defines how averaging is done for multi-dimensional multi-class inputs (on top of the <code>average</code> parameter). Should be one of the following:</p> <ul> <li> <p><code>None</code> [default]: Should be left unchanged if your data is not multi-dimensional multi-class.</p> </li> <li> <p><code>'samplewise'</code>: In this case, the statistics are computed separately for each   sample on the <code>N</code> axis, and then averaged over samples.   The computation for each sample is done by treating the flattened extra axes <code>...</code>   (see :ref:<code>pages/classification:input types</code>) as the <code>N</code> dimension within the sample,   and computing the metric for the sample based on that.</p> </li> <li> <p><code>'global'</code>: In this case the <code>N</code> and <code>...</code> dimensions of the inputs   (see :ref:<code>pages/classification:input types</code>)   are flattened into a new <code>N_X</code> sample axis, i.e. the inputs are treated as if they   were <code>(N_X, C)</code>. From here on the <code>average</code> parameter applies as usual.</p> </li> </ul> <code>'global'</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes. Necessary for <code>'macro'</code>, <code>'weighted'</code> and <code>None</code> average methods.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.</p> <code>0.5</code> <code>top_k</code> <code>Optional[int]</code> <p>Number of the highest probability or logit score predictions considered finding the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value (<code>None</code>) will be interpreted as 1 for these inputs.</p> <p>Should be left at default (<code>None</code>) for all other types of inputs.</p> <code>None</code> <code>multiclass</code> <code>Optional[bool]</code> <p>Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref:<code>documentation section &lt;pages/classification:using the multiclass parameter&gt;</code> for a more detailed explanation and examples.</p> <code>None</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and <code>average=None</code> or <code>'none'</code>, the score for the ignored class will be returned as <code>nan</code>.</p> <code>None</code> <code>subset_accuracy</code> <code>bool</code> <p>Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types).</p> <ul> <li> <p>For multi-label inputs, if the parameter is set to <code>True</code>, then all labels for   each sample must be correctly predicted for the sample to count as correct. If it   is set to <code>False</code>, then all labels are counted separately - this is equivalent to   flattening inputs beforehand (i.e. <code>preds = preds.flatten()</code> and same for <code>target</code>).</p> </li> <li> <p>For multi-dimensional multi-class inputs, if the parameter is set to <code>True</code>, then all   sub-sample (on the extra axis) must be correct for the sample to be counted as correct.   If it is set to <code>False</code>, then all sub-samples are counter separately - this is equivalent,   in the case of label predictions, to flattening the inputs beforehand (i.e.   <code>preds = preds.flatten()</code> and same for <code>target</code>). Note that the <code>top_k</code> parameter   still applies in both cases, if set.</p> </li> </ul> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>top_k</code> parameter is set for <code>multi-label</code> inputs.</p> <code>ValueError</code> <p>If <code>average</code> is none of <code>\"micro\"</code>, <code>\"macro\"</code>, <code>\"weighted\"</code>, <code>\"samples\"</code>, <code>\"none\"</code>, <code>None</code>.</p> <code>ValueError</code> <p>If <code>mdmc_average</code> is not one of <code>None</code>, <code>\"samplewise\"</code>, <code>\"global\"</code>.</p> <code>ValueError</code> <p>If <code>average</code> is set but <code>num_classes</code> is not provided.</p> <code>ValueError</code> <p>If <code>num_classes</code> is set and <code>ignore_index</code> is not in the range <code>[0, num_classes)</code>.</p> <code>ValueError</code> <p>If <code>top_k</code> is not an <code>integer</code> larger than <code>0</code>.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.auroc_ipu","title":"<code>auroc_ipu(preds, target, num_classes=None, task=None, pos_label=None, average='macro', max_fpr=None, sample_weights=None)</code>","text":"<p>A modified version of the <code>torchmetrics.functional.auroc</code> that can ignore NaNs by giving them the same value for both <code>preds</code> and <code>target</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.average_precision_ipu","title":"<code>average_precision_ipu(preds, target, num_classes=None, task=None, ignore_index=None, pos_label=None, average='macro', sample_weights=None)</code>","text":"<p>A modified version of the <code>torchmetrics.functional.average_precision</code> that can ignore NaNs by giving them the same value for both <code>preds</code> and <code>target</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.f1_score_ipu","title":"<code>f1_score_ipu(preds, target, beta=1.0, average='micro', mdmc_average=None, ignore_index=None, num_classes=None, threshold=0.5, top_k=None, multiclass=None)</code>","text":"<p>A modified version of the <code>torchmetrics.functional.classification.f_beta._fbeta_compute</code> that can ignore NaNs by giving them the same value for both <code>preds</code> and <code>target</code>. Used to calculate the f1_score on IPU with beta parameter equal to 1.0 This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p> <p>Computes f_beta metric from stat scores: true positives, false positives, true negatives, false negatives.</p> <p>Parameters:</p> Name Type Description Default <code>tp</code> <p>True positives</p> required <code>fp</code> <p>False positives</p> required <code>tn</code> <p>True negatives</p> required <code>fn</code> <p>False negatives</p> required <code>beta</code> <code>float</code> <p>The parameter <code>beta</code> (which determines the weight of recall in the combined score)</p> <code>1.0</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method</p> <code>None</code> <code>average</code> <code>Optional[str]</code> <p>Defines the reduction that is applied</p> <code>'micro'</code> <code>mdmc_average</code> <code>Optional[str]</code> <p>Defines how averaging is done for multi-dimensional multi-class inputs (on top of the <code>average</code> parameter)</p> <code>None</code>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.fbeta_score_ipu","title":"<code>fbeta_score_ipu(preds, target, beta=1.0, average='micro', mdmc_average=None, ignore_index=None, num_classes=None, threshold=0.5, top_k=None, multiclass=None)</code>","text":"<p>A modified version of the <code>torchmetrics.functional.classification.f_beta._fbeta_compute</code> that can ignore NaNs by giving them the same value for both <code>preds</code> and <code>target</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predictions from model (probabilities, logits or labels)</p> required <code>target</code> <code>Tensor</code> <p>Ground truth labels</p> required <code>average</code> <code>Optional[str]</code> <p>Defines the reduction that is applied. Should be one of the following:</p> <ul> <li><code>'micro'</code> [default]: Calculate the metric globally, across all samples and classes.</li> <li><code>'macro'</code>: Calculate the metric for each class separately, and average the   metrics across classes (with equal weights for each class).</li> <li><code>'weighted'</code>: Calculate the metric for each class separately, and average the   metrics across classes, weighting each class by its support (<code>tp + fn</code>).</li> <li><code>'none'</code> or <code>None</code>: Calculate the metric for each class separately, and return   the metric for every class.</li> <li><code>'samples'</code>: Calculate the metric for each sample, and average the metrics   across samples (with equal weights for each sample).</li> </ul> <p>.. note:: What is considered a sample in the multi-dimensional multi-class case     depends on the value of <code>mdmc_average</code>.</p> <p>.. note:: If <code>'none'</code> and a given class doesn't occur in the <code>preds</code> or <code>target</code>,     the value for the class will be <code>nan</code>.</p> <code>'micro'</code> <code>mdmc_average</code> <code>Optional[str]</code> <p>Defines how averaging is done for multi-dimensional multi-class inputs (on top of the <code>average</code> parameter). Should be one of the following:</p> <ul> <li> <p><code>None</code> [default]: Should be left unchanged if your data is not multi-dimensional multi-class.</p> </li> <li> <p><code>'samplewise'</code>: In this case, the statistics are computed separately for each   sample on the <code>N</code> axis, and then averaged over samples.   The computation for each sample is done by treating the flattened extra axes <code>...</code>   (see :ref:<code>pages/classification:input types</code>) as the <code>N</code> dimension within the sample,   and computing the metric for the sample based on that.</p> </li> <li> <p><code>'global'</code>: In this case the <code>N</code> and <code>...</code> dimensions of the inputs   (see :ref:<code>pages/classification:input types</code>)   are flattened into a new <code>N_X</code> sample axis, i.e. the inputs are treated as if they   were <code>(N_X, C)</code>. From here on the <code>average</code> parameter applies as usual.</p> </li> </ul> <code>None</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes. Necessary for <code>'macro'</code>, <code>'weighted'</code> and <code>None</code> average methods.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.</p> <code>0.5</code> <code>top_k</code> <code>Optional[int]</code> <p>Number of the highest probability or logit score predictions considered finding the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value (<code>None</code>) will be interpreted as 1 for these inputs.</p> <p>Should be left at default (<code>None</code>) for all other types of inputs.</p> <code>None</code> <code>multiclass</code> <code>Optional[bool]</code> <p>Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref:<code>documentation section &lt;pages/classification:using the multiclass parameter&gt;</code> for a more detailed explanation and examples.</p> <code>None</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and <code>average=None</code> or <code>'none'</code>, the score for the ignored class will be returned as <code>nan</code>.</p> <code>None</code> <code>subset_accuracy</code> <p>Whether to compute subset accuracy for multi-label and multi-dimensional multi-class inputs (has no effect for other input types).</p> <ul> <li> <p>For multi-label inputs, if the parameter is set to <code>True</code>, then all labels for   each sample must be correctly predicted for the sample to count as correct. If it   is set to <code>False</code>, then all labels are counted separately - this is equivalent to   flattening inputs beforehand (i.e. <code>preds = preds.flatten()</code> and same for <code>target</code>).</p> </li> <li> <p>For multi-dimensional multi-class inputs, if the parameter is set to <code>True</code>, then all   sub-sample (on the extra axis) must be correct for the sample to be counted as correct.   If it is set to <code>False</code>, then all sub-samples are counter separately - this is equivalent,   in the case of label predictions, to flattening the inputs beforehand (i.e.   <code>preds = preds.flatten()</code> and same for <code>target</code>). Note that the <code>top_k</code> parameter   still applies in both cases, if set.</p> </li> </ul> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>top_k</code> parameter is set for <code>multi-label</code> inputs.</p> <code>ValueError</code> <p>If <code>average</code> is none of <code>\"micro\"</code>, <code>\"macro\"</code>, <code>\"weighted\"</code>, <code>\"samples\"</code>, <code>\"none\"</code>, <code>None</code>.</p> <code>ValueError</code> <p>If <code>mdmc_average</code> is not one of <code>None</code>, <code>\"samplewise\"</code>, <code>\"global\"</code>.</p> <code>ValueError</code> <p>If <code>average</code> is set but <code>num_classes</code> is not provided.</p> <code>ValueError</code> <p>If <code>num_classes</code> is set and <code>ignore_index</code> is not in the range <code>[0, num_classes)</code>.</p> <code>ValueError</code> <p>If <code>top_k</code> is not an <code>integer</code> larger than <code>0</code>.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.get_confusion_matrix","title":"<code>get_confusion_matrix(preds, target, average='micro', mdmc_average='global', threshold=0.5, top_k=None, subset_accuracy=False, num_classes=None, multiclass=None, ignore_index=None)</code>","text":"<p>Calculates the confusion matrix according to the specified average method.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predictions from model (probabilities, logits or labels)</p> required <code>target</code> <code>Tensor</code> <p>Ground truth labels</p> required <code>average</code> <code>Optional[str]</code> <p>Defines the reduction that is applied. Should be one of the following:</p> <ul> <li><code>'micro'</code> [default]: Calculate the metric globally, across all samples and classes.</li> <li><code>'macro'</code>: Calculate the metric for each class separately, and average the   metrics across classes (with equal weights for each class).</li> <li><code>'weighted'</code>: Calculate the metric for each class separately, and average the   metrics across classes, weighting each class by its support (<code>tp + fn</code>).</li> <li><code>'none'</code> or <code>None</code>: Calculate the metric for each class separately, and return   the metric for every class.</li> <li><code>'samples'</code>: Calculate the metric for each sample, and average the metrics   across samples (with equal weights for each sample).</li> </ul> <p>.. note:: What is considered a sample in the multi-dimensional multi-class case     depends on the value of <code>mdmc_average</code>.</p> <p>.. note:: If <code>'none'</code> and a given class doesn't occur in the <code>preds</code> or <code>target</code>,     the value for the class will be <code>nan</code>.</p> <code>'micro'</code> <code>mdmc_average</code> <code>Optional[str]</code> <p>Defines how averaging is done for multi-dimensional multi-class inputs (on top of the <code>average</code> parameter). Should be one of the following:</p> <ul> <li> <p><code>None</code> [default]: Should be left unchanged if your data is not multi-dimensional multi-class.</p> </li> <li> <p><code>'samplewise'</code>: In this case, the statistics are computed separately for each   sample on the <code>N</code> axis, and then averaged over samples.   The computation for each sample is done by treating the flattened extra axes <code>...</code>   (see :ref:<code>pages/classification:input types</code>) as the <code>N</code> dimension within the sample,   and computing the metric for the sample based on that.</p> </li> <li> <p><code>'global'</code>: In this case the <code>N</code> and <code>...</code> dimensions of the inputs   (see :ref:<code>pages/classification:input types</code>)   are flattened into a new <code>N_X</code> sample axis, i.e. the inputs are treated as if they   were <code>(N_X, C)</code>. From here on the <code>average</code> parameter applies as usual.</p> </li> </ul> <code>'global'</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes. Necessary for <code>'macro'</code>, <code>'weighted'</code> and <code>None</code> average methods.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.</p> <code>0.5</code> <code>top_k</code> <code>Optional[int]</code> <p>Number of the highest probability or logit score predictions considered finding the correct label, relevant only for (multi-dimensional) multi-class inputs. The default value (<code>None</code>) will be interpreted as 1 for these inputs.</p> <p>Should be left at default (<code>None</code>) for all other types of inputs.</p> <code>None</code> <code>multiclass</code> <code>Optional[bool]</code> <p>Used only in certain special cases, where you want to treat inputs as a different type than what they appear to be. See the parameter's :ref:<code>documentation section &lt;pages/classification:using the multiclass parameter&gt;</code> for a more detailed explanation and examples.</p> <code>None</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Integer specifying a target class to ignore. If given, this class index does not contribute to the returned score, regardless of reduction method. If an index is ignored, and <code>average=None</code></p> <code>None</code>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.mean_absolute_error_ipu","title":"<code>mean_absolute_error_ipu(preds, target)</code>","text":"<p>Computes mean absolute error.</p> <p>Handles NaNs without reshaping tensors in order to work on IPU.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>estimated labels</p> required <code>target</code> <code>Tensor</code> <p>ground truth labels</p> required Return <p>Tensor with MAE</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.mean_squared_error_ipu","title":"<code>mean_squared_error_ipu(preds, target, squared)</code>","text":"<p>Computes mean squared error.</p> <p>Handles NaNs without reshaping tensors in order to work on IPU.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>estimated labels</p> required <code>target</code> <code>Tensor</code> <p>ground truth labels</p> required <code>squared</code> <code>bool</code> <p>returns RMSE value if set to False</p> required Return <p>Tensor with MSE</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.pearson_ipu","title":"<code>pearson_ipu(preds, target)</code>","text":"<p>Computes pearson correlation coefficient.</p> <p>Handles NaNs in the target without reshaping tensors in order to work on IPU.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <p>estimated scores</p> required <code>target</code> <p>ground truth scores</p> required"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.precision_ipu","title":"<code>precision_ipu(preds, target, average='micro', mdmc_average=None, ignore_index=None, num_classes=None, threshold=0.5, top_k=None, multiclass=None)</code>","text":"<p>A modified version of the <code>torchmetrics.functional.precision</code> that can ignore NaNs by giving them the same value for both <code>preds</code> and <code>target</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.r2_score_ipu","title":"<code>r2_score_ipu(preds, target, *args, **kwargs)</code>","text":"<p>Computes r2 score also known as <code>R2 Score_Coefficient Determination</code>_:</p> <p>.. math:: R^2 = 1 - \frac{SS_{res}}{SS_{tot}}</p> <p>where :math:<code>SS_{res}=\\sum_i (y_i - f(x_i))^2</code> is the sum of residual squares, and :math:<code>SS_{tot}=\\sum_i (y_i - \bar{y})^2</code> is total sum of squares. Can also calculate adjusted r2 score given by</p> <p>.. math:: R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-k-1}</p> <p>where the parameter :math:<code>k</code> (the number of independent regressors) should be provided as the <code>adjusted</code> argument. Handles NaNs without reshaping tensors in order to work on IPU.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <p>estimated labels</p> required <code>target</code> <p>ground truth labels</p> required <code>adjusted</code> <p>number of independent regressors for calculating adjusted r2 score.</p> required <code>multioutput</code> <p>Defines aggregation in the case of multiple output scores. Can be one of the following strings:</p> <ul> <li><code>'raw_values'</code> returns full set of scores</li> <li><code>'uniform_average'</code> scores are uniformly averaged</li> <li><code>'variance_weighted'</code> scores are weighted by their individual variances</li> </ul> required"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.recall_ipu","title":"<code>recall_ipu(preds, target, average='micro', mdmc_average=None, ignore_index=None, num_classes=None, threshold=0.5, top_k=None, multiclass=None)</code>","text":"<p>A modified version of the <code>torchmetrics.functional.recall</code> that can ignore NaNs by giving them the same value for both <code>preds</code> and <code>target</code>. This allows it to work with compilation and IPUs since it doesn't modify the tensor's shape.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_metrics.spearman_ipu","title":"<code>spearman_ipu(preds, target)</code>","text":"<p>Computes spearman rank correlation coefficient.</p> <p>Handles NaNs in the target without reshaping tensors in order to work on IPU.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <p>estimated scores</p> required <code>target</code> <p>ground truth scores</p> required"},{"location":"api/graphium.ipu.html#ipu-simple-lightning","title":"IPU Simple Lightning","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_simple_lightning","title":"<code>graphium.ipu.ipu_simple_lightning</code>","text":""},{"location":"api/graphium.ipu.html#ipu-utils","title":"IPU Utils","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils","title":"<code>graphium.ipu.ipu_utils</code>","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils.import_poptorch","title":"<code>import_poptorch(raise_error=True)</code>","text":"<p>Import poptorch and returns it. It is wrapped in a function to avoid breaking the code for non-IPU devices which did not install poptorch.</p> <p>Parameters:</p> Name Type Description Default <code>raise_error</code> <p>Whether to raise an error if poptorch is unavailable. If <code>False</code>, return <code>None</code></p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[ModuleType]</code> <p>The poptorch module</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils.ipu_options_list_to_file","title":"<code>ipu_options_list_to_file(ipu_opts)</code>","text":"<p>Create a temporary file from a list of ipu configs, such that it can be read by <code>poptorch.Options.loadFromFile</code></p> <p>Parameters:</p> Name Type Description Default <code>ipu_opts</code> <code>Optional[List[str]]</code> <p>The list  configurations for the IPU, written as a list of strings to make use of <code>poptorch.Options.loadFromFile</code></p> required <p>Returns:</p> Name Type Description <code>tmp_file</code> <code>tempfile._TemporaryFileWrapper</code> <p>The temporary file of ipu configs</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils.is_running_on_ipu","title":"<code>is_running_on_ipu()</code>","text":"<p>Returns whether the current module is running on ipu. Needs to be used in the <code>forward</code> or <code>backward</code> pass.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils.load_ipu_options","title":"<code>load_ipu_options(ipu_opts, seed=None, model_name=None, gradient_accumulation=None, precision=None, ipu_inference_opts=None)</code>","text":"<p>Load the IPU options from the config file.</p> <p>Parameters:</p> Name Type Description Default <code>ipu_cfg</code> <p>The list  configurations for the IPU, written as a list of strings to make use of <code>poptorch.Options.loadFromFile</code></p> <p>write a temporary config gile, and read it. See <code>Options.loadFromFile</code></p> required <code>seed</code> <code>Optional[int]</code> <p>random seed for the IPU</p> <code>None</code> <code>model_name</code> <code>Optional[str]</code> <p>Name of the model, to be used for ipu profiling</p> <code>None</code> <code>ipu_inference_opts</code> <code>Optional[List[str]]</code> <p>optional IPU configuration overrides for inference. If this is provided, options in this file override those in <code>ipu_file</code> for inference.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>training_opts</code> <code>poptorch.Options</code> <p>IPU options for the training set.</p> <code>inference_opts</code> <code>poptorch.Options</code> <p>IPU options for inference. It differs from the <code>training_opts</code> by enforcing <code>gradientAccumulation</code> to 1</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils.load_ipu_options--see-the-tutorial-for-ipu-options-here","title":"? see the tutorial for IPU options here","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils.load_ipu_options--httpsgithubcomgraphcoretutorialstreesdk-release-26tutorialspytorchefficient_data_loading","title":"https://github.com/graphcore/tutorials/tree/sdk-release-2.6/tutorials/pytorch/efficient_data_loading","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils.load_ipu_options--see-the-full-documentation-for-ipu-options-here","title":"? see the full documentation for ipu options here","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_utils.load_ipu_options--httpsdocsgraphcoreaiprojectspoptorch-user-guideenlatestreferencehtmlhighlightoptionspoptorchoptions","title":"https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/reference.html?highlight=options#poptorch.Options","text":"<p>minibatch size: The number of samples processed by one simple fwd/bwd pass. = # of samples in a minibatch</p> <p>device iterations: A device iteration corresponds to one iteration of the training loop executed on the IPU, starting with data-loading and ending with a weight update. In this simple case, when we set n deviceIterations, the host will prepare n mini-batches in an infeed queue so the IPU can perform efficiently n iterations. = # of minibatches to be processed at a time = # of training / backward pass in this call</p> <p>gradient accumulation factor: After each backward pass the gradients are accumulated together for K mini-batches. set K in the argument = # of minibatches to accumulate gradients from</p> <p>replication factor: Replication describes the process of running multiple instances of the same model simultaneously on different IPUs to achieve data parallelism. If the model requires N IPUs and the replication factor is M, N x M IPUs will be necessary. = # of times the model is copied to speed up computation, each replica of the model is sent a different subset of the dataset</p> <p>global batch size: In a single device iteration, many mini-batches may be processed and the resulting gradients accumulated. We call this total number of samples processed for one optimiser step the global batch size. = total number of samples processed for one optimiser step = (minibatch size x Gradient accumulation factor) x Number of replicas</p>"},{"location":"api/graphium.ipu.html#ipu-wrapper","title":"IPU Wrapper","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_wrapper","title":"<code>graphium.ipu.ipu_wrapper</code>","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_wrapper.PredictorModuleIPU","title":"<code>PredictorModuleIPU</code>","text":"<p>         Bases: <code>PredictorModule</code></p> <p>This class wraps around the <code>PredictorModule</code> to make it work with IPU and the <code>IPUPluginGraphium</code>.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_wrapper.PredictorModuleIPU.convert_from_fp16","title":"<code>convert_from_fp16(data)</code>","text":"<p>Converts tensors from FP16 to FP32. Useful to convert the IPU program output data</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_wrapper.PredictorModuleIPU.get_num_graphs","title":"<code>get_num_graphs(data)</code>","text":"<p>IPU specific method to compute the number of graphs in a Batch, that considers gradient accumulation, multiple IPUs and multiple device iterations. Essential to estimate throughput in graphs/s.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_wrapper.PyGArgsParser","title":"<code>PyGArgsParser</code>","text":"<p>         Bases: <code>poptorch.ICustomArgParser</code></p> <p>This class is responsible for converting a PyG Batch from and to a tensor of tuples. This allows PyG Batch to be used as inputs to IPU programs. Copied from poppyg repo, in the future import from the repo directly.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_wrapper.PyGArgsParser.reconstruct","title":"<code>reconstruct(original_structure, tensor_iterator)</code>","text":"<p>Create a new instance with the same class type as the original_structure. This new instance will be initialized with tensors from the provided iterator and uses the same sorted keys from the yieldTensors() implementation.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_wrapper.PyGArgsParser.sortedTensorKeys","title":"<code>sortedTensorKeys(struct)</code>  <code>staticmethod</code>","text":"<p>Find all the keys that map to a tensor value in struct. The keys are returned in sorted order.</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.ipu_wrapper.PyGArgsParser.yieldTensors","title":"<code>yieldTensors(struct)</code>","text":"<p>yield every torch.Tensor in struct in sorted order</p>"},{"location":"api/graphium.ipu.html#to-dense-batch","title":"To Dense Batch","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.to_dense_batch","title":"<code>graphium.ipu.to_dense_batch</code>","text":""},{"location":"api/graphium.ipu.html#graphium.ipu.to_dense_batch.to_dense_batch","title":"<code>to_dense_batch(x, batch=None, fill_value=0.0, max_num_nodes_per_graph=None, batch_size=None, drop_nodes_last_graph=False)</code>","text":"<p>Given a sparse batch of node features :math:<code>\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}</code> (with :math:<code>N_i</code> indicating the number of nodes in graph :math:<code>i</code>), creates a dense node feature tensor :math:<code>\\mathbf{X} \\in \\mathbb{R}^{B \\times N_{\\max} \\times F}</code> (with :math:<code>N_{\\max} = \\max_i^B N_i</code>). In addition, a mask of shape :math:<code>\\mathbf{M} \\in \\{ 0, 1 \\}^{B \\times N_{\\max}}</code> is returned, holding information about the existence of fake-nodes in the dense representation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Node feature matrix :math:<code>\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}</code>.</p> required <code>batch</code> <code>Optional[Tensor]</code> <p>Batch vector :math:<code>\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N</code>, which assigns each node to a specific example. Must be ordered. (default: :obj:<code>None</code>)</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>The value for invalid entries in the resulting dense output tensor. (default: :obj:<code>0</code>)</p> <code>0.0</code> <code>max_num_nodes_per_graph</code> <code>Optional[int]</code> <p>The size of the output node dimension. (default: :obj:<code>None</code>)</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>The batch size. (default: :obj:<code>None</code>)</p> <code>None</code> <code>drop_nodes_last_graph</code> <p>Whether to drop the nodes of the last graphs that exceed the <code>max_num_nodes_per_graph</code>. Useful when the last graph is a padding.</p> <code>False</code> <p>:rtype: (:class:<code>Tensor</code>, :class:<code>BoolTensor</code>)</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.to_dense_batch.to_packed_dense_batch","title":"<code>to_packed_dense_batch(x, pack_from_node_idx, pack_attn_mask, fill_value=0.0, max_num_nodes_per_pack=None)</code>","text":"<p>Given a sparse batch of node features :math:<code>\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}</code> (with :math:<code>N_i</code> indicating the number of nodes in graph :math:<code>i</code>), creates a dense node feature tensor :math:<code>\\mathbf{X} \\in \\mathbb{R}^{B \\times N_{\\max} \\times F}</code> (with :math:<code>N_{\\max} = \\max_i^B N_i</code>). In addition, a mask of shape :math:<code>\\mathbf{M} \\in \\{ 0, 1 \\}^{B \\times N_{\\max}}</code> is returned, holding information about the existence of fake-nodes in the dense representation.</p> <p># TODO: Update docstring</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Node feature matrix :math:<code>\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}</code>.</p> required <code>batch</code> <p>Batch vector :math:<code>\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N</code>, which assigns each node to a specific example. Must be ordered. (default: :obj:<code>None</code>)</p> required <code>fill_value</code> <code>float</code> <p>The value for invalid entries in the resulting dense output tensor. (default: :obj:<code>0</code>)</p> <code>0.0</code> <code>max_num_nodes_per_graph</code> <p>The size of the output node dimension. (default: :obj:<code>None</code>)</p> required <code>batch_size</code> <p>The batch size. (default: :obj:<code>None</code>)</p> required <code>drop_nodes_last_graph</code> <p>Whether to drop the nodes of the last graphs that exceed the <code>max_num_nodes_per_graph</code>. Useful when the last graph is a padding.</p> required <p>:rtype: (:class:<code>Tensor</code>, :class:<code>BoolTensor</code>)</p>"},{"location":"api/graphium.ipu.html#graphium.ipu.to_dense_batch.to_sparse_batch","title":"<code>to_sparse_batch(x, mask_idx)</code>","text":"<p>Reverse function of <code>to_dense_batch</code></p>"},{"location":"api/graphium.ipu.html#graphium.ipu.to_dense_batch.to_sparse_batch_from_packed","title":"<code>to_sparse_batch_from_packed(x, pack_from_node_idx)</code>","text":"<p>Reverse function of <code>to_packed_dense_batch</code></p>"},{"location":"api/graphium.trainer.html","title":"graphium.trainer","text":"<p>Code for training models</p> Contents <ul> <li>Predictor</li> <li>Metrics</li> <li>Predictor Summaries</li> <li>Predictor Options</li> </ul>"},{"location":"api/graphium.trainer.html#predictor","title":"Predictor","text":""},{"location":"api/graphium.trainer.html#graphium.trainer.predictor","title":"<code>graphium.trainer.predictor</code>","text":""},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule","title":"<code>PredictorModule</code>","text":"<p>         Bases: <code>lightning.LightningModule</code></p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule.__init__","title":"<code>__init__(model_class, model_kwargs, loss_fun, random_seed=42, optim_kwargs=None, torch_scheduler_kwargs=None, scheduler_kwargs=None, target_nan_mask=None, multitask_handling=None, metrics=None, metrics_on_progress_bar=[], metrics_on_training_set=None, flag_kwargs=None, task_norms=None)</code>","text":"<p>The Lightning module responsible for handling the predictions, losses, metrics, optimization, etc. It works in a multi-task setting, with different losses and metrics per class</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>Type[nn.Module]</code> <p>The torch Module containing the main forward function</p> required <code>model_kwargs</code> <code>Dict[str, Any]</code> <p>The arguments to initialize the model from <code>model_class</code></p> required <code>loss_fun</code> <code>Dict[str, Union[str, Callable]]</code> <p>A <code>dict[str, fun]</code>, where <code>str</code> is the task name and <code>fun</code> the loss function</p> required <code>random_seed</code> <code>int</code> <p>The seed for random initialization</p> <code>42</code> <code>optim_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>The optimization arguments. See class <code>OptimOptions</code></p> <code>None</code> <code>torch_scheduler_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>The torch scheduler arguments. See class <code>OptimOptions</code></p> <code>None</code> <code>scheduler_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>The lightning scheduler arguments. See class <code>OptimOptions</code></p> <code>None</code> <code>target_nan_mask</code> <code>Optional[Union[str, int]]</code> <p>How to handle the NaNs. See <code>MetricsWrapper</code> for options</p> <code>None</code> <code>metrics</code> <code>Dict[str, Callable]</code> <p>A <code>dict[str, fun]</code>, where <code>str</code> is the task name and <code>fun</code> the metric function</p> <code>None</code> <code>metrics_on_progress_bar</code> <code>Dict[str, List[str]]</code> <p>A <code>dict[str, list[str2]</code>, where <code>str</code> is the task name and <code>str2</code> the metrics to include on the progress bar</p> <code>[]</code> <code>metrics_on_training_set</code> <code>Optional[Dict[str, List[str]]]</code> <p>A <code>dict[str, list[str2]</code>, where <code>str</code> is the task name and <code>str2</code> the metrics to include on the training set</p> <code>None</code> <code>flag_kwargs</code> <code>Dict[str, Any]</code> <p>Arguments related to using the FLAG adversarial augmentation</p> <code>None</code> <code>task_norms</code> <code>Optional[Dict[Callable, Any]]</code> <p>the normalization for each task</p> <code>None</code>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule.compute_loss","title":"<code>compute_loss(preds, targets, weights, loss_fun, target_nan_mask=None, multitask_handling=None)</code>  <code>staticmethod</code>","text":"<p>Compute the loss using the specified loss function, and dealing with the nans in the <code>targets</code>.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Dict[str, Tensor]</code> <p>Predicted values</p> required <code>targets</code> <code>Dict[str, Tensor]</code> <p>Target values</p> required <code>weights</code> <code>Optional[Tensor]</code> <p>No longer supported, will raise an error.</p> required <code>target_nan_mask</code> <code>Optional[Union[str, int]]</code> <ul> <li> <p>None: Do not change behaviour if there are NaNs</p> </li> <li> <p>int, float: Value used to replace NaNs. For example, if <code>target_nan_mask==0</code>, then   all NaNs will be replaced by zeros</p> </li> <li> <p>'ignore': The NaN values will be removed from the tensor before computing the metrics.   Must be coupled with the <code>multitask_handling='flatten'</code> or <code>multitask_handling='mean-per-label'</code>.</p> </li> </ul> <code>None</code> <code>multitask_handling</code> <code>Optional[str]</code> <ul> <li> <p>None: Do not process the tensor before passing it to the metric.   Cannot use the option <code>multitask_handling=None</code> when <code>target_nan_mask=ignore</code>.   Use either 'flatten' or 'mean-per-label'.</p> </li> <li> <p>'flatten': Flatten the tensor to produce the equivalent of a single task</p> </li> <li> <p>'mean-per-label': Loop all the labels columns, process them as a single task,     and average the results over each task   This option might slowdown the computation if there are too many labels</p> </li> </ul> <code>None</code> <code>loss_fun</code> <code>Dict[str, Callable]</code> <p>Loss function to use</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tuple[Tensor, Dict[str, Tensor]]</code> <p>weighted_loss: Resulting weighted loss all_task_losses: Loss per task</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule.flag_step","title":"<code>flag_step(batch, step_name, to_cpu)</code>","text":"<p>Perform adversarial data agumentation during one training step using FLAG. Paper: https://arxiv.org/abs/2010.09891 Github: https://github.com/devnkong/FLAG</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule.forward","title":"<code>forward(inputs)</code>","text":"<p>Returns the result of <code>self.model.forward(*inputs)</code> on the inputs. If the output of <code>out = self.model.forward</code> is a dictionary with a <code>\"preds\"</code> key, it is returned directly. Otherwise, a new dictionary is created and returns <code>{\"preds\": out}</code>.</p> <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, Dict[str, Tensor], Dict[str, Dict[str, Tensor]]]]</code> <p>A dict with a key <code>\"preds\"</code> representing the prediction of the network.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule.get_num_graphs","title":"<code>get_num_graphs(data)</code>","text":"<p>Method to compute number of graphs in a Batch. Essential to estimate throughput in graphs/s.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule.list_pretrained_models","title":"<code>list_pretrained_models()</code>  <code>staticmethod</code>","text":"<p>List available pretrained models.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor.PredictorModule.load_pretrained_models","title":"<code>load_pretrained_models(name)</code>  <code>staticmethod</code>","text":"<p>Load a pretrained model from its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model to load. List available from <code>graphium.trainer.PredictorModule.list_pretrained_models()</code>.</p> required"},{"location":"api/graphium.trainer.html#metrics","title":"Metrics","text":""},{"location":"api/graphium.trainer.html#graphium.trainer.metrics","title":"<code>graphium.trainer.metrics</code>","text":""},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.MetricWrapper","title":"<code>MetricWrapper</code>","text":"<p>Allows to initialize a metric from a name or Callable, and initialize the <code>Thresholder</code> in case the metric requires a threshold.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.MetricWrapper.__call__","title":"<code>__call__(preds, target)</code>","text":"<p>Compute the metric with the method <code>self.compute</code></p>"},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.MetricWrapper.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Serialize the class for pickling.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.MetricWrapper.__init__","title":"<code>__init__(metric, threshold_kwargs=None, target_nan_mask=None, multitask_handling=None, squeeze_targets=False, target_to_int=False, **kwargs)</code>","text":"<p>Parameters     metric:         The metric to use. See <code>METRICS_DICT</code></p> <pre><code>threshold_kwargs:\n    If `None`, no threshold is applied.\n    Otherwise, we use the class `Thresholder` is initialized with the\n    provided argument, and called before the `compute`\n\ntarget_nan_mask:\n\n    - None: Do not change behaviour if there are NaNs\n\n    - int, float: Value used to replace NaNs. For example, if `target_nan_mask==0`, then\n      all NaNs will be replaced by zeros\n\n    - 'ignore': The NaN values will be removed from the tensor before computing the metrics.\n      Must be coupled with the `multitask_handling='flatten'` or `multitask_handling='mean-per-label'`.\n\nmultitask_handling:\n    - None: Do not process the tensor before passing it to the metric.\n      Cannot use the option `multitask_handling=None` when `target_nan_mask=ignore`.\n      Use either 'flatten' or 'mean-per-label'.\n\n    - 'flatten': Flatten the tensor to produce the equivalent of a single task\n\n    - 'mean-per-label': Loop all the labels columns, process them as a single task,\n        and average the results over each task\n      *This option might slow down the computation if there are too many labels*\n\nsqueeze_targets:\n    If true, targets will be squeezed prior to computing the metric.\n    Required in classifigression task.\n\ntarget_to_int:\n    If true, targets will be converted to integers prior to computing the metric.\n\nkwargs:\n    Other arguments to call with the metric\n</code></pre>"},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.MetricWrapper.__repr__","title":"<code>__repr__()</code>","text":"<p>Control how the class is printed</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.MetricWrapper.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Reload the class from pickling.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.MetricWrapper.compute","title":"<code>compute(preds, target)</code>","text":"<p>Compute the metric, apply the thresholder if provided, and manage the NaNs</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.Thresholder","title":"<code>Thresholder</code>","text":""},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.Thresholder.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Serialize the class for pickling.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.metrics.Thresholder.__repr__","title":"<code>__repr__()</code>","text":"<p>Control how the class is printed</p>"},{"location":"api/graphium.trainer.html#predictor-summaries","title":"Predictor Summaries","text":""},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries","title":"<code>graphium.trainer.predictor_summaries</code>","text":"<p>Classes to store information about resulting evaluation metrics when using a Predictor Module.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary","title":"<code>Summary</code>","text":"<p>         Bases: <code>SummaryInterface</code></p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.Results","title":"<code>Results</code>","text":""},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.Results.__init__","title":"<code>__init__(targets=None, predictions=None, loss=None, metrics=None, monitored_metric=None, n_epochs=None)</code>","text":"<p>This inner class is used as a container for storing the results of the summary.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>Tensor</code> <p>the targets</p> <code>None</code> <code>predictions</code> <code>Tensor</code> <p>the prediction tensor</p> <code>None</code> <code>loss</code> <code>float</code> <p>the loss, float or tensor</p> <code>None</code> <code>metrics</code> <code>dict</code> <p>the metrics</p> <code>None</code> <code>monitored_metric</code> <code>str</code> <p>the monitored metric</p> <code>None</code> <code>n_epochs</code> <code>int</code> <p>the number of epochs</p> <code>None</code>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.__init__","title":"<code>__init__(loss_fun, metrics, metrics_on_training_set=[], metrics_on_progress_bar=[], monitor='loss', mode='min', task_name=None)</code>","text":"<p>A container to be used by the Predictor Module that stores the results for the given metrics on the predictions and targets provided.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fun</code> <code>Union[str, Callable]</code> required <code>metrics</code> <code>Dict[str, Callable]</code> required <code>metrics_on_training_set</code> <code>List[str]</code> <code>[]</code> <code>metrics_on_progress_bar</code> <code>List[str]</code> <code>[]</code> <code>monitor</code> <code>str</code> <code>'loss'</code> <code>task_name</code> <code>Optional[str]</code> <code>None</code>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.get_best_results","title":"<code>get_best_results(step_name)</code>","text":"<p>retrieve the best results for a given step</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>which stage you are in, e.g. \"train\"</p> required <p>Returns:</p> Type Description <p>the best results for the given step</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.get_dict_summary","title":"<code>get_dict_summary()</code>","text":"<p>retrieve the full summary in a dictionary</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>the full summary in a dictionary</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.get_metrics_logs","title":"<code>get_metrics_logs()</code>","text":"<p>Get the data about metrics to log. Note: This function requires that self.update_predictor_state() be called before it.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of metrics to log.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.get_results","title":"<code>get_results(step_name)</code>","text":"<p>retrieve the results for a given step</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>which stage you are in, e.g. \"train\"</p> required <p>Returns:</p> Type Description <p>the results for the given step</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.get_results_on_progress_bar","title":"<code>get_results_on_progress_bar(step_name)</code>","text":"<p>retrieve the results to be displayed on the progress bar for a given step</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>which stage you are in, e.g. \"train\"</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>the results to be displayed on the progress bar for the given step</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.is_best_epoch","title":"<code>is_best_epoch(step_name, loss, metrics)</code>","text":"<p>check if the current epoch is the best epoch based on self.mode criteria</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>which stage you are in, e.g. \"train\"</p> required <code>loss</code> <code>Tensor</code> <p>the loss tensor</p> required <code>metrics</code> <code>Dict[str, Tensor]</code> <p>a dictionary of metrics</p> required"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.set_results","title":"<code>set_results(metrics)</code>","text":"<p>set the reults from the metrics [!] This function requires that self.update_predictor_state() be called before it.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Dict[str, Tensor]</code> <p>a dictionary of metrics</p> required"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.Summary.update_predictor_state","title":"<code>update_predictor_state(step_name, targets, predictions, loss, n_epochs)</code>","text":"<p>update the state of the predictor</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>which stage you are in, e.g. \"train\"</p> required <code>targets</code> <code>Tensor</code> <p>the targets tensor</p> required <code>predictions</code> <code>Tensor</code> <p>the predictions tensor</p> required <code>loss</code> <code>Tensor</code> <p>the loss tensor</p> required <code>n_epochs</code> <code>int</code> <p>the number of epochs</p> required"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.SummaryInterface","title":"<code>SummaryInterface</code>","text":"<p>         Bases: <code>object</code></p> <p>An interface to define the functions implemented by summary classes that implement SummaryInterface.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries","title":"<code>TaskSummaries</code>","text":"<p>         Bases: <code>SummaryInterface</code></p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.__init__","title":"<code>__init__(task_loss_fun, task_metrics, task_metrics_on_training_set, task_metrics_on_progress_bar, monitor='loss', mode='min')</code>","text":"<p>class to store the summaries of the tasks</p> <p>Parameters:</p> Name Type Description Default <code>task_loss_fun</code> <code>Callable</code> <p>the loss function for each task</p> required <code>task_metrics</code> <code>Dict[str, Callable]</code> <p>the metrics for each task</p> required <code>task_metrics_on_training_set</code> <code>List[str]</code> <p>the metrics to use on the training set</p> required <code>task_metrics_on_progress_bar</code> <code>List[str]</code> <p>the metrics to use on the progress bar</p> required <code>monitor</code> <code>str</code> <p>the metric to monitor</p> <code>'loss'</code> <code>mode</code> <code>str</code> <p>the mode of the metric to monitor</p> <code>'min'</code>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.concatenate_metrics_logs","title":"<code>concatenate_metrics_logs(metrics_logs)</code>","text":"<p>concatenate the metrics logs</p> <p>Parameters:</p> Name Type Description Default <code>metrics_logs</code> <code>Dict[str, Dict[str, Tensor]]</code> <p>the metrics logs</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>the concatenated metrics logs</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.get_best_results","title":"<code>get_best_results(step_name)</code>","text":"<p>retrieve the best results</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>the name of the step, i.e. \"train\"</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>the best results</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.get_dict_summary","title":"<code>get_dict_summary()</code>","text":"<p>get task summaries in a dictionary</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>the task summaries</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.get_metrics_logs","title":"<code>get_metrics_logs()</code>","text":"<p>get the logs for the metrics</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Tensor]]</code> <p>the task logs for the metrics</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.get_results","title":"<code>get_results(step_name)</code>","text":"<p>retrieve the results</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>the name of the step, i.e. \"train\"</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>the results</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.get_results_on_progress_bar","title":"<code>get_results_on_progress_bar(step_name)</code>","text":"<p>return all results from all tasks for the progress bar Combine the dictionaries. Instead of having keys as task names, we merge all the task-specific dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>the name of the step</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>the results for the progress bar</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.metric_log_name","title":"<code>metric_log_name(task_name, metric_name, step_name)</code>","text":"<p>print the metric name, task name and step name</p> <p>Returns:</p> Type Description <code>str</code> <p>the metric name, task name and step name</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.set_results","title":"<code>set_results(task_metrics)</code>","text":"<p>set the results for all tasks</p> <p>Parameters:</p> Name Type Description Default <code>task_metrics</code> <code>Dict[str, Dict[str, Tensor]]</code> <p>the metrics for each task</p> required"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_summaries.TaskSummaries.update_predictor_state","title":"<code>update_predictor_state(step_name, targets, predictions, loss, task_losses, n_epochs)</code>","text":"<p>update the state for all predictors</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>the name of the step</p> required <code>targets</code> <code>Dict[str, Tensor]</code> <p>the target tensors</p> required <code>predictions</code> <code>Dict[str, Tensor]</code> <p>the prediction tensors</p> required <code>loss</code> <code>Tensor</code> <p>the loss tensor</p> required <code>task_losses</code> <code>Dict[str, Tensor]</code> <p>the task losses</p> required <code>n_epochs</code> <code>int</code> <p>the number of epochs</p> required"},{"location":"api/graphium.trainer.html#predictor-options","title":"Predictor Options","text":""},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_options","title":"<code>graphium.trainer.predictor_options</code>","text":"<p>Data classes to group together related arguments for the creation of a Predictor Module.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_options.EvalOptions","title":"<code>EvalOptions</code>  <code>dataclass</code>","text":"<p>This data class stores the arguments necessary to instantiate a model for the Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fun</code> <code>Union[str, Dict, Callable]</code> <p>Loss function used during training. Acceptable strings are graphium.utils.spaces.LOSS_DICT.keys(). If a dict, must contain a 'name' key with one of the acceptable loss function strings as a value. The rest of the dict will be used as the arguments passed to the loss object. Otherwise, a callable object must be provided, with a method <code>loss_fun._get_name()</code>.</p> required <code>metrics</code> <code>Dict[str, Callable]</code> <p>A dictionnary of metrics to compute on the prediction, other than the loss function. These metrics will be logged into WandB or other.</p> <code>None</code> <code>metrics_on_progress_bar</code> <code>List[str]</code> <p>The metrics names from <code>metrics</code> to display also on the progress bar of the training</p> <code>field(default_factory=List[str])</code> <code>metrics_on_training_set</code> <code>Optional[List[str]]</code> <p>The metrics names from <code>metrics</code> to be computed on the training set for each iteration. If <code>None</code>, all the metrics are computed. Using less metrics can significantly improve performance, depending on the number of readouts.</p> <code>None</code>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_options.EvalOptions.check_metrics_validity","title":"<code>check_metrics_validity()</code>","text":"<p>Check that the metrics for the progress_par and training_set are valid</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_options.EvalOptions.parse_loss_fun","title":"<code>parse_loss_fun(loss_fun)</code>  <code>staticmethod</code>","text":"<p>Parse the loss function from a string or a dict</p> <p>Parameters:</p> Name Type Description Default <code>loss_fun</code> <code>Union[str, Dict, Callable]</code> <p>A callable corresponding to the loss function, a string specifying the loss function from <code>LOSS_DICT</code>, or a dict containing a key 'name' specifying the loss function, and the rest of the dict used as the arguments for the loss. Accepted strings are: graphium.utils.spaces.LOSS_DICT.keys().</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Function or callable to compute the loss, takes <code>preds</code> and <code>targets</code> as inputs.</p>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_options.FlagOptions","title":"<code>FlagOptions</code>  <code>dataclass</code>","text":"<p>This data class stores the arguments necessary to instantiate a model for the Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>flag_kwargs</code> <code>Dict[str, Any]</code> <p>Keyword arguments used for FLAG, and adversarial data augmentation for graph networks. See: https://arxiv.org/abs/2010.09891</p> <ul> <li> <p>n_steps: An integer that specifies the number of ascent steps when running FLAG during training.     Default value of 0 trains GNNs without FLAG, and any value greater than 0 will use FLAG with that     many iterations.</p> </li> <li> <p>alpha: A float that specifies the ascent step size when running FLAG. Default=0.01</p> </li> </ul> <code>None</code>"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_options.ModelOptions","title":"<code>ModelOptions</code>  <code>dataclass</code>","text":"<p>This data class stores the arguments necessary to instantiate a model for the Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>Type[nn.Module]</code> <p>pytorch module used to create a model</p> required <code>model_kwargs</code> <code>Dict[str, Any]</code> <p>Key-word arguments used to initialize the model from <code>model_class</code>.</p> required"},{"location":"api/graphium.trainer.html#graphium.trainer.predictor_options.OptimOptions","title":"<code>OptimOptions</code>  <code>dataclass</code>","text":"<p>This data class stores the arguments necessary to configure the optimizer for the Predictor.</p> <p>Parameters:</p> Name Type Description Default <code>optim_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionnary used to initialize the optimizer, with possible keys below.</p> <ul> <li>lr <code>float</code>: Learning rate (Default=<code>1e-3</code>)</li> <li>weight_decay <code>float</code>: Weight decay used to regularize the optimizer (Default=<code>0.</code>)</li> </ul> <code>None</code> <code>torch_scheduler_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionnary for the scheduling of learning rate, with possible keys below.</p> <ul> <li>type <code>str</code>: Type of the learning rate to use from pytorch. Examples are     <code>'ReduceLROnPlateau'</code> (default), <code>'CosineAnnealingWarmRestarts'</code>, <code>'StepLR'</code>, etc.</li> <li>**kwargs: Any other argument for the learning rate scheduler</li> </ul> <code>None</code> <code>scheduler_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionnary for the scheduling of the learning rate modification used by pytorch-lightning</p> <ul> <li>monitor <code>str</code>: metric to track (Default=<code>\"loss/val\"</code>)</li> <li>interval <code>str</code>: Whether to look at iterations or epochs (Default=<code>\"epoch\"</code>)</li> <li>strict <code>bool</code>: if set to True will enforce that value specified in monitor is available     while trying to call scheduler.step(), and stop training if not found. If False will     only give a warning and continue training (without calling the scheduler). (Default=<code>True</code>)</li> <li>frequency <code>int</code>: TODO: NOT REALLY SURE HOW IT WORKS! (Default=<code>1</code>)</li> </ul> <code>None</code> <code>scheduler_class</code> <code>Optional[Union[str, Type]]</code> <p>The class to use for the scheduler, or the str representing the scheduler.</p> <code>None</code>"},{"location":"api/graphium.utils.html","title":"graphium.utils","text":"<p>module for utility functions</p> Contents <ul> <li>Argument Checker</li> <li>Decorators</li> <li>Dictionary of Tensors</li> <li>File System</li> <li>Hashing</li> <li>Moving Average Tracker</li> <li>MUP</li> <li>Read File</li> <li>Safe Run</li> <li>Spaces</li> <li>Tensor</li> </ul>"},{"location":"api/graphium.utils.html#argument-checker","title":"Argument Checker","text":""},{"location":"api/graphium.utils.html#graphium.utils.arg_checker","title":"<code>graphium.utils.arg_checker</code>","text":"<p>Argument checker module</p>"},{"location":"api/graphium.utils.html#graphium.utils.arg_checker.check_arg_iterator","title":"<code>check_arg_iterator(arg, enforce_type=None, enforce_subtype=None, cast_subtype=True)</code>","text":"<p>Verify if the type is an iterator. If it is <code>None</code>, convert to an empty list/tuple. If it is not a list/tuple/str, try to convert to an iterator. If it is a str or cannot be converted to an iterator, then put the <code>arg</code> inside an iterator. Possibly enforce the iterator type to <code>list</code> or <code>tuple</code>, if <code>enfoce_type</code> is not None. Possibly enforce the subtype to any given type if <code>enforce_subtype</code> is not None, and decide whether to cast the subtype or to throw an error.</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>any type</code> <p>The input to verify/convert to an iterator (list or tuple). If None, an empty iterator is returned.</p> required <code>enforce_type</code> <code>str or type</code> <p>The type to enforce the iterator. The valid choices are : <code>None</code>, <code>list</code>, <code>tuple</code>, <code>'none'</code>, <code>'list'</code>, <code>'tuple'</code>. If <code>None</code>, then the iterator type is not enforced.</p> <code>None</code> <code>enforce_subtype</code> <code>type, np.dtype or str representing basic type</code> <p>Verify if all the elements inside the iterator are the desired type. If <code>None</code>, then the sub-type is not enforced. Accepted strings are ['none', 'str', 'list', 'tuple', 'dict', 'int', 'float', 'complex', 'bool', 'callable']</p> <code>None</code> <code>cast_subtype</code> <code>bool</code> <p>If True, then the type specified by <code>enforce_subtype</code> is used to cast the elements inside the iterator. If False, then an error is thrown if the types do not match.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>output</code> <code>iterator</code> <p>An iterator based on the input of the desired type (list or tuple) and the desired subtypes.</p>"},{"location":"api/graphium.utils.html#graphium.utils.arg_checker.check_columns_choice","title":"<code>check_columns_choice(dataframe, columns_choice, extra_accepted_cols=None, enforce_type='list')</code>","text":"<p>Verify if the choice of column <code>columns_choice</code> is inside the dataframe or the extra_accepted_cols. Otherwise, errors are thrown by the sub-functions.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <p>(pd.DataFrame) The dataframe on which to verify if the column choice is valid. columns_choice: str, iterator(str) The columns chosen from the dataframe</p> required <code>extra_accepted_cols</code> <p>str, iterator(str) A list</p> <code>None</code> <code>enforce_type</code> <p>str or type The type to enforce the iterator. The valid choices are : <code>None</code>, <code>list</code>, <code>tuple</code>, <code>'none'</code>, <code>'list'</code>, <code>'tuple'</code>. If <code>None</code>, then the iterator type is not enforced.</p> <code>'list'</code> <p>Returns:</p> Name Type Description <code>output</code> <p>iterator A str iterator based on the input of the desired type (list or tuple)</p>"},{"location":"api/graphium.utils.html#graphium.utils.arg_checker.check_list1_in_list2","title":"<code>check_list1_in_list2(list1, list2, throw_error=True)</code>","text":"<p>Verify if the list1 (iterator) is included in list2 (iterator). If not, raise an error.</p> <p>Parameters:</p> Name Type Description Default <code>list1,</code> <code>list2</code> <p>list, tuple or object A list or tuple containing the elements to verify the inclusion. If an object is provided other than a list or tuple, then it is considered as a list of a single element.</p> required <code>throw_error</code> <p>bool Whether to throw an error if list1 is not in list2</p> <code>True</code> <p>Returns:</p> Name Type Description <code>list1_in_list2</code> <p>bool A boolean representing the inclusion of list1 in list2. It is returned if throw_error is set to false</p>"},{"location":"api/graphium.utils.html#decorators","title":"Decorators","text":""},{"location":"api/graphium.utils.html#graphium.utils.decorators","title":"<code>graphium.utils.decorators</code>","text":""},{"location":"api/graphium.utils.html#graphium.utils.decorators.classproperty","title":"<code>classproperty</code>","text":"<p>         Bases: <code>property</code></p> <p>Decorator used to declare a class property, defined for the class without needing to instanciate an object.</p> <p>Example</p> <pre><code>    @classproperty\n    def my_class_property(cls):\n        return 5\n</code></pre>"},{"location":"api/graphium.utils.html#dictionary-of-tensors","title":"Dictionary of Tensors","text":""},{"location":"api/graphium.utils.html#graphium.utils.dict_tensor","title":"<code>graphium.utils.dict_tensor</code>","text":""},{"location":"api/graphium.utils.html#graphium.utils.dict_tensor.DictTensor","title":"<code>DictTensor</code>","text":"<p>         Bases: <code>dict</code></p> <p>A class that combines the functionality of <code>dict</code> and <code>torch.Tensor</code>. Specifically, it is a dict of Tensor, but it has all the methods and attributes of a Tensor. When a given method or attribute is called, it will be called on each element of the dict, and a new dict is returned.</p> <p>All methods from <code>torch.Tensor</code> and other functions are expected to work on <code>DictTensor</code></p> with the following rules <ul> <li>The function receives one <code>DictTensor</code>: The function will be applied on   all values of the dictionary. Examples are <code>torch.sum</code> or <code>torch.abs</code>.</li> <li>The function receives two <code>DictTensor</code>: The function will be applied on   each pair of Tensor with matching keys. If the keys don't match, an error   will be thrown. Example are operators such as <code>dict_tensor1 + dict_tensor2</code>   or <code>dict_tensor1 == dict_tensor2</code>.</li> </ul> <p>The output of the functions that act on the <code>torch.Tensor</code> level is a <code>dict[Any]</code>. If the output is a <code>dict[torch.Tensor]</code>, it is converted automatically to <code>DictTensor</code>.</p> <p>If a given method is available in both <code>dict</code> and <code>torch.Tensor</code>, then the one from <code>Tensor</code> is not available. Exceptions to the above rules are the <code>__dict__</code> and all comparison methods:</p> <ul> <li><code>__dict__</code>: Not available for this class.</li> <li><code>__lt__</code> or <code>&lt;</code>: Supports two-way comparison between <code>DictTensor</code> and number, <code>torch.Tensor</code> or <code>DictTensor</code>.</li> <li><code>__le__</code> or <code>&lt;=</code>: Supports two-way comparison between <code>DictTensor</code> and number, <code>torch.Tensor</code> or <code>DictTensor</code>.</li> <li><code>__eq__</code> or <code>==</code>: Supports two-way comparison between <code>DictTensor</code> and number, <code>torch.Tensor</code> or <code>DictTensor</code>.</li> <li><code>__ne__</code> or <code>!=</code>: Supports two-way comparison between <code>DictTensor</code> and number, <code>torch.Tensor</code> or <code>DictTensor</code>.</li> <li><code>__gt__</code> or <code>&gt;</code>: Supports two-way comparison between <code>DictTensor</code> and number, <code>torch.Tensor</code> or <code>DictTensor</code>.</li> <li><code>__ge__</code> or <code>&gt;=</code>: Supports two-way comparison between <code>DictTensor</code> and number, <code>torch.Tensor</code> or <code>DictTensor</code>.</li> </ul> <p>The only major function from <code>torch.Tensor</code> that doesn't work (to my knowledge) is the indexing. Indexing has to be done by manually looping the dictionary.</p> Example <pre><code># Summing a float to a DictTensor\ndict_ten = DictTensor({\n        \"a\": torch.zeros(5),\n        \"b\": torch.zeros(2, 3),\n        \"c\": torch.zeros(1, 2),\n    })\ndict_ten + 2\n&gt;&gt;\n{'a': tensor([2., 2., 2., 2., 2.]),\n'b': tensor([[2., 2., 2.],\n        [2., 2., 2.]]),\n'c': tensor([[2., 2.]])}\n\n# Summing a DictTensor to a DictTensor\ndict_ten = DictTensor({\n        \"a\": torch.zeros(5),\n        \"b\": torch.zeros(2, 3) + 0.1,\n        \"c\": torch.zeros(1, 2) + 0.5,\n    })\ndict_ten2 = DictTensor({\n        \"a\": torch.zeros(5) + 0.2,\n        \"b\": torch.zeros(2, 3) + 0.3,\n        \"c\": torch.zeros(1, 2) + 0.4,\n    })\ndict_ten + dict_ten2\n&gt;&gt;\n{'a': tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000]),\n'b': tensor([[0.4000, 0.4000, 0.4000],\n        [0.4000, 0.4000, 0.4000]]),\n'c': tensor([[0.9000, 0.9000]])}\n\n\n# Summing a accross the first axis\ndict_ten = DictTensor({\n        \"a\": torch.zeros(5) + 0.1,\n        \"b\": torch.zeros(2, 3) + 0.2,\n        \"c\": torch.zeros(1, 2) + 0.5,\n    })\ndict_ten.sum(axis=0)\n&gt;&gt;\n{'a': tensor(0.5000),\n'b': tensor([0.4000, 0.4000, 0.4000]),\n'c': tensor([0.5000, 0.5000])}\n\n# Getting the shape of each tensor\ndict_ten = DictTensor({\n        \"a\": torch.zeros(5) + 0.1,\n        \"b\": torch.zeros(2, 3) + 0.2,\n        \"c\": torch.zeros(1, 2) + 0.5,\n    })\ndict_ten.shape\n&gt;&gt;\n{'a': torch.Size([5]), 'b': torch.Size([2, 3]), 'c': torch.Size([1, 2])}\n</code></pre>"},{"location":"api/graphium.utils.html#graphium.utils.dict_tensor.DictTensor.__init__","title":"<code>__init__(dic)</code>","text":"<p>Take a dictionary of <code>torch.Tensors</code>, and transform it into a <code>DictTensor</code>. Register all the required methods from <code>torch.Tensors</code>, but modify them to work on dictionary instead.</p>"},{"location":"api/graphium.utils.html#graphium.utils.dict_tensor.DictTensor.apply","title":"<code>apply(func, *args, **kwargs)</code>","text":"<p>Apply a function on every Tensor \"value\" of the current dictionary</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>function to be called on each Tensor</p> required <code>*args,</code> <code>**kwargs</code> <p>Additional parameters to the function. Note that the Tensor should be the first input to the function.</p> required <p>Returns:</p> Type Description <code>Union[DictTensor, Dict[Any]]</code> <p><code>DictTensor</code> if the output of the function is a <code>torch.Tensor</code>,</p> <code>Union[DictTensor, Dict[Any]]</code> <p>or <code>dict[X]</code> if the output of the function is of type<code>X</code>.</p>"},{"location":"api/graphium.utils.html#file-system","title":"File System","text":""},{"location":"api/graphium.utils.html#graphium.utils.fs","title":"<code>graphium.utils.fs</code>","text":""},{"location":"api/graphium.utils.html#graphium.utils.fs.copy","title":"<code>copy(source, destination, chunk_size=None, force=False, progress=False, leave_progress=True)</code>","text":"<p>Copy one file to another location across different filesystem (local, S3, GCS, etc).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, os.PathLike, io.IOBase, fsspec.core.OpenFile]</code> <p>path or file-like object to copy from.</p> required <code>destination</code> <code>Union[str, os.PathLike, io.IOBase, fsspec.core.OpenFile]</code> <p>path or file-like object to copy to.</p> required <code>chunk_size</code> <code>int</code> <p>the chunk size to use. If progress is enabled the chunk size is <code>None</code>, it is set to 2048.</p> <code>None</code> <code>force</code> <code>bool</code> <p>whether to overwrite the destination file it it exists.</p> <code>False</code> <code>progress</code> <code>bool</code> <p>whether to display a progress bar.</p> <code>False</code> <code>leave_progress</code> <code>bool</code> <p>whether to hide the progress bar once the copy is done.</p> <code>True</code>"},{"location":"api/graphium.utils.html#graphium.utils.fs.exists","title":"<code>exists(path)</code>","text":"<p>Check whether a file exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, os.PathLike, fsspec.core.OpenFile, io.IOBase]</code> <p>a path supported by <code>fsspec</code> such as local, s3, gcs, etc.</p> required"},{"location":"api/graphium.utils.html#graphium.utils.fs.exists_and_not_empty","title":"<code>exists_and_not_empty(path)</code>","text":"<p>Check whether a directory exists and is not empty.</p>"},{"location":"api/graphium.utils.html#graphium.utils.fs.get_basename","title":"<code>get_basename(path)</code>","text":"<p>Get the basename of a file or a folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, os.PathLike]</code> <p>a path supported by <code>fsspec</code> such as local, s3, gcs, etc.</p> required"},{"location":"api/graphium.utils.html#graphium.utils.fs.get_cache_dir","title":"<code>get_cache_dir(suffix=None, create=True)</code>","text":"<p>Get a local cache directory. You can append a suffix folder to it and optionnaly create the folder if it doesn't exist.</p>"},{"location":"api/graphium.utils.html#graphium.utils.fs.get_extension","title":"<code>get_extension(path)</code>","text":"<p>Get the extension of a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, os.PathLike]</code> <p>a path supported by <code>fsspec</code> such as local, s3, gcs, etc.</p> required"},{"location":"api/graphium.utils.html#graphium.utils.fs.get_mapper","title":"<code>get_mapper(path)</code>","text":"<p>Get the fsspec mapper.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, os.PathLike]</code> <p>a path supported by <code>fsspec</code> such as local, s3, gcs, etc.</p> required"},{"location":"api/graphium.utils.html#graphium.utils.fs.get_size","title":"<code>get_size(file)</code>","text":"<p>Get the size of a file given its path. Return None if the size can't be retrieved.</p>"},{"location":"api/graphium.utils.html#graphium.utils.fs.join","title":"<code>join(*paths)</code>","text":"<p>Join paths together. The first element determine the filesystem to use (and so the separator.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <p>a list of paths supported by <code>fsspec</code> such as local, s3, gcs, etc.</p> <code>()</code>"},{"location":"api/graphium.utils.html#graphium.utils.fs.mkdir","title":"<code>mkdir(path, exist_ok=True)</code>","text":"<p>Create directory including potential parents.</p>"},{"location":"api/graphium.utils.html#graphium.utils.fs.rm","title":"<code>rm(path, recursive=False, maxdepth=None)</code>","text":"<p>Delete a file or a directory with all nested files.</p>"},{"location":"api/graphium.utils.html#hashing","title":"Hashing","text":""},{"location":"api/graphium.utils.html#graphium.utils.hashing","title":"<code>graphium.utils.hashing</code>","text":""},{"location":"api/graphium.utils.html#graphium.utils.hashing.get_md5_hash","title":"<code>get_md5_hash(object)</code>","text":"<p>MD5 hash of any object. The object is converted to a YAML string before being hashed. This allows for nested dictionaries/lists and for hashing of classes and their attributes.</p>"},{"location":"api/graphium.utils.html#moving-average-tracker","title":"Moving Average Tracker","text":""},{"location":"api/graphium.utils.html#graphium.utils.moving_average_tracker","title":"<code>graphium.utils.moving_average_tracker</code>","text":""},{"location":"api/graphium.utils.html#mup","title":"MUP","text":""},{"location":"api/graphium.utils.html#graphium.utils.mup","title":"<code>graphium.utils.mup</code>","text":""},{"location":"api/graphium.utils.html#graphium.utils.mup.apply_infshapes","title":"<code>apply_infshapes(model, infshapes)</code>","text":"<p>Modified from the regular <code>mup.apply_infshapes</code> by explicitly adding <code>base_dim</code> to the <code>MuReadoutGraphium</code>. This allows the code to work on IPUs.</p>"},{"location":"api/graphium.utils.html#graphium.utils.mup.set_base_shapes","title":"<code>set_base_shapes(model, base, rescale_params=True, delta=None, savefile=None, do_assert=True)</code>","text":"<p>Sets the <code>p.infshape</code> attribute for each parameter <code>p</code> of <code>model</code>.</p> <p>Code taken from the <code>mup</code> package from Microsoft https://github.com/microsoft/mup. No change except in the <code>apply_inf_shapes</code>, using the one from Graphium instead of <code>mup</code></p> Inputs <p>model: nn.Module instance base: The base model.     Can be nn.Module, a dict of shapes, a str, or None.     If None, then defaults to <code>model</code>     If str, then treated as filename for yaml encoding of a dict of base shapes. rescale_params:     assuming the model is initialized using the default pytorch init (or     He initialization etc that scale the same way with fanin): If True     (default), rescales parameters to have the correct (\u03bcP) variances. do_assert:</p> Output <p>same object as <code>model</code>, after setting the <code>infshape</code> attribute of each parameter.</p>"},{"location":"api/graphium.utils.html#read-file","title":"Read File","text":""},{"location":"api/graphium.utils.html#graphium.utils.read_file","title":"<code>graphium.utils.read_file</code>","text":"<p>Utiles for data parsing</p>"},{"location":"api/graphium.utils.html#graphium.utils.read_file.file_opener","title":"<code>file_opener(filename, mode='r')</code>","text":"<p>File reader stream</p>"},{"location":"api/graphium.utils.html#graphium.utils.read_file.parse_sdf_to_dataframe","title":"<code>parse_sdf_to_dataframe(sdf_path, as_cxsmiles=True, skiprows=None)</code>","text":"<p>Allows to read an SDF file containing molecular informations, convert it to a pandas DataFrame and convert the molecules to SMILES. It also lists a warning of all the molecules that couldn't be read.</p>"},{"location":"api/graphium.utils.html#graphium.utils.read_file.parse_sdf_to_dataframe--arguments","title":"Arguments","text":"<pre><code>sdf_path: str\n    The full path and name of the sdf file to read\nas_cxsmiles: bool, optional\n    Whether to use the CXSMILES notation, which preserves atomic coordinates,\n    stereocenters, and much more.\n    See `https://dl.chemaxon.com/marvin-archive/latest/help/formats/cxsmiles-doc.html`\n    (Default = True)\nskiprows: int, list\n    The rows to skip from dataset. The enumerate index starts from 1 insted of 0.\n    (Default = None)\n</code></pre>"},{"location":"api/graphium.utils.html#graphium.utils.read_file.read_file","title":"<code>read_file(filepath, as_ext=None, **kwargs)</code>","text":"<p>Allow to read different file format and parse them into a MolecularDataFrame. Supported formats are: * csv (.csv, .smile, .smiles, .tsv) * txt (.txt) * xls (.xls, .xlsx, .xlsm, .xls*) * sdf (.sdf) * pkl (.pkl)</p>"},{"location":"api/graphium.utils.html#graphium.utils.read_file.read_file--arguments","title":"Arguments","text":"<pre><code>filepath: str\n    The full path and name of the file to read.\n    It also supports the s3 url path.\nas_ext: str, Optional\n    The file extension used to read the file. If None, the extension is deduced\n    from the extension of the file. Otherwise, no matter the file extension,\n    the file will be read according to the specified ``as_ext``.\n    (Default=None)\n**kwargs: All the optional parameters required for the desired file reader.\n</code></pre> <p>TODO: unit test to make sure it works well with all extensions</p>"},{"location":"api/graphium.utils.html#graphium.utils.read_file.read_file--returns","title":"Returns","text":"<pre><code>df: pandas.DataFrame\n    The ``pandas.DataFrame`` containing the parsed data\n</code></pre>"},{"location":"api/graphium.utils.html#safe-run","title":"Safe Run","text":""},{"location":"api/graphium.utils.html#graphium.utils.safe_run","title":"<code>graphium.utils.safe_run</code>","text":""},{"location":"api/graphium.utils.html#graphium.utils.safe_run.SafeRun","title":"<code>SafeRun</code>","text":""},{"location":"api/graphium.utils.html#graphium.utils.safe_run.SafeRun.__enter__","title":"<code>__enter__()</code>","text":"<p>Print that the with-statement started, if <code>self.verbose &gt;= 2</code></p>"},{"location":"api/graphium.utils.html#graphium.utils.safe_run.SafeRun.__exit__","title":"<code>__exit__(type, value, traceback)</code>","text":"<p>Handle the error. Raise it if <code>self.raise_error==True</code>, otherwise ignore it and print it if <code>self.verbose &gt;= 1</code>. Also print that the with-statement is completed if <code>self.verbose &gt;= 2</code>.</p>"},{"location":"api/graphium.utils.html#graphium.utils.safe_run.SafeRun.__init__","title":"<code>__init__(name, raise_error=True, verbose=2)</code>","text":"<p>Run some code with error handling and some printing, using the with statment.</p> Example <p>In the example below, the <code>2+None</code>, an error will be caught and printed. <pre><code>with SafeRun(name=\"Addition that fails\", raise_error=False):\n    2 + None\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the code, used for printing</p> required <code>raise_error</code> <code>bool</code> <p>Whether to raise an error, or to catch it and print instead</p> <code>True</code> <code>verbose</code> <code>int</code> <p>The level of verbosity 0: Do not print anything 1: Print only the traced stack when an error is caught and <code>raise_error</code> is False 2: Print headers and footers at the start and exit of the with statement.</p> <code>2</code>"},{"location":"api/graphium.utils.html#spaces","title":"Spaces","text":""},{"location":"api/graphium.utils.html#graphium.utils.spaces","title":"<code>graphium.utils.spaces</code>","text":""},{"location":"api/graphium.utils.html#tensor","title":"Tensor","text":""},{"location":"api/graphium.utils.html#graphium.utils.tensor","title":"<code>graphium.utils.tensor</code>","text":""},{"location":"api/graphium.utils.html#graphium.utils.tensor.ModuleListConcat","title":"<code>ModuleListConcat</code>","text":"<p>         Bases: <code>torch.nn.ModuleList</code></p> <p>A list of neural modules similar to <code>torch.nn.ModuleList</code>, but where the modules are applied on the same input and concatenated together, instead of being applied sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The dimension for the concatenation</p> <code>-1</code>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.ModuleListConcat.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Apply all layers on the <code>args</code> and <code>kwargs</code>, and concatenate their output alongside the dimension <code>self.dim</code>.</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.ModuleWrap","title":"<code>ModuleWrap</code>","text":"<p>         Bases: <code>torch.nn.Module</code></p> <p>Wrap a function into a <code>torch.nn.Module</code>, with possible <code>*args</code> and <code>**kwargs</code></p> <p>Parameters:</p> Name Type Description Default <code>func</code> <p>function to wrap into a module</p> required"},{"location":"api/graphium.utils.html#graphium.utils.tensor.ModuleWrap.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Calls the function <code>self.func</code> with the arguments <code>self.func(*self.args, *args, **self.kwargs, **kwargs)</code></p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.arg_in_func","title":"<code>arg_in_func(fn, arg)</code>","text":"<p>Check if a function takes the given argument.</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.arg_in_func--parameters","title":"Parameters","text":"func <p>The function to check the argument.</p> str <p>The name of the argument.</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.arg_in_func--returns","title":"Returns","text":"<pre><code>res: bool\n    True if the function contains the argument, otherwise False.\n</code></pre>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.is_device_cuda","title":"<code>is_device_cuda(device, ignore_errors=False)</code>","text":"<p>Check wheter the given device is a cuda device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>torch.device</code> <p>str, torch.device object to check for cuda</p> required <code>ignore_errors</code> <code>bool</code> <p>bool Whether to ignore the error if the device is not recognized. Otherwise, <code>False</code> is returned in case of errors.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>is_cuda</code> <code>bool</code> <p>bool</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.is_dtype_numpy_array","title":"<code>is_dtype_numpy_array(dtype)</code>","text":"<p>Verify if the dtype is a numpy dtype</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>Union[np.dtype, torch.dtype]</code> <p>dtype The dtype of a value. E.g. np.int32, str, torch.float</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean saying if the dtype is a numpy dtype</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.is_dtype_torch_tensor","title":"<code>is_dtype_torch_tensor(dtype)</code>","text":"<p>Verify if the dtype is a torch dtype</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>Union[np.dtype, torch.dtype]</code> <p>dtype The dtype of a value. E.g. np.int32, str, torch.float</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A boolean saying if the dtype is a torch dtype</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.nan_mad","title":"<code>nan_mad(input, normal=True, **kwargs)</code>","text":"<p>Return the median absolute deviation of all elements, while ignoring the NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>normal</code> <code>bool</code> <p>whether to multiply the result by 1.4826 to mimic the standard deviation for normal distributions.</p> <code>True</code> <code>dim</code> <code>int or tuple(int</code> <p>The dimension or dimensions to reduce.</p> required <code>keepdim</code> <code>bool</code> <p>whether the output tensor has dim retained or not.</p> required <code>dtype</code> <code>torch.dtype</code> <p>The desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>The resulting median absolute deviation of the tensor</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.nan_mean","title":"<code>nan_mean(input, *args, **kwargs)</code>","text":"<p>Return the mean of all elements, while ignoring the NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int or tuple(int</code> <p>The dimension or dimensions to reduce.</p> required <code>keepdim</code> <code>bool</code> <p>whether the output tensor has dim retained or not.</p> required <code>dtype</code> <code>torch.dtype</code> <p>The desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>The resulting mean of the tensor</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.nan_median","title":"<code>nan_median(input, **kwargs)</code>","text":"<p>Return the median of all elements, while ignoring the NaNs. Contrarily to <code>torch.nanmedian</code>, this function supports a list of dimensions, or <code>dim=None</code>, and does not return the index of the median</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int or tuple(int</code> <p>The dimension or dimensions to reduce.</p> required <code>keepdim</code> <code>bool</code> <p>whether the output tensor has dim retained or not.</p> required <code>dtype</code> <code>torch.dtype</code> <p>The desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>The resulting median of the tensor. Contrarily to <code>torch.median</code>, it does not return the index of the median</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.nan_std","title":"<code>nan_std(input, unbiased=True, **kwargs)</code>","text":"<p>Return the standard deviation of all elements, while ignoring the NaNs. If unbiased is True, Bessel\u2019s correction will be used. Otherwise, the sample deviation is calculated, without any correction.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>unbiased</code> <code>bool</code> <p>whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1).</p> <code>True</code> <code>dim</code> <code>int or tuple(int</code> <p>The dimension or dimensions to reduce.</p> required <code>keepdim</code> <code>bool</code> <p>whether the output tensor has dim retained or not.</p> required <code>dtype</code> <code>torch.dtype</code> <p>The desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>The resulting standard deviation of the tensor</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.nan_var","title":"<code>nan_var(input, unbiased=True, **kwargs)</code>","text":"<p>Return the variace of all elements, while ignoring the NaNs. If unbiased is True, Bessel\u2019s correction will be used. Otherwise, the sample deviation is calculated, without any correction.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>unbiased</code> <code>bool</code> <p>whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1).</p> <code>True</code> <code>dim</code> <code>int or tuple(int</code> <p>The dimension or dimensions to reduce.</p> required <code>keepdim</code> <code>bool</code> <p>whether the output tensor has dim retained or not.</p> required <code>dtype</code> <code>torch.dtype</code> <p>The desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Tensor</code> <p>The resulting variance of the tensor</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.one_of_k_encoding","title":"<code>one_of_k_encoding(val, classes)</code>","text":"<p>Converts a single value to a one-hot vector.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Any</code> <p>int class to be converted into a one hot vector (integers from 0 to num_classes).</p> required <code>num_classes</code> <p>iterator a list or 1D array of allowed choices for val to take</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>A list of length len(num_classes) + 1</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.parse_valid_args","title":"<code>parse_valid_args(param_dict, fn)</code>","text":"<p>Check if a function takes the given argument.</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.parse_valid_args--parameters","title":"Parameters","text":"func <p>The function to check the argument.</p> dict <p>Dictionary of the argument.</p>"},{"location":"api/graphium.utils.html#graphium.utils.tensor.parse_valid_args--returns","title":"Returns","text":"<pre><code>param_dict: dict\n    Valid paramter dictionary for the given fucntions.\n</code></pre>"},{"location":"api/graphium.visualization.html","title":"graphium.visualization","text":"<p>Code for visualizing graphs</p>"},{"location":"api/graphium.visualization.html#graphium.visualization.vis_utils","title":"<code>graphium.visualization.vis_utils</code>","text":""},{"location":"api/graphium.nn/architectures.html","title":"graphium.nn.architectures","text":"<p>High level architectures in the library</p> Contents <ul> <li>Global Architectures</li> <li>PyG Architectures</li> <li>Encoder Manager</li> </ul>"},{"location":"api/graphium.nn/architectures.html#global-architectures","title":"Global Architectures","text":""},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures","title":"<code>graphium.nn.architectures.global_architectures</code>","text":""},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardGraph","title":"<code>FeedForwardGraph</code>","text":"<p>         Bases: <code>FeedForwardNN</code></p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardGraph.__init__","title":"<code>__init__(in_dim, out_dim, hidden_dims, layer_type, depth=None, activation='relu', last_activation='none', dropout=0.0, last_dropout=0.0, normalization='none', first_normalization='none', last_normalization='none', residual_type='none', residual_skip_steps=1, in_dim_edges=0, hidden_dims_edges=[], name='GNN', layer_kwargs=None, virtual_node='none', use_virtual_edges=False, last_layer_is_readout=False)</code>","text":"<p>A flexible neural network architecture, with variable hidden dimensions, support for multiple layer types, and support for different residual connections.</p> <p>This class is meant to work with different graph neural networks layers. Any layer must inherit from <code>graphium.nn.base_graph_layer.BaseGraphStructure</code> or <code>graphium.nn.base_graph_layer.BaseGraphLayer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the layer</p> required <code>hidden_dims</code> <code>Union[List[int], int]</code> <p>List of dimensions in the hidden layers. Be careful, the \"simple\" residual type only supports hidden dimensions of the same value.</p> required <code>layer_type</code> <code>Union[str, nn.Module]</code> <p>Type of layer to use. Can be a string or nn.Module.</p> required <code>depth</code> <code>Optional[int]</code> <p>If <code>hidden_dims</code> is an integer, <code>depth</code> is 1 + the number of hidden layers to use. If <code>hidden_dims</code> is a <code>list</code>, <code>depth</code> must be <code>None</code>.</p> <code>None</code> <code>activation</code> <code>Union[str, Callable]</code> <p>activation function to use in the hidden layers.</p> <code>'relu'</code> <code>last_activation</code> <code>Union[str, Callable]</code> <p>activation function to use in the last layer.</p> <code>'none'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>last_dropout</code> <code>float</code> <p>The ratio of units to dropout for the last layer. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>first_normalization</code> <code>Union[str, Callable]</code> <p>Whether to use batch normalization before the first layer</p> <code>'none'</code> <code>last_normalization</code> <code>Union[str, Callable]</code> <p>Whether to use batch normalization in the last layer</p> <code>'none'</code> <code>residual_type</code> <code>str</code> <ul> <li>\"none\": No residual connection</li> <li>\"simple\": Residual connection similar to the ResNet architecture.   See class <code>ResidualConnectionSimple</code></li> <li>\"weighted\": Residual connection similar to the Resnet architecture,   but with weights applied before the summation. See class <code>ResidualConnectionWeighted</code></li> <li>\"concat\": Residual connection where the residual is concatenated instead   of being added.</li> <li>\"densenet\": Residual connection where the residual of all previous layers   are concatenated. This leads to a strong increase in the number of parameters   if there are multiple hidden layers.</li> </ul> <code>'none'</code> <code>residual_skip_steps</code> <code>int</code> <p>The number of steps to skip between each residual connection. If <code>1</code>, all the layers are connected. If <code>2</code>, half of the layers are connected.</p> <code>1</code> <code>in_dim_edges</code> <code>int</code> <p>Input edge-feature dimensions of the network. Keep at 0 if not using edge features, or if the layer doesn't support edges.</p> <code>0</code> <code>hidden_dims_edges</code> <code>List[int]</code> <p>Hidden dimensions for the edges. Most models don't support it, so it should only be used for those that do, i.e. <code>GatedGCNLayer</code></p> <code>[]</code> <code>name</code> <code>str</code> <p>Name attributed to the current network, for display and printing purposes.</p> <code>'GNN'</code> <code>layer_type</code> <code>Union[str, nn.Module]</code> <p>The type of layers to use in the network. A class that inherits from <code>graphium.nn.base_graph_layer.BaseGraphStructure</code>, or one of the following strings</p> <ul> <li>\"pyg:gin\": GINConvPyg</li> <li>\"pyg:gine\": GINEConvPyg</li> <li>\"pyg:gated-gcn\": GatedGCNPyg</li> <li>\"pyg:pna-msgpass\": PNAMessagePassingPyg</li> </ul> required <code>layer_kwargs</code> <code>Optional[Dict]</code> <p>The arguments to be used in the initialization of the layer provided by <code>layer_type</code></p> <code>None</code> <code>virtual_node</code> <code>str</code> <p>A string associated to the type of virtual node to use, either <code>None</code>, \"none\", \"mean\", \"sum\", \"max\", \"logsum\". See <code>graphium.nn.pooling_pyg.VirtualNode</code>.</p> <p>The virtual node will not use any residual connection if <code>residual_type</code> is \"none\". Otherwise, it will use a simple ResNet like residual connection.</p> <code>'none'</code> <code>use_virtual_edges</code> <code>bool</code> <p>A bool flag used to select if the virtual node should use the edges or not</p> <code>False</code> <code>last_layer_is_readout</code> <code>bool</code> <p>Whether the last layer should be treated as a readout layer. Allows to use the <code>mup.MuReadout</code> from the muTransfer method https://github.com/microsoft/mup</p> <code>False</code>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardGraph.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardGraph.forward","title":"<code>forward(g)</code>","text":"<p>Apply the full graph neural network on the input graph and node features.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Batch</code> <p>pyg Batch graph on which the convolution is done with the keys:</p> <ul> <li> <p><code>\"feat\"</code>: torch.Tensor[..., N, Din]   Node feature tensor, before convolution.   <code>N</code> is the number of nodes, <code>Din</code> is the input features</p> </li> <li> <p><code>\"edge_feat\"</code> (torch.Tensor[..., N, Ein]):   Edge feature tensor, before convolution.   <code>N</code> is the number of nodes, <code>Ein</code> is the input edge features</p> </li> </ul> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p><code>torch.Tensor[..., M, Dout]</code> or <code>torch.Tensor[..., N, Dout]</code>: Node or graph feature tensor, after the network. <code>N</code> is the number of nodes, <code>M</code> is the number of graphs, <code>Dout</code> is the output dimension <code>self.out_dim</code> If the <code>self.pooling</code> is [<code>None</code>], then it returns node features and the output dimension is <code>N</code>, otherwise it returns graph features and the output dimension is <code>M</code></p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardGraph.get_init_kwargs","title":"<code>get_init_kwargs()</code>","text":"<p>Get a dictionary that can be used to instanciate a new object with identical parameters.</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardGraph.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width. factor_in_dim: Whether to factor the input dimension for the nodes</p> <p>Returns:</p> Name Type Description <code>kwargs</code> <code>Dict[str, Any]</code> <p>Dictionary of parameters to be used to instanciate the base model divided by the factor</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardNN","title":"<code>FeedForwardNN</code>","text":"<p>         Bases: <code>nn.Module</code>, <code>MupMixin</code></p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardNN.__init__","title":"<code>__init__(in_dim, out_dim, hidden_dims, depth=None, activation='relu', last_activation='none', dropout=0.0, last_dropout=0.0, normalization='none', first_normalization='none', last_normalization='none', residual_type='none', residual_skip_steps=1, name='LNN', layer_type='fc', layer_kwargs=None, last_layer_is_readout=False)</code>","text":"<p>A flexible neural network architecture, with variable hidden dimensions, support for multiple layer types, and support for different residual connections.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the layer</p> required <code>hidden_dims</code> <code>Union[List[int], int]</code> <p>Either an integer specifying all the hidden dimensions, or a list of dimensions in the hidden layers. Be careful, the \"simple\" residual type only supports hidden dimensions of the same value.</p> required <code>depth</code> <code>Optional[int]</code> <p>If <code>hidden_dims</code> is an integer, <code>depth</code> is 1 + the number of hidden layers to use. If <code>hidden_dims</code> is a list, then <code>depth</code> must be <code>None</code> or equal to <code>len(hidden_dims) + 1</code></p> <code>None</code> <code>activation</code> <code>Union[str, Callable]</code> <p>activation function to use in the hidden layers.</p> <code>'relu'</code> <code>last_activation</code> <code>Union[str, Callable]</code> <p>activation function to use in the last layer.</p> <code>'none'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>last_dropout</code> <code>float</code> <p>The ratio of units to dropout for the last_layer. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>first_normalization</code> <code>Union[str, Callable]</code> <p>Whether to use batch normalization before the first layer</p> <code>'none'</code> <code>last_normalization</code> <code>Union[str, Callable]</code> <p>Whether to use batch normalization in the last layer</p> <code>'none'</code> <code>residual_type</code> <code>str</code> <ul> <li>\"none\": No residual connection</li> <li>\"simple\": Residual connection similar to the ResNet architecture.   See class <code>ResidualConnectionSimple</code></li> <li>\"weighted\": Residual connection similar to the Resnet architecture,   but with weights applied before the summation. See class <code>ResidualConnectionWeighted</code></li> <li>\"concat\": Residual connection where the residual is concatenated instead   of being added.</li> <li>\"densenet\": Residual connection where the residual of all previous layers   are concatenated. This leads to a strong increase in the number of parameters   if there are multiple hidden layers.</li> </ul> <code>'none'</code> <code>residual_skip_steps</code> <code>int</code> <p>The number of steps to skip between each residual connection. If <code>1</code>, all the layers are connected. If <code>2</code>, half of the layers are connected.</p> <code>1</code> <code>name</code> <code>str</code> <p>Name attributed to the current network, for display and printing purposes.</p> <code>'LNN'</code> <code>layer_type</code> <code>Union[str, nn.Module]</code> <p>The type of layers to use in the network. Either \"fc\" as the <code>FCLayer</code>, or a class representing the <code>nn.Module</code> to use.</p> <code>'fc'</code> <code>layer_kwargs</code> <code>Optional[Dict]</code> <p>The arguments to be used in the initialization of the layer provided by <code>layer_type</code></p> <code>None</code> <code>last_layer_is_readout</code> <code>bool</code> <p>Whether the last layer should be treated as a readout layer. Allows to use the <code>mup.MuReadout</code> from the muTransfer method https://github.com/microsoft/mup</p> <code>False</code>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardNN.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardNN.forward","title":"<code>forward(h)</code>","text":"<p>Apply the neural network on the input features.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>torch.Tensor</code> <p><code>torch.Tensor[..., Din]</code>: Input feature tensor, before the network. <code>Din</code> is the number of input features</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p><code>torch.Tensor[..., Dout]</code>: Output feature tensor, after the network. <code>Dout</code> is the number of output features</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardNN.get_init_kwargs","title":"<code>get_init_kwargs()</code>","text":"<p>Get a dictionary that can be used to instanciate a new object with identical parameters.</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FeedForwardNN.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width. factor_in_dim: Whether to factor the input dimension</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork","title":"<code>FullGraphMultiTaskNetwork</code>","text":"<p>         Bases: <code>nn.Module</code>, <code>MupMixin</code></p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.in_dim","title":"<code>in_dim: int</code>  <code>property</code>","text":"<p>Returns the input dimension of the network</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.in_dim_edges","title":"<code>in_dim_edges: int</code>  <code>property</code>","text":"<p>Returns the input edge dimension of the network</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.out_dim","title":"<code>out_dim: int</code>  <code>property</code>","text":"<p>Returns the output dimension of the network</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.out_dim_edges","title":"<code>out_dim_edges: int</code>  <code>property</code>","text":"<p>Returns the output dimension of the edges of the network.</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.__init__","title":"<code>__init__(gnn_kwargs, pre_nn_kwargs=None, pre_nn_edges_kwargs=None, pe_encoders_kwargs=None, task_heads_kwargs=None, graph_output_nn_kwargs=None, accelerator_kwargs=None, num_inference_to_average=1, last_layer_is_readout=False, name='FullGNN')</code>","text":"<p>Class that allows to implement a full graph neural network architecture, including the pre-processing MLP and the post processing MLP.</p> <p>Parameters:</p> Name Type Description Default <code>gnn_kwargs</code> <code>Dict[str, Any]</code> <p>key-word arguments to use for the initialization of the pre-processing GNN network using the class <code>FeedForwardGraph</code>. It must respect the following criteria:</p> <ul> <li>gnn_kwargs[\"in_dim\"] must be equal to pre_nn_kwargs[\"out_dim\"]</li> <li>gnn_kwargs[\"out_dim\"] must be equal to graph_output_nn_kwargs[\"in_dim\"]</li> </ul> required <code>pe_encoders_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>key-word arguments to use for the initialization of all positional encoding encoders. See the class <code>EncoderManager</code> for more details.</p> <code>None</code> <code>pre_nn_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>key-word arguments to use for the initialization of the pre-processing MLP network of the node features before the GNN, using the class <code>FeedForwardNN</code>. If <code>None</code>, there won't be a pre-processing MLP.</p> <code>None</code> <code>pre_nn_edges_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>key-word arguments to use for the initialization of the pre-processing MLP network of the edge features before the GNN, using the class <code>FeedForwardNN</code>. If <code>None</code>, there won't be a pre-processing MLP.</p> <code>None</code> <code>task_heads_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>This argument is a list of dictionaries containing the arguments for task heads. Each argument is used to initialize a task-specific MLP.</p> <code>None</code> <code>graph_output_nn_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>This argument is a list of dictionaries corresponding to the arguments for a FeedForwardNN. Each dict of arguments is used to initialize a shared MLP.</p> <code>None</code> <code>accelerator_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>key-word arguments specific to the accelerator being used, e.g. pipeline split points</p> <code>None</code> <code>num_inference_to_average</code> <code>int</code> <p>Number of inferences to average at val/test time. This is used to avoid the noise introduced by positional encodings with sign-flips. In case no such encoding is given, this parameter is ignored. NOTE: The inference time will be slowed-down proportionaly to this parameter.</p> <code>1</code> <code>last_layer_is_readout</code> <code>bool</code> <p>Whether the last layer should be treated as a readout layer. Allows to use the <code>mup.MuReadout</code> from the muTransfer method https://github.com/microsoft/mup</p> <code>False</code> <code>name</code> <code>str</code> <p>Name attributed to the current network, for display and printing purposes.</p> <code>'FullGNN'</code>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.forward","title":"<code>forward(g)</code>","text":"<p>Apply the pre-processing neural network, the graph neural network, and the post-processing neural network on the graph features.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Batch</code> <p>pyg Batch graph on which the convolution is done. Must contain the following elements:</p> <ul> <li> <p>Node key <code>\"feat\"</code>: <code>torch.Tensor[..., N, Din]</code>.   Input node feature tensor, before the network.   <code>N</code> is the number of nodes, <code>Din</code> is the input features dimension <code>self.pre_nn.in_dim</code></p> </li> <li> <p>Edge key <code>\"edge_feat\"</code>: <code>torch.Tensor[..., N, Ein]</code> Optional.   The edge features to use. It will be ignored if the   model doesn't supporte edge features or if   <code>self.in_dim_edges==0</code>.</p> </li> <li> <p>Other keys related to positional encodings <code>\"pos_enc_feats_sign_flip\"</code>,   <code>\"pos_enc_feats_no_flip\"</code>.</p> </li> </ul> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor[..., M, Dout]</code> or <code>torch.Tensor[..., N, Dout]</code>: Node or graph feature tensor, after the network. <code>N</code> is the number of nodes, <code>M</code> is the number of graphs, <code>Dout</code> is the output dimension <code>self.graph_output_nn.out_dim</code> If the <code>self.gnn.pooling</code> is [<code>None</code>], then it returns node features and the output dimension is <code>N</code>, otherwise it returns graph features and the output dimension is <code>M</code></p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with the kwargs to create the base model.</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.FullGraphMultiTaskNetwork.set_max_num_nodes_edges_per_graph","title":"<code>set_max_num_nodes_edges_per_graph(max_nodes, max_edges)</code>","text":"<p>Set the maximum number of nodes and edges for all gnn layers and encoder layers</p> <p>Parameters:</p> Name Type Description Default <code>max_nodes</code> <code>Optional[int]</code> <p>Maximum number of nodes in the dataset. This will be useful for certain architecture, but ignored by others.</p> required <code>max_edges</code> <code>Optional[int]</code> <p>Maximum number of edges in the dataset. This will be useful for certain architecture, but ignored by others.</p> required"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN","title":"<code>GraphOutputNN</code>","text":"<p>         Bases: <code>nn.Module</code>, <code>MupMixin</code></p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.concat_last_layers","title":"<code>concat_last_layers: Optional[Iterable[int]]</code>  <code>writable</code> <code>property</code>","text":"<p>Property to control the output of the <code>self.forward</code>. If set to a list of integer, the <code>forward</code> function will concatenate the output of different layers.</p> <p>If set to <code>None</code>, the output of the last layer is returned.</p> <p>NOTE: The indexes are inverted. 0 is the last layer, 1 is the second last, etc.</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.out_dim","title":"<code>out_dim: int</code>  <code>property</code>","text":"<p>Returns the output dimension of the network</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.__init__","title":"<code>__init__(in_dim, in_dim_edges, task_level, graph_output_nn_kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>in_dim_edges</code> <code>int</code> <p>Input edge feature dimensions of the layer</p> required <code>task_level</code> <code>str</code> <p>graph/node/edge/nodepair depending on wether it is graph/node/edge/nodepair level task</p> required <code>graph_output_nn_kwargs</code> <code>Dict[str, Any]</code> <p>key-word arguments to use for the initialization of the post-processing MLP network after the GNN, using the class <code>FeedForwardNN</code>.</p> required"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.compute_nodepairs","title":"<code>compute_nodepairs(node_feats, batch, max_num_nodes=None, fill_value=float('nan'), batch_size=None, drop_nodes_last_graph=False)</code>","text":"<p>Vectorized implementation of nodepair-level task:</p> <p>Parameters:</p> Name Type Description Default <code>node_feats</code> <code>torch.Tensor</code> <p>Node features</p> required <code>batch</code> <code>torch.Tensor</code> <p>Batch vector</p> required <code>max_num_nodes</code> <code>int</code> <p>The maximum number of nodes per graph</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>The value for invalid entries in the resulting dense output tensor. (default: :obj:<code>NaN</code>)</p> <code>float('nan')</code> <code>batch_size</code> <code>int</code> <p>The batch size. (default: :obj:<code>None</code>)</p> <code>None</code> <code>drop_nodes_last_graph</code> <code>bool</code> <p>Whether to drop the nodes of the last graphs that exceed the <code>max_num_nodes_per_graph</code>. Useful when the last graph is a padding.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>result</code> <code>torch.Tensor</code> <p>concatenated node features of shape B * max_num_nodes * 2*h,</p> <code>torch.Tensor</code> <p>where B is number of graphs, max_num_nodes is the chosen maximum number nodes, and h is the feature dim</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.drop_graph_output_nn_layers","title":"<code>drop_graph_output_nn_layers(num_layers_to_drop)</code>","text":"<p>Remove the last layers of the model. Useful for Transfer Learning.</p> <p>Parameters:</p> Name Type Description Default <code>num_layers_to_drop</code> <code>int</code> <p>The number of layers to drop from the <code>self.graph_output_nn</code> network.</p> required"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.extend_graph_output_nn_layers","title":"<code>extend_graph_output_nn_layers(layers)</code>","text":"<p>Add layers at the end of the model. Useful for Transfer Learning.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>nn.ModuleList</code> <p>A ModuleList of all the layers to extend</p> required"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.forward","title":"<code>forward(g)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>g</code> <code>Batch</code> <p>pyg Batch graph</p> required <p>Returns:</p> Name Type Description <code>h</code> <p>Output features after applying graph_output_nn</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width. factor_in_dim: Whether to factor the input dimension</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with the kwargs to create the base model.</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.GraphOutputNN.set_max_num_nodes_edges_per_graph","title":"<code>set_max_num_nodes_edges_per_graph(max_nodes, max_edges)</code>","text":"<p>Set the maximum number of nodes and edges for all gnn layers and encoder layers</p> <p>Parameters:</p> Name Type Description Default <code>max_nodes</code> <code>Optional[int]</code> <p>Maximum number of nodes in the dataset. This will be useful for certain architecture, but ignored by others.</p> required <code>max_edges</code> <code>Optional[int]</code> <p>Maximum number of edges in the dataset. This will be useful for certain architecture, but ignored by others.</p> required"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.TaskHeads","title":"<code>TaskHeads</code>","text":"<p>         Bases: <code>nn.Module</code>, <code>MupMixin</code></p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.TaskHeads.out_dim","title":"<code>out_dim: Dict[str, int]</code>  <code>property</code>","text":"<p>Returns the output dimension of each task head</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.TaskHeads.__init__","title":"<code>__init__(in_dim, in_dim_edges, task_heads_kwargs, graph_output_nn_kwargs, last_layer_is_readout=True)</code>","text":"<p>Class that groups all multi-task output heads together to provide the task-specific outputs.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>in_dim_edges</code> <code>int</code> <p>Input edge feature dimensions of the layer</p> required <code>last_layer_is_readout</code> <code>bool</code> <p>Whether the last layer should be treated as a readout layer. Allows to use the <code>mup.MuReadout</code> from the muTransfer method</p> <code>True</code> <code>task_heads_kwargs</code> <code>Dict[str, Any]</code> <p>This argument is a list of dictionaries corresponding to the arguments for a FeedForwardNN. Each dict of arguments is used to initialize a task-specific MLP.</p> required <code>graph_output_nn_kwargs</code> <code>Dict[str, Any]</code> <p>key-word arguments to use for the initialization of the post-processing MLP network after the GNN, using the class <code>FeedForwardNN</code>.</p> required"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.TaskHeads.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a string representation of the task heads</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.TaskHeads.forward","title":"<code>forward(g)</code>","text":"<p>forward function of the task head</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Batch</code> <p>pyg Batch graph</p> required <p>Returns:</p> Name Type Description <code>task_head_outputs</code> <code>Dict[str, torch.Tensor]</code> <p>Return a dictionary: Dict[task_name, Tensor]</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.TaskHeads.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width. factor_in_dim: Whether to factor the input dimension</p> <p>Returns:</p> Name Type Description <code>kwargs</code> <code>Dict[str, Any]</code> <p>Dictionary of arguments to be used to initialize the base model</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.global_architectures.TaskHeads.set_max_num_nodes_edges_per_graph","title":"<code>set_max_num_nodes_edges_per_graph(max_nodes, max_edges)</code>","text":"<p>Set the maximum number of nodes and edges for all gnn layers and encoder layers</p> <p>Parameters:</p> Name Type Description Default <code>max_nodes</code> <code>Optional[int]</code> <p>Maximum number of nodes in the dataset. This will be useful for certain architecture, but ignored by others.</p> required <code>max_edges</code> <code>Optional[int]</code> <p>Maximum number of edges in the dataset. This will be useful for certain architecture, but ignored by others.</p> required"},{"location":"api/graphium.nn/architectures.html#pyg-architectures","title":"PyG Architectures","text":""},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.pyg_architectures","title":"<code>graphium.nn.architectures.pyg_architectures</code>","text":""},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.pyg_architectures.FeedForwardPyg","title":"<code>FeedForwardPyg</code>","text":"<p>         Bases: <code>FeedForwardGraph</code></p>"},{"location":"api/graphium.nn/architectures.html#encoder-manager","title":"Encoder Manager","text":""},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager","title":"<code>graphium.nn.architectures.encoder_manager</code>","text":""},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager","title":"<code>EncoderManager</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager.in_dims","title":"<code>in_dims: Iterable[int]</code>  <code>property</code>","text":"<p>Returns the input dimensions for all pe-encoders</p> <p>Returns:</p> Name Type Description <code>in_dims</code> <code>Iterable[int]</code> <p>the input dimensions for all pe-encoders</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager.input_keys","title":"<code>input_keys: Iterable[str]</code>  <code>property</code>","text":"<p>Returns the input keys for all pe-encoders</p> <p>Returns:</p> Name Type Description <code>input_keys</code> <code>Iterable[str]</code> <p>the input keys for all pe-encoders</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager.out_dim","title":"<code>out_dim: int</code>  <code>property</code>","text":"<p>Returns the output dimension of the pooled embedding from all the pe encoders</p> <p>Returns:</p> Name Type Description <code>out_dim</code> <code>int</code> <p>the output dimension of the pooled embedding from all the pe encoders</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager.__init__","title":"<code>__init__(pe_encoders_kwargs=None, max_num_nodes_per_graph=None, name='encoder_manager')</code>","text":"<p>Class that allows to runs multiple encoders in parallel and concatenate / pool their outputs.</p> <p>Parameters:</p> Name Type Description Default <code>pe_encoders_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>key-word arguments to use for the initialization of all positional encoding encoders can use the class PE_ENCODERS_DICT: \"la_encoder\"(tested) , \"mlp_encoder\" (not tested), \"signnet_encoder\" (not tested)</p> <code>None</code> <code>name</code> <code>str</code> <p>Name attributed to the current network, for display and printing purposes.</p> <code>'encoder_manager'</code>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager.forward","title":"<code>forward(g)</code>","text":"<p>forward pass of the pe encoders and pooling</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Batch</code> <p>ptg Batch on which the convolution is done. Must contain the following elements:</p> <ul> <li> <p>Node key <code>\"feat\"</code>: <code>torch.Tensor[..., N, Din]</code>.   Input node feature tensor, before the network.   <code>N</code> is the number of nodes, <code>Din</code> is the input features dimension <code>self.pre_nn.in_dim</code></p> </li> <li> <p>Edge key <code>\"edge_feat\"</code>: <code>torch.Tensor[..., N, Ein]</code> Optional.   The edge features to use. It will be ignored if the   model doesn't supporte edge features or if   <code>self.in_dim_edges==0</code>.</p> </li> <li> <p>Other keys related to positional encodings <code>\"pos_enc_feats_sign_flip\"</code>,   <code>\"pos_enc_feats_no_flip\"</code>.</p> </li> </ul> required <p>Returns:</p> Name Type Description <code>g</code> <code>Batch</code> <p>pyg Batch with the positional encodings added to the graph</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager.forward_positional_encoding","title":"<code>forward_positional_encoding(g)</code>","text":"<p>Forward pass for the positional encodings (PE), with each PE having it's own encoder defined in <code>self.pe_encoders</code>. All the positional encodings with the same keys are pooled together using <code>self.pe_pooling</code>.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Batch</code> <p>pyg Batch containing the node positional encodings</p> required <p>Returns:</p> Name Type Description <code>pe_node_pooled</code> <code>Dict[str, Tensor]</code> <p>The positional / structural encodings go through</p> <code>Dict[str, Tensor]</code> <p>encoders, then are pooled together according to their keys.</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager.forward_simple_pooling","title":"<code>forward_simple_pooling(h, pooling, dim)</code>","text":"<p>Apply sum, mean, or max pooling on a Tensor.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>Tensor</code> <p>the Tensor to pool</p> required <code>pooling</code> <code>str</code> <p>string specifiying the pooling method</p> required <code>dim</code> <code>int</code> <p>the dimension to pool over</p> required <p>Returns:</p> Name Type Description <code>pooled</code> <code>Tensor</code> <p>the pooled Tensor</p>"},{"location":"api/graphium.nn/architectures.html#graphium.nn.architectures.encoder_manager.EncoderManager.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width.</p> <p>Returns:</p> Name Type Description <code>pe_kw</code> <code>Dict[str, Any]</code> <p>the model kwargs where the dimensions are divided by the factor</p>"},{"location":"api/graphium.nn/encoders.html","title":"graphium.nn.encoders","text":"<p>Implementations of positional encoders in the library</p> Contents <ul> <li>Base Encoder</li> <li>Gaussian Kernal Positional Encoder</li> <li>Laplacian Positional Encoder</li> <li>MLP Encoder</li> <li>Signnet Positional Encoder</li> </ul>"},{"location":"api/graphium.nn/encoders.html#base-encoder","title":"Base Encoder","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.base_encoder","title":"<code>graphium.nn.encoders.base_encoder</code>","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.base_encoder.BaseEncoder","title":"<code>BaseEncoder</code>","text":"<p>         Bases: <code>torch.nn.Module</code>, <code>MupMixin</code></p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.base_encoder.BaseEncoder.__init__","title":"<code>__init__(input_keys, output_keys, in_dim, out_dim, num_layers, activation='relu', first_normalization=None, use_input_keys_prefix=True)</code>","text":"<p>Base class for all positional and structural encoders. Initialize the encoder with the following arguments:</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>The keys from the graph to use as input</p> required <code>output_keys</code> <code>List[str]</code> <p>The keys to return as output encodings</p> required <code>in_dim</code> <code>int</code> <p>The input dimension for the encoder</p> required <code>out_dim</code> <code>int</code> <p>The output dimension of the encodings</p> required <code>num_layers</code> <code>int</code> <p>The number of layers of the encoder</p> required <code>activation</code> <code>Union[str, Callable]</code> <p>The activation function to use</p> <code>'relu'</code> <code>first_normalization</code> <p>The normalization to use before the first layer</p> <code>None</code> <code>use_input_keys_prefix</code> <code>bool</code> <p>Whether to use the <code>key_prefix</code> argument in the <code>forward</code> method.</p> <code>True</code>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.base_encoder.BaseEncoder.forward","title":"<code>forward(graph, key_prefix=None)</code>  <code>abstractmethod</code>","text":"<p>Forward pass of the encoder on a graph. This is a method to be implemented by the child class.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Batch</code> <p>The input pyg Batch</p> required"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.base_encoder.BaseEncoder.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> <p>Parameters:</p> Name Type Description Default <code>divide_factor</code> <code>float</code> <p>Factor by which to divide the width.</p> <code>2.0</code> <code>factor_in_dim</code> <code>bool</code> <p>Whether to factor the input dimension</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary with the base model arguments</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.base_encoder.BaseEncoder.parse_input_keys","title":"<code>parse_input_keys(input_keys)</code>  <code>abstractmethod</code>","text":"<p>Parse the <code>input_keys</code> argument. This is a method to be implemented by the child class.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>The input keys to parse</p> required"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.base_encoder.BaseEncoder.parse_input_keys_with_prefix","title":"<code>parse_input_keys_with_prefix(key_prefix)</code>","text":"<p>Parse the <code>input_keys</code> argument, given a certain prefix. If the prefix is <code>None</code>, it is ignored</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.base_encoder.BaseEncoder.parse_output_keys","title":"<code>parse_output_keys(output_keys)</code>  <code>abstractmethod</code>","text":"<p>Parse the <code>output_keys</code> argument.  This is a method to be implemented by the child class.</p> <p>Parameters:</p> Name Type Description Default <code>output_keys</code> <code>List[str]</code> <p>The output keys to parse</p> required"},{"location":"api/graphium.nn/encoders.html#gaussian-kernal-positional-encoder","title":"Gaussian Kernal Positional Encoder","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.gaussian_kernel_pos_encoder","title":"<code>graphium.nn.encoders.gaussian_kernel_pos_encoder</code>","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.gaussian_kernel_pos_encoder.GaussianKernelPosEncoder","title":"<code>GaussianKernelPosEncoder</code>","text":"<p>         Bases: <code>BaseEncoder</code></p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.gaussian_kernel_pos_encoder.GaussianKernelPosEncoder.__init__","title":"<code>__init__(input_keys, output_keys, in_dim, out_dim, embed_dim, num_layers, max_num_nodes_per_graph=None, activation='gelu', first_normalization='none', use_input_keys_prefix=True, num_heads=1)</code>","text":"<p>Configurable gaussian kernel-based Positional Encoding node and edge encoder. Useful for encoding 3D conformation positions.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>The keys from the pyg graph to use as input</p> required <code>output_keys</code> <code>List[str]</code> <p>The keys to return corresponding to the output encodings</p> required <code>in_dim</code> <code>int</code> <p>The input dimension for the encoder</p> required <code>out_dim</code> <code>int</code> <p>The output dimension of the encodings</p> required <code>embed_dim</code> <code>int</code> <p>The dimension of the embedding</p> required <code>num_layers</code> <code>int</code> <p>The number of layers of the encoder</p> required <code>max_num_nodes_per_graph</code> <code>Optional[int]</code> <p>The maximum number of nodes per graph</p> <code>None</code> <code>activation</code> <code>Union[str, Callable]</code> <p>The activation function to use</p> <code>'gelu'</code> <code>first_normalization</code> <p>The normalization to use before the first layer</p> <code>'none'</code> <code>use_input_keys_prefix</code> <code>bool</code> <p>Whether to use the <code>key_prefix</code> argument in the <code>forward</code> method.</p> <code>True</code> <code>num_heads</code> <code>int</code> <p>The number of heads to use for the multi-head attention</p> <code>1</code>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.gaussian_kernel_pos_encoder.GaussianKernelPosEncoder.forward","title":"<code>forward(batch, key_prefix=None)</code>","text":"<p>forward function of the GaussianKernelPosEncoder class</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p>The batch of pyg graphs</p> required <code>key_prefix</code> <code>Optional[str]</code> <p>The prefix to use for the input keys</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of the output encodings with keys specified by <code>output_keys</code></p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.gaussian_kernel_pos_encoder.GaussianKernelPosEncoder.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width. factor_in_dim: Whether to factor the input dimension</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of the base model kwargs</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.gaussian_kernel_pos_encoder.GaussianKernelPosEncoder.parse_input_keys","title":"<code>parse_input_keys(input_keys)</code>","text":"<p>Parse the <code>input_keys</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>The input keys to parse</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>The parsed input keys</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.gaussian_kernel_pos_encoder.GaussianKernelPosEncoder.parse_output_keys","title":"<code>parse_output_keys(output_keys)</code>","text":"<p>Parse the <code>output_keys</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output_keys</code> <code>List[str]</code> <p>The output keys to parse</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>The parsed output keys</p>"},{"location":"api/graphium.nn/encoders.html#laplacian-positional-encoder","title":"Laplacian Positional Encoder","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.laplace_pos_encoder","title":"<code>graphium.nn.encoders.laplace_pos_encoder</code>","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.laplace_pos_encoder.LapPENodeEncoder","title":"<code>LapPENodeEncoder</code>","text":"<p>         Bases: <code>BaseEncoder</code></p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.laplace_pos_encoder.LapPENodeEncoder.__init__","title":"<code>__init__(input_keys, output_keys, in_dim, hidden_dim, out_dim, num_layers, activation='relu', model_type='DeepSet', num_layers_post=1, dropout=0.0, first_normalization=None, use_input_keys_prefix=True, **model_kwargs)</code>","text":"<p>Laplace Positional Embedding node encoder. LapPE of size dim_pe will get appended to each node feature vector.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>List of input keys to use from the data object.</p> required <code>output_keys</code> <code>List[str]</code> <p>List of output keys to add to the data object.</p> required <code>in_dim</code> <p>Size of Laplace PE embedding. Only used by the MLP model</p> required <code>hidden_dim</code> <code>int</code> <p>Size of hidden layer</p> required <code>out_dim</code> <code>int</code> <p>Size of final node embedding</p> required <code>num_layers</code> <code>int</code> <p>Number of layers in the MLP</p> required <code>activation</code> <code>Optional[Union[str, Callable]]</code> <p>Activation function to use.</p> <code>'relu'</code> <code>model_type</code> <code>str</code> <p>'Transformer' or 'DeepSet' or 'MLP'</p> <code>'DeepSet'</code> <code>num_layers_post</code> <p>Number of layers to apply after pooling</p> <code>1</code> <code>dropout</code> <p>Dropout rate</p> <code>0.0</code> <code>first_normalization</code> <p>Normalization to apply to the first layer.</p> <code>None</code>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.laplace_pos_encoder.LapPENodeEncoder.forward","title":"<code>forward(batch, key_prefix=None)</code>","text":"<p>Forward pass of the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p>pyg Batches of graphs</p> required <code>key_prefix</code> <code>Optional[str]</code> <p>Prefix to use for the input and output keys.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, torch.Tensor]</code> <p>output dictionary with keys as specified in <code>output_keys</code> and their output embeddings.</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.laplace_pos_encoder.LapPENodeEncoder.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width. factor_in_dim: Whether to factor the input dimension</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of kwargs to be used to create the base model.</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.laplace_pos_encoder.LapPENodeEncoder.parse_input_keys","title":"<code>parse_input_keys(input_keys)</code>","text":"<p>Parse the input keys and make sure they are supported for this encoder</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>List of input keys to use from the data object.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of parsed input keys</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.laplace_pos_encoder.LapPENodeEncoder.parse_output_keys","title":"<code>parse_output_keys(output_keys)</code>","text":"<p>parse the output keys</p> <p>Parameters:</p> Name Type Description Default <code>output_keys</code> <code>List[str]</code> <p>List of output keys to add to the data object.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of parsed output keys</p>"},{"location":"api/graphium.nn/encoders.html#mlp-encoder","title":"MLP Encoder","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder","title":"<code>graphium.nn.encoders.mlp_encoder</code>","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.CatMLPEncoder","title":"<code>CatMLPEncoder</code>","text":"<p>         Bases: <code>BaseEncoder</code></p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.CatMLPEncoder.__init__","title":"<code>__init__(input_keys, output_keys, in_dim, hidden_dim, out_dim, num_layers, activation='relu', dropout=0.0, normalization='none', first_normalization='none', use_input_keys_prefix=True)</code>","text":"<p>Configurable kernel-based Positional Encoding node/edge-level encoder. Concatenates the list of input (node or edge) features in the feature dimension</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>List of input keys; inputs are concatenated in feat dimension and passed through mlp</p> required <code>output_keys</code> <code>str</code> <p>List of output keys to add to the pyg batch graph</p> required <code>in_dim</code> <p>input dimension of the mlp encoder; sum of input dimensions of inputs</p> required <code>hidden_dim</code> <p>hidden dimension of the mlp encoder</p> required <code>out_dim</code> <p>output dimension of the mlp encoder</p> required <code>num_layers</code> <p>number of layers of the mlp encoder</p> required <code>activation</code> <p>activation function to use</p> <code>'relu'</code> <code>dropout</code> <p>dropout to use</p> <code>0.0</code> <code>normalization</code> <p>normalization to use</p> <code>'none'</code> <code>first_normalization</code> <p>normalization to use before the first layer</p> <code>'none'</code> <code>use_input_keys_prefix</code> <code>bool</code> <p>Whether to use the <code>key_prefix</code> argument</p> <code>True</code>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.CatMLPEncoder.forward","title":"<code>forward(batch, key_prefix=None)</code>","text":"<p>forward function of the mlp encoder</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p>pyg batch graph</p> required <code>key_prefix</code> <code>Optional[str]</code> <p>Prefix to use for the input keys</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Dict[str, torch.Tensor]</code> <p>Dictionary of output embeddings with keys specified by input_keys</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.CatMLPEncoder.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width. factor_in_dim: Whether to factor the input dimension</p> <p>Returns:</p> Name Type Description <code>base_kwargs</code> <code>Dict[str, Any]</code> <p>Dictionary of kwargs to use for the base model</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.CatMLPEncoder.parse_input_keys","title":"<code>parse_input_keys(input_keys)</code>","text":"<p>Parse the <code>input_keys</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>List of input keys to use from pyg batch graph</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>parsed input_keys</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.CatMLPEncoder.parse_output_keys","title":"<code>parse_output_keys(output_keys)</code>","text":"<p>Parse the <code>output_keys</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output_keys</code> <code>List[str]</code> <p>List of output keys to add to the pyg batch graph</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>parsed output_keys</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.MLPEncoder","title":"<code>MLPEncoder</code>","text":"<p>         Bases: <code>BaseEncoder</code></p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.MLPEncoder.__init__","title":"<code>__init__(input_keys, output_keys, in_dim, hidden_dim, out_dim, num_layers, activation='relu', dropout=0.0, normalization='none', first_normalization='none', use_input_keys_prefix=True)</code>","text":"<p>Configurable kernel-based Positional Encoding node/edge-level encoder.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>List of input keys to use from pyg batch graph</p> required <code>output_keys</code> <code>str</code> <p>List of output keys to add to the pyg batch graph</p> required <code>in_dim</code> <p>input dimension of the mlp encoder</p> required <code>hidden_dim</code> <p>hidden dimension of the mlp encoder</p> required <code>out_dim</code> <p>output dimension of the mlp encoder</p> required <code>num_layers</code> <p>number of layers of the mlp encoder</p> required <code>activation</code> <p>activation function to use</p> <code>'relu'</code> <code>dropout</code> <p>dropout to use</p> <code>0.0</code> <code>normalization</code> <p>normalization to use</p> <code>'none'</code> <code>first_normalization</code> <p>normalization to use before the first layer</p> <code>'none'</code> <code>use_input_keys_prefix</code> <code>bool</code> <p>Whether to use the <code>key_prefix</code> argument</p> <code>True</code>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.MLPEncoder.forward","title":"<code>forward(batch, key_prefix=None)</code>","text":"<p>forward function of the mlp encoder</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p>pyg batch graph</p> required <code>key_prefix</code> <code>Optional[str]</code> <p>Prefix to use for the input keys</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Dict[str, torch.Tensor]</code> <p>Dictionary of output embeddings with keys specified by input_keys</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.MLPEncoder.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p> Parameter <p>divide_factor: Factor by which to divide the width. factor_in_dim: Whether to factor the input dimension</p> <p>Returns:</p> Name Type Description <code>base_kwargs</code> <code>Dict[str, Any]</code> <p>Dictionary of kwargs to use for the base model</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.MLPEncoder.parse_input_keys","title":"<code>parse_input_keys(input_keys)</code>","text":"<p>Parse the <code>input_keys</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_keys</code> <code>List[str]</code> <p>List of input keys to use from pyg batch graph</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>parsed input_keys</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.mlp_encoder.MLPEncoder.parse_output_keys","title":"<code>parse_output_keys(output_keys)</code>","text":"<p>Parse the <code>output_keys</code>.</p> <p>Parameters:</p> Name Type Description Default <code>output_keys</code> <code>List[str]</code> <p>List of output keys to add to the pyg batch graph</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>parsed output_keys</p>"},{"location":"api/graphium.nn/encoders.html#signnet-positional-encoder","title":"Signnet Positional Encoder","text":""},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.signnet_pos_encoder","title":"<code>graphium.nn.encoders.signnet_pos_encoder</code>","text":"<p>SignNet https://arxiv.org/abs/2202.13013 based on https://github.com/cptq/SignNet-BasisNet</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.signnet_pos_encoder.GINDeepSigns","title":"<code>GINDeepSigns</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Sign invariant neural network with MLP aggregation. f(v1, ..., vk) = rho(enc(v1) + enc(-v1), ..., enc(vk) + enc(-vk))</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.signnet_pos_encoder.MaskedGINDeepSigns","title":"<code>MaskedGINDeepSigns</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Sign invariant neural network with sum pooling and DeepSet. f(v1, ..., vk) = rho(enc(v1) + enc(-v1), ..., enc(vk) + enc(-vk))</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.signnet_pos_encoder.SignNetNodeEncoder","title":"<code>SignNetNodeEncoder</code>","text":"<p>         Bases: <code>BaseEncoder</code></p> <p>SignNet Positional Embedding node encoder. https://arxiv.org/abs/2202.13013 https://github.com/cptq/SignNet-BasisNet</p> <p>Uses precomputated Laplacian eigen-decomposition, but instead of eigen-vector sign flipping + DeepSet/Transformer, computes the PE as: SignNetPE(v_1, ... , v_k) = \\rho ( [\\phi(v_i) + \\rhi(-v_i)]^k_i=1 ) where \\phi is GIN network applied to k first non-trivial eigenvectors, and \\rho is an MLP if k is a constant, but if all eigenvectors are used then \\rho is DeepSet with sum-pooling.</p> <p>SignNetPE of size dim_pe will get appended to each node feature vector.</p> <p>Parameters:</p> Name Type Description Default <code>dim_emb</code> <p>Size of final node embedding</p> required"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.signnet_pos_encoder.SignNetNodeEncoder.make_mup_base_kwargs","title":"<code>make_mup_base_kwargs(divide_factor=2.0, factor_in_dim=False)</code>","text":"<p>Create a 'base' model to be used by the <code>mup</code> or <code>muTransfer</code> scaling of the model. The base model is usually identical to the regular model, but with the layers width divided by a given factor (2 by default)</p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.signnet_pos_encoder.SignNetNodeEncoder.make_mup_base_kwargs--todo-update-this-it-is-broken","title":"TODO: Update this. It is broken","text":"<p>Parameters:</p> Name Type Description Default <code>divide_factor</code> <code>float</code> <p>Factor by which to divide the width.</p> <code>2.0</code> <code>factor_in_dim</code> <code>bool</code> <p>Whether to factor the input dimension</p> <code>False</code>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.signnet_pos_encoder.SimpleGIN","title":"<code>SimpleGIN</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/encoders.html#graphium.nn.encoders.signnet_pos_encoder.SimpleGIN.__init__","title":"<code>__init__(in_dim, hidden_dim, out_dim, num_layers, normalization='none', dropout=0.5, activation='relu')</code>","text":"<p>not supported yet</p>"},{"location":"api/graphium.nn/graphium.nn.html","title":"graphium.nn","text":"<p>Base structures for neural networks in the library (to be inherented by other modules)</p> Contents <ul> <li>Base Graph Layer</li> <li>Base Layers</li> <li>Residual Connection</li> </ul>"},{"location":"api/graphium.nn/graphium.nn.html#base-graph-layer","title":"Base Graph Layer","text":""},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer","title":"<code>graphium.nn.base_graph_layer</code>","text":""},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphModule","title":"<code>BaseGraphModule</code>","text":"<p>         Bases: <code>BaseGraphStructure</code>, <code>nn.Module</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphModule.__init__","title":"<code>__init__(in_dim, out_dim, activation='relu', dropout=0.0, normalization='none', layer_idx=None, layer_depth=None, droppath_rate=0.0)</code>","text":"<p>Abstract class used to standardize the implementation of Pyg layers in the current library. It will allow a network to seemlesly swap between different GNN layers by better understanding the expected inputs and outputs.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the layer</p> required <code>activation</code> <code>Union[str, Callable]</code> <p>activation function to use in the layer</p> <code>'relu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>layer_idx</code> <code>Optional[int]</code> <p>The index of the current layer</p> <code>None</code> <code>layer_depth</code> <code>Optional[int]</code> <p>The total depth (number of layers) associated to this specific layer</p> <code>None</code> <code>droppath_rate</code> <code>float</code> <p>stochastic depth drop rate, between 0 and 1, see https://arxiv.org/abs/1603.09382</p> <code>0.0</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure","title":"<code>BaseGraphStructure</code>","text":""},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.layer_inputs_edges","title":"<code>layer_inputs_edges: bool</code>  <code>property</code> <code>abstractmethod</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_input_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the layer uses input edges in the forward pass</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.layer_outputs_edges","title":"<code>layer_outputs_edges: bool</code>  <code>property</code> <code>abstractmethod</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_output_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the layer outputs edges in the forward pass</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.max_num_edges_per_graph","title":"<code>max_num_edges_per_graph: Optional[int]</code>  <code>writable</code> <code>property</code>","text":"<p>Get the maximum number of nodes per graph. Useful for reshaping a compiled model (IPU)</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.max_num_nodes_per_graph","title":"<code>max_num_nodes_per_graph: Optional[int]</code>  <code>writable</code> <code>property</code>","text":"<p>Get the maximum number of nodes per graph. Useful for reshaping a compiled model (IPU)</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.out_dim_factor","title":"<code>out_dim_factor: int</code>  <code>property</code> <code>abstractmethod</code>","text":"<p>Abstract method. Get the factor by which the output dimension is multiplied for the next layer.</p> <p>For standard layers, this will return <code>1</code>.</p> <p>But for others, such as <code>GatLayer</code>, the output is the concatenation of the outputs from each head, so the out_dim gets multiplied by the number of heads, and this function should return the number of heads.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The factor that multiplies the output dimensions</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.__init__","title":"<code>__init__(in_dim, out_dim, activation='relu', dropout=0.0, normalization='none', layer_idx=None, layer_depth=None, droppath_rate=0.0)</code>","text":"<p>Abstract class used to standardize the implementation of Pyg layers in the current library. It will allow a network to seemlesly swap between different GNN layers by better understanding the expected inputs and outputs.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the layer</p> required <code>activation</code> <code>Union[str, Callable]</code> <p>activation function to use in the layer</p> <code>'relu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>layer_idx</code> <code>Optional[int]</code> <p>The index of the current layer</p> <code>None</code> <code>layer_depth</code> <code>Optional[int]</code> <p>The total depth (number of layers) associated to this specific layer</p> <code>None</code> <code>droppath_rate</code> <code>float</code> <p>stochastic depth drop rate, between 0 and 1, see https://arxiv.org/abs/1603.09382</p> <code>0.0</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.apply_norm_activation_dropout","title":"<code>apply_norm_activation_dropout(feat, normalization=True, activation=True, dropout=True, droppath=True, batch_idx=None, batch_size=None)</code>","text":"<p>Apply the different normalization and the dropout to the output layer.</p> <p>Parameters:</p> Name Type Description Default <code>feat</code> <code>Tensor</code> <p>Feature tensor, to be normalized</p> required <code>normalization</code> <code>bool</code> <p>Whether to apply the normalization</p> <code>True</code> <code>activation</code> <code>bool</code> <p>Whether to apply the activation layer</p> <code>True</code> <code>dropout</code> <code>bool</code> <p>Whether to apply the dropout layer</p> <code>True</code> <code>droppath</code> <code>bool</code> <p>Whether to apply the DropPath layer</p> <code>True</code> <p>Returns:</p> Name Type Description <code>feat</code> <p>Normalized and dropped-out features</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.layer_supports_edges","title":"<code>layer_supports_edges()</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type supports output edges edges or not.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the layer supports the use of edges</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.check_intpus_allow_int","title":"<code>check_intpus_allow_int(obj, edge_index, size)</code>","text":"<p>Overwrite the check_input to allow for int32 and int16 TODO: Remove when PyG and pytorch supports int32.</p>"},{"location":"api/graphium.nn/graphium.nn.html#base-layers","title":"Base Layers","text":""},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers","title":"<code>graphium.nn.base_layers</code>","text":""},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.DropPath","title":"<code>DropPath</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.DropPath.__init__","title":"<code>__init__(drop_rate)</code>","text":"<p>DropPath class for stochastic depth Deep Networks with Stochastic Depth Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra and Kilian Weinberger https://arxiv.org/abs/1603.09382</p> <p>Parameters:</p> Name Type Description Default <code>drop_rate</code> <code>float</code> <p>Drop out probability</p> required"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.DropPath.forward","title":"<code>forward(input, batch_idx, batch_size=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p><code>torch.Tensor[total_num_nodes, hidden]</code></p> required <code>batch</code> <p>batch attribute of the batch object, batch.batch</p> required <code>batch_size</code> <code>Optional[int]</code> <p>The batch size. Must be provided when working on IPU</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: <code>torch.Tensor[total_num_nodes, hidde]</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.DropPath.get_stochastic_drop_rate","title":"<code>get_stochastic_drop_rate(drop_rate, layer_idx=None, layer_depth=None)</code>  <code>staticmethod</code>","text":"<p>Get the stochastic drop rate from the nominal drop rate, the layer index, and the layer depth.</p> <p><code>return drop_rate * (layer_idx / (layer_depth - 1))</code></p> <p>Parameters:</p> Name Type Description Default <code>drop_rate</code> <code>float</code> <p>Drop out nominal probability</p> required <code>layer_idx</code> <code>Optional[int]</code> <p>The index of the current layer</p> <code>None</code> <code>layer_depth</code> <code>Optional[int]</code> <p>The total depth (number of layers) associated to this specific layer</p> <code>None</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.FCLayer","title":"<code>FCLayer</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.FCLayer.in_channels","title":"<code>in_channels: int</code>  <code>property</code>","text":"<p>Get the input channel size. For compatibility with PyG.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.FCLayer.out_channels","title":"<code>out_channels: int</code>  <code>property</code>","text":"<p>Get the output channel size. For compatibility with PyG.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.FCLayer.__init__","title":"<code>__init__(in_dim, out_dim, activation='relu', dropout=0.0, normalization='none', bias=True, init_fn=None, is_readout_layer=False, droppath_rate=0.0)</code>","text":"<p>A simple fully connected and customizable layer. This layer is centered around a <code>torch.nn.Linear</code> module. The order in which transformations are applied is:</p> <ul> <li>Dense Layer</li> <li>Activation</li> <li>Dropout (if applicable)</li> <li>Batch Normalization (if applicable)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimension of the layer (the <code>torch.nn.Linear</code>)</p> required <code>out_dim</code> <code>int</code> <p>Output dimension of the layer.</p> required <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. No dropout by default.</p> <code>0.0</code> <code>activation</code> <code>Union[str, Callable]</code> <p>Activation function to use.</p> <code>'relu'</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>bias</code> <code>bool</code> <p>Whether to enable bias in for the linear layer.</p> <code>True</code> <code>init_fn</code> <code>Optional[Callable]</code> <p>Initialization function to use for the weight of the layer. Default is \\(\\(\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})\\)\\) with \\(\\(k=\\frac{1}{ \\text{in_dim}}\\)\\)</p> <code>None</code> <code>is_readout_layer</code> <code>bool</code> <p>Whether the layer should be treated as a readout layer by replacing of <code>torch.nn.Linear</code> by <code>mup.MuReadout</code> from the muTransfer method https://github.com/microsoft/mup</p> <code>False</code> <code>droppath_rate</code> <code>float</code> <p>stochastic depth drop rate, between 0 and 1, see https://arxiv.org/abs/1603.09382</p> <code>0.0</code> <p>Attributes:</p> Name Type Description <code>dropout</code> <code>int</code> <p>The ratio of units to dropout.</p> <code>normalization</code> <code>None or Callable</code> <p>Normalization layer</p> <code>linear</code> <code>`torch.nn.Linear`</code> <p>The linear layer</p> <code>activation</code> <code>`torch.nn.Module`</code> <p>The activation layer</p> <code>init_fn</code> <code>Callable</code> <p>Initialization function used for the weight of the layer</p> <code>in_dim</code> <code>int</code> <p>Input dimension of the linear layer</p> <code>out_dim</code> <code>int</code> <p>Output dimension of the linear layer</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.FCLayer.forward","title":"<code>forward(h)</code>","text":"<p>Apply the FC layer on the input features.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>torch.Tensor</code> <p><code>torch.Tensor[..., Din]</code>: Input feature tensor, before the FC. <code>Din</code> is the number of input features</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p><code>torch.Tensor[..., Dout]</code>: Output feature tensor, after the FC. <code>Dout</code> is the number of output features</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.FCLayer.reset_parameters","title":"<code>reset_parameters(init_fn=None)</code>","text":"<p>Reset the parameters of the linear layer using the <code>init_fn</code>.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.GRU","title":"<code>GRU</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.GRU.__init__","title":"<code>__init__(in_dim, hidden_dim)</code>","text":"<p>Wrapper class for the GRU used by the GNN framework, nn.GRU is used for the Gated Recurrent Unit itself</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimension of the GRU layer</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden dimension of the GRU layer.</p> required"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.GRU.forward","title":"<code>forward(x, y)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p><code>torch.Tensor[B, N, Din]</code> where Din &lt;= in_dim (difference is padded)</p> required <code>y</code> <p><code>torch.Tensor[B, N, Dh]</code> where Dh &lt;= hidden_dim (difference is padded)</p> required <p>Returns:</p> Type Description <p>torch.Tensor: <code>torch.Tensor[B, N, Dh]</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP","title":"<code>MLP</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP.__init__","title":"<code>__init__(in_dim, hidden_dims, out_dim, depth, activation='relu', last_activation='none', dropout=0.0, last_dropout=0.0, normalization='none', last_normalization='none', first_normalization='none', last_layer_is_readout=False, droppath_rate=0.0, constant_droppath_rate=True)</code>","text":"<p>Simple multi-layer perceptron, built of a series of FCLayers</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimension of the MLP</p> required <code>hidden_dims</code> <code>Union[Iterable[int], int]</code> <p>Either an integer specifying all the hidden dimensions, or a list of dimensions in the hidden layers.</p> required <code>out_dim</code> <code>int</code> <p>Output dimension of the MLP.</p> required <code>depth</code> <code>int</code> <p>If <code>hidden_dims</code> is an integer, <code>depth</code> is 1 + the number of hidden layers to use. If <code>hidden_dims</code> is a list, then <code>depth</code> must be <code>None</code> or equal to <code>len(hidden_dims) + 1</code></p> required <code>activation</code> <code>Union[str, Callable]</code> <p>Activation function to use in all the layers except the last. if <code>layers==1</code>, this parameter is ignored</p> <code>'relu'</code> <code>last_activation</code> <code>Union[str, Callable]</code> <p>Activation function to use in the last layer.</p> <code>'none'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[Type[None], str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization in the hidden layers.</li> <li><code>Callable</code>: Any callable function</li> </ul> <p>if <code>layers==1</code>, this parameter is ignored</p> <code>'none'</code> <code>last_normalization</code> <code>Union[Type[None], str, Callable]</code> <p>Norrmalization to use after the last layer. Same options as <code>normalization</code>.</p> <code>'none'</code> <code>first_normalization</code> <code>Union[Type[None], str, Callable]</code> <p>Norrmalization to use in before the first layer. Same options as <code>normalization</code>.</p> <code>'none'</code> <code>last_dropout</code> <code>float</code> <p>The ratio of units to dropout at the last layer.</p> <code>0.0</code> <code>last_layer_is_readout</code> <code>bool</code> <p>Whether the last layer should be treated as a readout layer. Allows to use the <code>mup.MuReadout</code> from the muTransfer method https://github.com/microsoft/mup</p> <code>False</code> <code>droppath_rate</code> <code>float</code> <p>stochastic depth drop rate, between 0 and 1. See https://arxiv.org/abs/1603.09382</p> <code>0.0</code> <code>constant_droppath_rate</code> <code>bool</code> <p>If <code>True</code>, drop rates will remain constant accross layers. Otherwise, drop rates will vary stochastically. See <code>DropPath.get_stochastic_drop_rate</code></p> <code>True</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP.forward","title":"<code>forward(h)</code>","text":"<p>Apply the MLP on the input features.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>torch.Tensor</code> <p><code>torch.Tensor[..., Din]</code>: Input feature tensor, before the MLP. <code>Din</code> is the number of input features</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p><code>torch.Tensor[..., Dout]</code>: Output feature tensor, after the MLP. <code>Dout</code> is the number of output features</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MuReadoutGraphium","title":"<code>MuReadoutGraphium</code>","text":"<p>         Bases: <code>MuReadout</code></p> <p>PopTorch-compatible replacement for <code>mup.MuReadout</code></p> <p>Not quite a drop-in replacement for <code>mup.MuReadout</code> - you need to specify <code>base_width</code>.</p> <p>Set <code>base_width</code> to width of base model passed to <code>mup.set_base_shapes</code> to get same results on IPU and CPU. Should still \"work\" with any other value, but won't give the same results as CPU</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MultiheadAttentionMup","title":"<code>MultiheadAttentionMup</code>","text":"<p>         Bases: <code>nn.MultiheadAttention</code></p> <p>Modifying the MultiheadAttention to work with the muTransfer paradigm. The layers are initialized using the mup package. The <code>_scaled_dot_product_attention</code> normalizes the attention matrix with <code>1/d</code> instead of <code>1/sqrt(d)</code> The biased self-attention option is added to have 3D attention bias.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.TransformerEncoderLayerMup","title":"<code>TransformerEncoderLayerMup</code>","text":"<p>         Bases: <code>nn.TransformerEncoderLayer</code></p> <p>Modified version of <code>torch.nn.TransformerEncoderLayer</code> that uses :math:<code>1/n</code>-scaled attention for compatibility with muP (as opposed to the original :math:<code>1/\\sqrt{n}</code> scaling factor) Arguments are the same as <code>torch.nn.TransformerEncoderLayer</code>.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.get_activation","title":"<code>get_activation(activation)</code>","text":"<p>returns the activation function represented by the input string</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>Union[type(None), str, Callable]</code> <p>Callable, <code>None</code>, or string with value: \"none\", \"ReLU\", \"Sigmoid\", \"Tanh\", \"ELU\", \"SELU\", \"GLU\", \"GELU\", \"LeakyReLU\", \"Softplus\"</p> required <p>Returns:</p> Type Description <code>Optional[Callable]</code> <p>Callable or None: The activation function</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.get_activation_str","title":"<code>get_activation_str(activation)</code>","text":"<p>returns the string related to the activation function</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>Union[type(None), str, Callable]</code> <p>Callable, <code>None</code>, or string with value: \"none\", \"ReLU\", \"Sigmoid\", \"Tanh\", \"ELU\", \"SELU\", \"GLU\", \"LeakyReLU\", \"Softplus\"</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the activation function</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.get_norm","title":"<code>get_norm(normalization, dim=None)</code>","text":"<p>returns the normalization function represented by the input string</p> <p>Parameters:</p> Name Type Description Default <code>normalization</code> <code>Union[Type[None], str, Callable]</code> <p>Callable, <code>None</code>, or string with value: \"none\", \"batch_norm\", \"layer_norm\"</p> required <code>dim</code> <code>Optional[int]</code> <p>Dimension where to apply the norm. Mandatory for 'batch_norm' and 'layer_norm'</p> <code>None</code> <p>Returns:</p> Type Description <p>Callable or None: The normalization function</p>"},{"location":"api/graphium.nn/graphium.nn.html#residual-connections","title":"Residual Connections","text":""},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections","title":"<code>graphium.nn.residual_connections</code>","text":"<p>Different types of residual connections, including None, Simple (ResNet-like), Concat and DenseNet</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionBase","title":"<code>ResidualConnectionBase</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionBase.__init__","title":"<code>__init__(skip_steps=1)</code>","text":"<p>Abstract class for the residual connections. Using this class, we implement different types of residual connections, such as the ResNet, weighted-ResNet, skip-concat and DensNet.</p> <p>The following methods must be implemented in a children class</p> <ul> <li><code>h_dim_increase_type()</code></li> <li><code>has_weights()</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>skip_steps</code> <code>int</code> <p>int The number of steps to skip between the residual connections. If <code>1</code>, all the layers are connected. If <code>2</code>, half of the layers are connected.</p> <code>1</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionBase.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionBase.get_true_out_dims","title":"<code>get_true_out_dims(out_dims)</code>","text":"<p>find the true output dimensions</p> <p>Parameters:</p> Name Type Description Default <code>out_dims</code> <code>List</code> <p>List</p> required <p>Returns:</p> Name Type Description <code>true_out_dims</code> <code>List</code> <p>List</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionBase.h_dim_increase_type","title":"<code>h_dim_increase_type()</code>  <code>abstractmethod</code>","text":"<p>How does the dimension of the output features increases after each layer?</p> <p>Returns:</p> Name Type Description <code>h_dim_increase_type</code> <p>None or str - <code>None</code>: The dimension of the output features do not change at each layer. E.g. ResNet.</p> <ul> <li> <p>\"previous\": The dimension of the output features is the concatenation of the previous layer with the new layer.</p> </li> <li> <p>\"cumulative\": The dimension of the output features is the concatenation of all previous layers.</p> </li> </ul>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionBase.has_weights","title":"<code>has_weights()</code>  <code>abstractmethod</code>","text":"<p>Returns:</p> Name Type Description <code>has_weights</code> <p>bool Whether the residual connection uses weights</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionConcat","title":"<code>ResidualConnectionConcat</code>","text":"<p>         Bases: <code>ResidualConnectionBase</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionConcat.__init__","title":"<code>__init__(skip_steps=1)</code>","text":"<p>Class for the simple residual connections proposed but where the skip connection features are concatenated to the current layer features.</p> <p>Parameters:</p> Name Type Description Default <code>skip_steps</code> <code>int</code> <p>int The number of steps to skip between the residual connections. If <code>1</code>, all the layers are connected. If <code>2</code>, half of the layers are connected.</p> <code>1</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionConcat.forward","title":"<code>forward(h, h_prev, step_idx)</code>","text":"<p>Concatenate <code>h</code> with the previous layers with skip connection <code>h_prev</code>.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>torch.Tensor</code> <p>torch.Tensor(..., m) The current layer features</p> required <code>h_prev</code> <code>torch.Tensor</code> <p>torch.Tensor(..., n), None The features from the previous layer with a skip connection. Usually, we have <code>n</code> equal to <code>m</code>. At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p> required <code>step_idx</code> <code>int</code> <p>int Current layer index or step index in the forward loop of the architecture.</p> required <p>Returns:</p> Name Type Description <code>h</code> <p>torch.Tensor(..., m) or torch.Tensor(..., m + n) Either return <code>h</code> unchanged, or the concatenation with <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p> <code>h_prev</code> <p>torch.Tensor(..., m) or torch.Tensor(..., m + n) Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionConcat.h_dim_increase_type","title":"<code>h_dim_increase_type()</code>","text":"<p>Returns:</p> Type Description <p>\"previous\": The dimension of the output layer is the concatenation with the previous layer.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionConcat.has_weights","title":"<code>has_weights()</code>","text":"<p>Returns:</p> Type Description <p>False The current class does not use weights</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionDenseNet","title":"<code>ResidualConnectionDenseNet</code>","text":"<p>         Bases: <code>ResidualConnectionBase</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionDenseNet.__init__","title":"<code>__init__(skip_steps=1)</code>","text":"<p>Class for the residual connections proposed by DenseNet, where all previous skip connection features are concatenated to the current layer features.</p> <p>Parameters:</p> Name Type Description Default <code>skip_steps</code> <code>int</code> <p>int The number of steps to skip between the residual connections. If <code>1</code>, all the layers are connected. If <code>2</code>, half of the layers are connected.</p> <code>1</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionDenseNet.forward","title":"<code>forward(h, h_prev, step_idx)</code>","text":"<p>Concatenate <code>h</code> with all the previous layers with skip connection <code>h_prev</code>.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>torch.Tensor</code> <p>torch.Tensor(..., m) The current layer features</p> required <code>h_prev</code> <code>torch.Tensor</code> <p>torch.Tensor(..., n), None The features from the previous layers. n = ((step_idx // self.skip_steps) + 1) * m</p> <p>At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p> required <code>step_idx</code> <code>int</code> <p>int Current layer index or step index in the forward loop of the architecture.</p> required <p>Returns:</p> Name Type Description <code>h</code> <p>torch.Tensor(..., m) or torch.Tensor(..., m + n) Either return <code>h</code> unchanged, or the concatenation with <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p> <code>h_prev</code> <p>torch.Tensor(..., m) or torch.Tensor(..., m + n) Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionDenseNet.h_dim_increase_type","title":"<code>h_dim_increase_type()</code>","text":"<p>Returns:</p> Type Description <p>\"cumulative\": The dimension of the output layer is the concatenation of all the previous layer.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionDenseNet.has_weights","title":"<code>has_weights()</code>","text":"<p>Returns:</p> Type Description <p>False The current class does not use weights</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionNone","title":"<code>ResidualConnectionNone</code>","text":"<p>         Bases: <code>ResidualConnectionBase</code></p> <p>No residual connection. This class is only used for simpler code compatibility</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionNone.__repr__","title":"<code>__repr__()</code>","text":"<p>Controls how the class is printed</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionNone.forward","title":"<code>forward(h, h_prev, step_idx)</code>","text":"<p>Ignore the skip connection.</p> <p>Returns:</p> Name Type Description <code>h</code> <p>torch.Tensor(..., m) Return same as input.</p> <code>h_prev</code> <p>torch.Tensor(..., m) Return same as input.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionNone.h_dim_increase_type","title":"<code>h_dim_increase_type()</code>","text":"<p>Returns:</p> Name Type Description <code>None</code> <p>The dimension of the output features do not change at each layer.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionNone.has_weights","title":"<code>has_weights()</code>","text":"<p>Returns:</p> Type Description <p>False The current class does not use weights</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionRandom","title":"<code>ResidualConnectionRandom</code>","text":"<p>         Bases: <code>ResidualConnectionBase</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionRandom.__init__","title":"<code>__init__(skip_steps=1, out_dims=None, num_layers=None)</code>","text":"<p>Class for the random residual connection, where each layer is connected to each following layer with a random weight between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>skip_steps</code> <p>Parameter only there for compatibility with other classes of the same parent.</p> <code>1</code> <code>out_dims</code> <code>List[int]</code> <p>The list of output dimensions. Only required to get the number of layers. Must be provided if <code>num_layers</code> is None.</p> <code>None</code> <code>num_layers</code> <code>int</code> <p>The number of layers. Must be provided if <code>out_dims</code> is None.</p> <code>None</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionRandom.forward","title":"<code>forward(h, h_prev, step_idx)</code>","text":"<p>Add <code>h</code> with the previous layers with skip connection <code>h_prev</code>, similar to ResNet.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>torch.Tensor</code> <p>torch.Tensor(..., m) The current layer features</p> required <code>h_prev</code> <code>torch.Tensor</code> <p>torch.Tensor(..., m), None The features from the previous layer with a skip connection. At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p> required <code>step_idx</code> <code>int</code> <p>int Current layer index or step index in the forward loop of the architecture.</p> required <p>Returns:</p> Name Type Description <code>h</code> <p>torch.Tensor(..., m) Either return <code>h</code> unchanged, or the sum with on <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p> <code>h_prev</code> <p>torch.Tensor(..., m) Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionRandom.h_dim_increase_type","title":"<code>h_dim_increase_type()</code>","text":"<p>Returns:</p> Name Type Description <code>None</code> <p>The dimension of the output features do not change at each layer.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionRandom.has_weights","title":"<code>has_weights()</code>","text":"<p>Returns:</p> Type Description <p>False The current class does not use weights</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionSimple","title":"<code>ResidualConnectionSimple</code>","text":"<p>         Bases: <code>ResidualConnectionBase</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionSimple.__init__","title":"<code>__init__(skip_steps=1)</code>","text":"<p>Class for the simple residual connections proposed by ResNet, where the current layer output is summed to a previous layer output.</p> <p>Parameters:</p> Name Type Description Default <code>skip_steps</code> <code>int</code> <p>int The number of steps to skip between the residual connections. If <code>1</code>, all the layers are connected. If <code>2</code>, half of the layers are connected.</p> <code>1</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionSimple.forward","title":"<code>forward(h, h_prev, step_idx)</code>","text":"<p>Add <code>h</code> with the previous layers with skip connection <code>h_prev</code>, similar to ResNet.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>torch.Tensor</code> <p>torch.Tensor(..., m) The current layer features</p> required <code>h_prev</code> <code>torch.Tensor</code> <p>torch.Tensor(..., m), None The features from the previous layer with a skip connection. At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p> required <code>step_idx</code> <code>int</code> <p>int Current layer index or step index in the forward loop of the architecture.</p> required <p>Returns:</p> Name Type Description <code>h</code> <p>torch.Tensor(..., m) Either return <code>h</code> unchanged, or the sum with on <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p> <code>h_prev</code> <p>torch.Tensor(..., m) Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionSimple.h_dim_increase_type","title":"<code>h_dim_increase_type()</code>","text":"<p>Returns:</p> Name Type Description <code>None</code> <p>The dimension of the output features do not change at each layer.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionSimple.has_weights","title":"<code>has_weights()</code>","text":"<p>Returns:</p> Type Description <p>False The current class does not use weights</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionWeighted","title":"<code>ResidualConnectionWeighted</code>","text":"<p>         Bases: <code>ResidualConnectionBase</code></p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionWeighted.__init__","title":"<code>__init__(out_dims, skip_steps=1, dropout=0.0, activation='none', normalization='none', bias=False)</code>","text":"<p>Class for the simple residual connections proposed by ResNet, with an added layer in the residual connection itself. The layer output is summed to a a non-linear transformation of a previous layer output.</p> <p>Parameters:</p> Name Type Description Default <code>skip_steps</code> <code>int</code> <p>int The number of steps to skip between the residual connections. If <code>1</code>, all the layers are connected. If <code>2</code>, half of the layers are connected.</p> <code>1</code> <code>out_dims</code> <p>list(int) list of all output dimensions for the network that will use this residual connection. E.g. <code>out_dims = [4, 8, 8, 8, 2]</code>.</p> required <code>dropout</code> <p>float value between 0 and 1.0 representing the percentage of dropout to use in the weights</p> <code>0.0</code> <code>activation</code> <code>Union[str, Callable]</code> <p>str, Callable The activation function to use after the skip weights</p> <code>'none'</code> <code>normalization</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization in the hidden layers.</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>bias</code> <p>bool Whether to apply add a bias after the weights</p> <code>False</code>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionWeighted.forward","title":"<code>forward(h, h_prev, step_idx)</code>","text":"<p>Add <code>h</code> with the previous layers with skip connection <code>h_prev</code>, after a feed-forward layer.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>torch.Tensor</code> <p>torch.Tensor(..., m) The current layer features</p> required <code>h_prev</code> <code>torch.Tensor</code> <p>torch.Tensor(..., m), None The features from the previous layer with a skip connection. At <code>step_idx==0</code>, <code>h_prev</code> can be set to <code>None</code>.</p> required <code>step_idx</code> <code>int</code> <p>int Current layer index or step index in the forward loop of the architecture.</p> required <p>Returns:</p> Name Type Description <code>h</code> <p>torch.Tensor(..., m) Either return <code>h</code> unchanged, or the sum with the output of a NN layer on <code>h_prev</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p> <code>h_prev</code> <p>torch.Tensor(..., m) Either return <code>h_prev</code> unchanged, or the same value as <code>h</code>, depending on the <code>step_idx</code> and <code>self.skip_steps</code>.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionWeighted.h_dim_increase_type","title":"<code>h_dim_increase_type()</code>","text":"<p>Returns:</p> Name Type Description <code>None</code> <p>The dimension of the output features do not change at each layer.</p>"},{"location":"api/graphium.nn/graphium.nn.html#graphium.nn.residual_connections.ResidualConnectionWeighted.has_weights","title":"<code>has_weights()</code>","text":"<p>Returns:</p> Type Description <p>True The current class uses weights</p>"},{"location":"api/graphium.nn/pyg_layers.html","title":"graphium.nn.pyg_layers","text":"<p>Implementations of GNN layers based on PyG in the library</p> Contents <ul> <li>Gated GCN Layer</li> <li>GPS Layer</li> <li>GIN and GINE Layers</li> <li>MPNN Layer</li> <li>PNA Layer</li> <li>Pooling Layer</li> </ul>"},{"location":"api/graphium.nn/pyg_layers.html#gated-gcn-layer","title":"Gated GCN Layer","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg","title":"<code>graphium.nn.pyg_layers.gated_gcn_pyg</code>","text":"<p>Unit tests for the different layers of graphium/nn/pyg_layers/...</p> <p>The layers are not thoroughly tested due to the difficulty of testing them</p> <p>adapated from https://github.com/rampasek/GraphGPS/blob/main/graphgps/layer/gps_layer.py</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg","title":"<code>GatedGCNPyg</code>","text":"<p>         Bases: <code>MessagePassing</code>, <code>BaseGraphStructure</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_inputs_edges","title":"<code>layer_inputs_edges: bool</code>  <code>property</code>","text":"<p>Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_outputs_edges","title":"<code>layer_outputs_edges: bool</code>  <code>property</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.out_dim_factor","title":"<code>out_dim_factor: int</code>  <code>property</code>","text":"<p>Get the factor by which the output dimension is multiplied for the next layer.</p> <p>For standard layers, this will return <code>1</code>.</p> <p>But for others, such as <code>GatLayer</code>, the output is the concatenation of the outputs from each head, so the out_dim gets multiplied by the number of heads, and this function should return the number of heads.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Always <code>1</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.__init__","title":"<code>__init__(in_dim, out_dim, in_dim_edges, out_dim_edges=None, activation='relu', dropout=0.0, normalization='none', **kwargs)</code>","text":"<p>ResGatedGCN: Residual Gated Graph ConvNets An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018) https://arxiv.org/pdf/1711.07553v2.pdf</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the layer, and for the edges</p> required <code>in_dim_edges</code> <code>int</code> <p>Input edge-feature dimensions of the layer</p> required <code>activation</code> <code>Union[Callable, str]</code> <p>activation function to use in the layer</p> <code>'relu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.aggregate","title":"<code>aggregate(sigma_ij, index, Bx_j, Bx)</code>","text":"<p>aggregation function of the layer</p> <p>Parameters:</p> Name Type Description Default <code>sigma_ij</code> <code>torch.Tensor</code> <p>the output from message() function with dim [n_edges, out_dim]</p> required <code>index</code> <code>torch.Tensor</code> <p>dim [n_edges]</p> required <code>Bx_j</code> <code>torch.Tensor</code> <p>dim [n_edges, out_dim]</p> required <code>Bx</code> <code>torch.Tensor</code> <p>dim [n_nodes, out_dim]</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>torch.Tensor</code> <p>dim [n_nodes, out_dim]</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.forward","title":"<code>forward(batch)</code>","text":"<p>Forward pass the Gated GCN layer extract the following from the batch: x, node features with dim [n_nodes, in_dim] e, edge features with dim [n_edges, in_dim] edge_index with dim [2, n_edges]</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Union[Data, Batch]</code> <p>pyg Batch graph to pass through the layer</p> required <p>Returns:</p> Name Type Description <code>batch</code> <code>Union[Data, Batch]</code> <p>pyg Batch graph</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.layer_supports_edges","title":"<code>layer_supports_edges()</code>","text":"<p>Return a boolean specifying if the layer type supports edges or not.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.message","title":"<code>message(Dx_i, Ex_j, Ce)</code>","text":"<p>message function</p> <p>Parameters:</p> Name Type Description Default <code>Dx_i</code> <code>torch.Tensor</code> <p>tensor with dimension [n_edges, out_dim]</p> required <code>Ex_j</code> <code>torch.Tensor</code> <p>tensor with dimension [n_edges, out_dim]</p> required <code>Ce</code> <code>torch.Tensor</code> <p>tensor with dimension [n_edges, out_dim]</p> required <p>Returns:</p> Name Type Description <code>sigma_ij</code> <code>torch.Tensor</code> <p>tensor with dimension [n_edges, out_dim]</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gated_gcn_pyg.GatedGCNPyg.update","title":"<code>update(aggr_out, Ax)</code>","text":"<p>update function of the layer</p> <p>Parameters:</p> Name Type Description Default <code>aggr_out</code> <code>torch.Tensor</code> <p>the output from aggregate() function with dim [n_nodes, out_dim]</p> required <code>Ax</code> <code>torch.Tensor</code> <p>tensor with dim [n_nodes, out_dim]</p> required <p>Returns:</p> Name Type Description <code>x</code> <p>dim [n_nodes, out_dim]</p> <code>e_out</code> <p>dim [n_edges, out_dim_edges]</p>"},{"location":"api/graphium.nn/pyg_layers.html#gps-layer","title":"GPS Layer","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gps_pyg","title":"<code>graphium.nn.pyg_layers.gps_pyg</code>","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gps_pyg.GPSLayerPyg","title":"<code>GPSLayerPyg</code>","text":"<p>         Bases: <code>BaseGraphModule</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_inputs_edges","title":"<code>layer_inputs_edges: bool</code>  <code>property</code>","text":"<p>Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_outputs_edges","title":"<code>layer_outputs_edges: bool</code>  <code>property</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>False</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gps_pyg.GPSLayerPyg.out_dim_factor","title":"<code>out_dim_factor: int</code>  <code>property</code>","text":"<p>Get the factor by which the output dimension is multiplied for the next layer.</p> <p>For standard layers, this will return <code>1</code>.</p> <p>But for others, such as <code>GatLayer</code>, the output is the concatenation of the outputs from each head, so the out_dim gets multiplied by the number of heads, and this function should return the number of heads.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Always <code>1</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gps_pyg.GPSLayerPyg.__init__","title":"<code>__init__(in_dim, out_dim, in_dim_edges=None, out_dim_edges=None, activation='relu', dropout=0.0, node_residual=True, normalization='none', mpnn_type='pyg:gine', mpnn_kwargs=None, attn_type='full-attention', precision='32', biased_attention_key=None, attn_kwargs=None, droppath_rate_attn=0.0, droppath_rate_ffn=0.0, hidden_dim_scaling=4.0, **kwargs)</code>","text":"<p>GPS layer implementation in pyg adapated from https://github.com/rampasek/GraphGPS/blob/main/graphgps/layer/gps_layer.py GPS: Recipe for a General, Powerful, Scalable Graph Transformer Ladislav Ramp\u00e1\u0161ek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, Dominique Beaini https://arxiv.org/abs/2205.12454</p> <p>GPS++: An Optimised Hybrid MPNN/Transformer for Molecular Property Prediction Dominic Masters, Josef Dean, Kerstin Klaser, Zhiyi Li, Sam Maddrell-Mander, Adam Sanders, Hatem Helal, Deniz Beker, Ladislav Ramp\u00e1\u0161ek, Dominique Beaini https://arxiv.org/abs/2212.02229</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input node feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output node feature dimensions of the layer</p> required <code>in_dim</code> <code>int</code> <p>Input edge feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output edge feature dimensions of the layer</p> required <code>in_dim_edges</code> <code>Optional[int]</code> <p>input edge-feature dimensions of the layer</p> <code>None</code> <code>out_dim_edges</code> <code>Optional[int]</code> <p>output edge-feature dimensions of the layer</p> <code>None</code> <code>activation</code> <code>Union[Callable, str]</code> <p>activation function to use in the layer</p> <code>'relu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>node_residual</code> <code>Optional[bool]</code> <p>If node residual is used after on the gnn layer output</p> <code>True</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>mpnn_type</code> <code>str</code> <p>type of mpnn used, choose from \"pyg:gin\", \"pyg:gine\", \"pyg:gated-gcn\", \"pyg:pna-msgpass\" and \"pyg:mpnnplus\"</p> <code>'pyg:gine'</code> <code>mpnn_kwargs</code> <p>kwargs for mpnn layer</p> <code>None</code> <code>attn_type</code> <code>str</code> <p>type of attention used, choose from \"full-attention\" and \"none\"</p> <code>'full-attention'</code> <code>attn_kwargs</code> <p>kwargs for attention layer</p> <code>None</code> <code>droppath_rate_attn</code> <code>float</code> <p>stochastic depth drop rate for attention layer https://arxiv.org/abs/1603.09382</p> <code>0.0</code> <code>droppath_rate_ffn</code> <code>float</code> <p>stochastic depth drop rate for ffn layer https://arxiv.org/abs/1603.09382</p> <code>0.0</code> <code>mpnn_type</code> <code>str</code> <p>Type of MPNN layer to use. Choices specified in PYG_LAYERS_DICT</p> <code>'pyg:gine'</code> <code>mpnn_kwargs</code> <p>Keyword arguments to pass to the MPNN layer</p> <code>None</code> <code>attn_type</code> <code>str</code> <p>Type of attention layer to use. Choices specified in ATTENTION_LAYERS_DICT</p> <code>'full-attention'</code> <code>biased_attention_key</code> <code>Optional[str]</code> <p>indicates if biased attention is used by specifying a key corresponding to the pyg attribute in the batch (processed by the gaussian kernel encoder) default: None means biased attention is not used</p> <code>None</code> <code>attn_kwargs</code> <p>Keyword arguments to pass to the attention layer</p> <code>None</code>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gps_pyg.GPSLayerPyg.forward","title":"<code>forward(batch)</code>","text":"<p>forward function of the layer</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p>pyg Batch graphs to pass through the layer</p> required <p>Returns:</p> Name Type Description <code>batch</code> <code>Batch</code> <p>pyg Batch graphs</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gps_pyg.GPSLayerPyg.layer_supports_edges","title":"<code>layer_supports_edges()</code>","text":"<p>Return a boolean specifying if the layer type supports edges or not.</p> <p>Returns:</p> Name Type Description <code>supports_edges</code> <code>bool</code> <p>bool Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#gin-and-gine-layers","title":"GIN and GINE Layers","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg","title":"<code>graphium.nn.pyg_layers.gin_pyg</code>","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINConvPyg","title":"<code>GINConvPyg</code>","text":"<p>         Bases: <code>BaseGraphModule</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINConvPyg.layer_inputs_edges","title":"<code>layer_inputs_edges: bool</code>  <code>property</code>","text":"<p>Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>False</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINConvPyg.layer_outputs_edges","title":"<code>layer_outputs_edges: bool</code>  <code>property</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>False</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINConvPyg.out_dim_factor","title":"<code>out_dim_factor: int</code>  <code>property</code>","text":"<p>Get the factor by which the output dimension is multiplied for the next layer.</p> <p>For standard layers, this will return <code>1</code>.</p> <p>But for others, such as <code>GatLayer</code>, the output is the concatenation of the outputs from each head, so the out_dim gets multiplied by the number of heads, and this function should return the number of heads.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Always <code>1</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINConvPyg.__init__","title":"<code>__init__(in_dim, out_dim, activation='relu', dropout=0.0, normalization='none', **kwargs)</code>","text":"<p>GIN: Graph Isomorphism Networks HOW POWERFUL ARE GRAPH NEURAL NETWORKS? (Keyulu Xu, Weihua Hu, Jure Leskovec and Stefanie Jegelka, ICLR 2019) https://arxiv.org/pdf/1810.00826.pdf</p> <p>[!] code uses the pytorch-geometric implementation of GINConv</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the layer</p> required <code>activation</code> <code>Union[Callable, str]</code> <p>activation function to use in the layer</p> <code>'relu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>init_eps</code> <p>Initial :math:<code>\\epsilon</code> value, default: <code>0</code>.</p> required <code>learn_eps</code> <p>If True, :math:<code>\\epsilon</code> will be a learnable parameter.</p> required"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINConvPyg.forward","title":"<code>forward(batch)</code>","text":"<p>forward function of the layer</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Union[Data, Batch]</code> <p>pyg Batch graphs to pass through the layer</p> required <p>Returns:</p> Name Type Description <code>batch</code> <code>Union[Data, Batch]</code> <p>pyg Batch graphs</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINConvPyg.layer_supports_edges","title":"<code>layer_supports_edges()</code>","text":"<p>Return a boolean specifying if the layer type supports edges or not.</p> <p>Returns:</p> Name Type Description <code>supports_edges</code> <code>bool</code> <p>bool Always <code>False</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINEConvPyg","title":"<code>GINEConvPyg</code>","text":"<p>         Bases: <code>BaseGraphModule</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_inputs_edges","title":"<code>layer_inputs_edges: bool</code>  <code>property</code>","text":"<p>Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_outputs_edges","title":"<code>layer_outputs_edges: bool</code>  <code>property</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>False</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINEConvPyg.out_dim_factor","title":"<code>out_dim_factor: int</code>  <code>property</code>","text":"<p>Get the factor by which the output dimension is multiplied for the next layer.</p> <p>For standard layers, this will return <code>1</code>.</p> <p>But for others, such as <code>GatLayer</code>, the output is the concatenation of the outputs from each head, so the out_dim gets multiplied by the number of heads, and this function should return the number of heads.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Always <code>1</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINEConvPyg.__init__","title":"<code>__init__(in_dim, out_dim, in_dim_edges=None, activation='relu', dropout=0.0, normalization='none', **kwargs)</code>","text":"<p>GINE: Graph Isomorphism Networks with Edges Strategies for Pre-training Graph Neural Networks Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, Jure Leskovec https://arxiv.org/abs/1905.12265</p> <p>[!] code uses the pytorch-geometric implementation of GINEConv</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the layer</p> required <code>activation</code> <code>Union[Callable, str]</code> <p>activation function to use in the layer</p> <code>'relu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>init_eps</code> <p>Initial :math:<code>\\epsilon</code> value, default: <code>0</code>.</p> required <code>learn_eps</code> <p>If True, :math:<code>\\epsilon</code> will be a learnable parameter.</p> required"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINEConvPyg.forward","title":"<code>forward(batch)</code>","text":"<p>forward function of the layer</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Union[Data, Batch]</code> <p>pyg Batch graphs to pass through the layer</p> required <p>Returns:</p> Name Type Description <code>batch</code> <code>Union[Data, Batch]</code> <p>pyg Batch graphs</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.gin_pyg.GINEConvPyg.layer_supports_edges","title":"<code>layer_supports_edges()</code>","text":"<p>Return a boolean specifying if the layer type supports edges or not.</p> <p>Returns:</p> Name Type Description <code>supports_edges</code> <code>bool</code> <p>bool Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#mpnn-layer","title":"MPNN Layer","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg","title":"<code>graphium.nn.pyg_layers.mpnn_pyg</code>","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg","title":"<code>MPNNPlusPyg</code>","text":"<p>         Bases: <code>BaseGraphModule</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg.layer_inputs_edges","title":"<code>layer_inputs_edges: bool</code>  <code>property</code>","text":"<p>Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg.layer_outputs_edges","title":"<code>layer_outputs_edges: bool</code>  <code>property</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg.out_dim_factor","title":"<code>out_dim_factor: int</code>  <code>property</code>","text":"<p>Get the factor by which the output dimension is multiplied for the next layer.</p> <p>For standard layers, this will return <code>1</code>.</p> <p>But for others, such as <code>GatLayer</code>, the output is the concatenation of the outputs from each head, so the out_dim gets multiplied by the number of heads, and this function should return the number of heads.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Always <code>1</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg.__init__","title":"<code>__init__(in_dim=64, out_dim=64, activation='gelu', dropout=0.3, normalization='layer_norm', gather_from='both', scatter_to='both', node_combine_method='concat', num_node_mlp=2, mlp_expansion_ratio=4, use_edges=True, in_dim_edges=32, out_dim_edges=32, aggregation_method=['sum'], num_edge_mlp=2, use_globals=True, edge_dropout_rate=0.0035, **kwargs)</code>","text":"<pre><code>MPNNPlusPyg: InteractionNetwork layer witg edges and global feature, GPS++ type of GNN layer\nGPS++: An Optimised Hybrid MPNN/Transformer for Molecular Property Prediction\nDominic Masters, Josef Dean, Kerstin Klaser, Zhiyi Li, Sam Maddrell-Mander, Adam Sanders,\nHatem Helal, Deniz Beker, Ladislav Ramp\u00e1\u0161ek, Dominique Beaini\nhttps://arxiv.org/abs/2212.02229\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the nodes</p> <code>64</code> <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the nodes</p> <code>64</code> <code>activation</code> <code>Union[str, Callable]</code> <p>Activation function to use in the edge and node model</p> <code>'gelu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout at the end within apply_norm_activation_dropout.</p> <code>0.3</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use with the edge, node models and at the end within apply_norm_activation_dropout. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'layer_norm'</code> <code>gather_from</code> <code>str</code> <p>The method to gather features from. Could choose from: \"senders\", \"receivers\" and \"both\".</p> <code>'both'</code> <code>scatter_to</code> <code>str</code> <p>The method to scatter features to. Could choose from: \"senders\", \"receivers\" and \"both\".</p> <code>'both'</code> <code>node_combine_method</code> <code>str</code> <p>The method to combine the node features, Could choose from: \"sum\" and \"concat\".</p> <code>'concat'</code> <code>aggregation_method</code> <code>Optional[List[Union[str, Aggregation]]]</code> <p>Methods for aggregating (scatter) messages built from node and edge features. Provide a list of <code>Aggregation</code> or strings. supported strings are:</p> <ul> <li>\"sum\" / \"add\" (Default)</li> <li>\"mean\"</li> <li>\"max\"</li> <li>\"min\"</li> <li>\"softmax\"</li> <li>\"median\"</li> <li>\"std\"</li> <li>\"var\"</li> </ul> <code>['sum']</code> <code>num_node_mlp</code> <code>int</code> <p>Number of mlp layer used for node model</p> <code>2</code> <code>mlp_expansion_ratio</code> <code>int</code> <p>Expansion ratio for node and edge mlp</p> <code>4</code> <code>use_edges</code> <code>bool</code> <p>If edge features are used</p> <code>True</code> <code>in_dim_edges</code> <code>Optional[int]</code> <p>Input feature dimensions of the edges</p> <code>32</code> <code>out_dim_edges</code> <code>Optional[int]</code> <p>Output feature dimensions of the edges</p> <code>32</code> <code>num_edge_mlp</code> <code>Optional[int]</code> <p>Number of mlp layer used for edge model</p> <code>2</code> <code>edge_dropout_rate</code> <code>Optional[float]</code> <p>dropout rate for the edges</p> <code>0.0035</code>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg.aggregate_features","title":"<code>aggregate_features(input_features, senders, receivers, sender_features, receiver_features, size)</code>","text":"<p>Function to aggregate (scatter) messages built from node and edge features.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>Tensor</code> <p>Edge features of the batch</p> required <code>senders</code> <code>Union[IntTensor, LongTensor]</code> <p>Senders of the edge_index of the batch</p> required <code>receivers</code> <code>Union[IntTensor, LongTensor]</code> <p>Receivers of the edge_index of the batch</p> required <code>sender_features</code> <code>Tensor</code> <p>Senders features gathered from the gather_features function</p> required <code>receiver_features</code> <code>Tensor</code> <p>Receiver features gathered from the gather_features function</p> required <code>size</code> <code>int</code> <p>size of the aggregation, equals to the total number of nodes</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>Tensor</code> <p>Aggregated node features</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg.forward","title":"<code>forward(batch)</code>","text":"<p>Forward function of the MPNN Plus layer</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p>pyg Batch graph to pass through the layer</p> required <p>Returns:</p> Name Type Description <code>batch</code> <code>Batch</code> <p>pyg Batch graph with updated node and edge features</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg.gather_features","title":"<code>gather_features(input_features, senders, receivers)</code>","text":"<p>Function to gather node features based on the senders and receivers of the edge indices.</p> <p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>Tensor</code> <p>Node features of the batch</p> required <code>senders</code> <code>Union[IntTensor, LongTensor]</code> <p>Senders of the edge_index of the batch</p> required <code>receivers</code> <code>Union[IntTensor, LongTensor]</code> <p>Receivers of the edge_index of the batch</p> required Output <p>Gathered node features (sender and receiver) summed up or concatenated Gathered sender features Gathered receiver features</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.mpnn_pyg.MPNNPlusPyg.layer_supports_edges","title":"<code>layer_supports_edges()</code>","text":"<p>Return a boolean specifying if the layer type supports edges or not.</p> <p>Returns:</p> Name Type Description <code>supports_edges</code> <code>bool</code> <p>bool Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#pna-layer","title":"PNA Layer","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg","title":"<code>graphium.nn.pyg_layers.pna_pyg</code>","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg","title":"<code>PNAMessagePassingPyg</code>","text":"<p>         Bases: <code>MessagePassing</code>, <code>BaseGraphStructure</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_inputs_edges","title":"<code>layer_inputs_edges: bool</code>  <code>property</code>","text":"<p>Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns <code>self.edge_features</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_outputs_edges","title":"<code>layer_outputs_edges: bool</code>  <code>property</code>","text":"<p>Abstract method. Return a boolean specifying if the layer type uses edges as input or not. It is different from <code>layer_supports_edges</code> since a layer that supports edges can decide to not use them.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>False</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.out_dim_factor","title":"<code>out_dim_factor: int</code>  <code>property</code>","text":"<p>Get the factor by which the output dimension is multiplied for the next layer.</p> <p>For standard layers, this will return <code>1</code>.</p> <p>But for others, such as <code>GatLayer</code>, the output is the concatenation of the outputs from each head, so the out_dim gets multiplied by the number of heads, and this function should return the number of heads.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Always <code>1</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.__init__","title":"<code>__init__(in_dim, out_dim, aggregators, scalers, activation='relu', dropout=0.0, normalization='none', avg_d={'log': 1.0, 'lin': 1.0}, last_activation='none', posttrans_layers=1, pretrans_layers=1, in_dim_edges=0, **kwargs)</code>","text":"<p>Implementation of the message passing architecture of the PNA message passing layer, previously known as <code>PNALayerComplex</code>. This layer applies an MLP as pretransformation to the concatenation of \\([h_u, h_v, e_{uv}]\\) to generate the messages, with \\(h_u\\) the node feature, \\(h_v\\) the neighbour node features, and \\(e_{uv}\\) the edge feature between the nodes \\(u\\) and \\(v\\).</p> <p>After the pre-transformation, it aggregates the messages multiple aggregators and scalers, concatenates their results, then applies an MLP on the concatenated features.</p> <p>PNA: Principal Neighbourhood Aggregation Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, Petar Velickovic https://arxiv.org/abs/2004.05718</p> <p>[!] code adapted from pytorch-geometric implementation of PNAConv</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the layer</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the layer</p> required <code>aggregators</code> <code>List[str]</code> <p>Set of aggregation function identifiers, e.g. \"mean\", \"max\", \"min\", \"std\", \"sum\", \"var\", \"moment3\". The results from all aggregators will be concatenated.</p> required <code>scalers</code> <code>List[str]</code> <p>Set of scaling functions identifiers e.g. \"identidy\", \"amplification\", \"attenuation\" The results from all scalers will be concatenated</p> required <code>activation</code> <code>Union[Callable, str]</code> <p>activation function to use in the layer</p> <code>'relu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>avg_d</code> <code>Dict[str, float]</code> <p>Average degree of nodes in the training set, used by scalers to normalize</p> <code>{'log': 1.0, 'lin': 1.0}</code> <code>last_activation</code> <code>Union[Callable, str]</code> <p>activation function to use in the last layer of the internal MLP</p> <code>'none'</code> <code>posttrans_layers</code> <code>int</code> <p>number of layers in the MLP transformation after the aggregation</p> <code>1</code> <code>pretrans_layers</code> <code>int</code> <p>number of layers in the transformation before the aggregation</p> <code>1</code> <code>in_dim_edges</code> <code>int</code> <p>size of the edge features. If 0, edges are ignored</p> <code>0</code>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.aggregate","title":"<code>aggregate(inputs, index, edge_index, dim_size=None)</code>","text":"<p>aggregate function</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Tensor</code> <p>input features</p> required <code>index</code> <code>Tensor</code> <p>index of the nodes</p> required <code>edge_index</code> <code>Tensor</code> <p>edge index</p> required <code>dim_size</code> <code>Optional[int]</code> <p>dimension size</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tensor</code> <p>aggregated features</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.forward","title":"<code>forward(batch)</code>","text":"<p>forward function of the layer</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Union[Data, Batch]</code> <p>pyg Batch graphs</p> required <p>Returns:</p> Name Type Description <code>batch</code> <code>Union[Data, Batch]</code> <p>pyg Batch graphs</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.layer_supports_edges","title":"<code>layer_supports_edges()</code>","text":"<p>Return a boolean specifying if the layer type supports edges or not.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Always <code>True</code> for the current class</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pna_pyg.PNAMessagePassingPyg.message","title":"<code>message(x_i, x_j, edge_feat)</code>","text":"<p>message function</p> <p>Parameters:</p> Name Type Description Default <code>x_i</code> <code>Tensor</code> <p>node features</p> required <code>x_j</code> <code>Tensor</code> <p>neighbour node features</p> required <code>edge_feat</code> <code>OptTensor</code> <p>edge features</p> required <p>Returns:</p> Name Type Description <code>feat</code> <code>Tensor</code> <p>the message</p>"},{"location":"api/graphium.nn/pyg_layers.html#pooling-layers","title":"Pooling Layers","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg","title":"<code>graphium.nn.pyg_layers.pooling_pyg</code>","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg.PoolingWrapperPyg","title":"<code>PoolingWrapperPyg</code>","text":"<p>         Bases: <code>ModuleWrap</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg.PoolingWrapperPyg.forward","title":"<code>forward(g, feature, *args, **kwargs)</code>","text":"<p>forward function</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Batch</code> <p>the pyg batch graph</p> required <code>feature</code> <code>Tensor</code> <p>the node features</p> required <p>Returns:</p> Type Description <p>the pooled features</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg.VirtualNodePyg","title":"<code>VirtualNodePyg</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg.VirtualNodePyg.__init__","title":"<code>__init__(in_dim, out_dim, in_dim_edges, out_dim_edges, vn_type='sum', activation='relu', dropout=0.0, normalization='none', bias=True, residual=True, use_edges=False, **kwargs)</code>","text":"<p>The VirtualNode is a layer that pool the features of the graph, applies a neural network layer on the pooled features, then add the result back to the node features of every node.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input feature dimensions of the virtual node layer.</p> required <code>out_dim</code> <code>int</code> <p>Output feature dimensions of the virtual node layer.</p> required <code>in_dim_edges</code> <code>Optional[int]</code> <p>Input feature dimensions of the virtual node layer for the edges.</p> required <code>out_dim_edges</code> <code>Optional[int]</code> <p>Output feature dimensions of the virtual node layer for the edges.</p> required <code>vn_type</code> <code>Union[type(None), str]</code> <p>The type of the virtual node. Choices are:</p> <ul> <li>\"none\": No pooling</li> <li>\"sum\": Sum all the nodes for each graph</li> <li>\"mean\": Mean all the nodes for each graph</li> <li>\"logsum\": Mean all the nodes then multiply by log(num_nodes) for each graph</li> <li>\"max\": Max all the nodes for each graph</li> <li>\"min\": Min all the nodes for each graph</li> <li>\"std\": Standard deviation of all the nodes for each graph</li> </ul> <code>'sum'</code> <code>activation</code> <code>Union[str, Callable]</code> <p>activation function to use in the neural network layer.</p> <code>'relu'</code> <code>dropout</code> <code>float</code> <p>The ratio of units to dropout. Must be between 0 and 1</p> <code>0.0</code> <code>normalization</code> <code>Union[str, Callable]</code> <p>Normalization to use. Choices:</p> <ul> <li>\"none\" or <code>None</code>: No normalization</li> <li>\"batch_norm\": Batch normalization</li> <li>\"layer_norm\": Layer normalization</li> <li><code>Callable</code>: Any callable function</li> </ul> <code>'none'</code> <code>bias</code> <code>bool</code> <p>Whether to add a bias to the neural network</p> <code>True</code> <code>residual</code> <code>bool</code> <p>Whether all virtual nodes should be connected together via a residual connection</p> <code>True</code> <code>use_edges</code> <code>bool</code> <p>Boolean flag to select if edges are used in the global node aggregation and update of features</p> <code>False</code>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg.VirtualNodePyg.forward","title":"<code>forward(g, feat, vn_feat, edge_feat)</code>","text":"<p>Apply the virtual node layer.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Union[Data, Batch]</code> <p>PyG Graphs or Batched graphs.</p> required <code>feat</code> <code>torch.Tensor[..., N, Din]</code> <p>Node feature tensor, before convolution. <code>N</code> is the number of nodes, <code>Din</code> is the input features</p> required <code>vn_feat</code> <code>torch.Tensor[..., M, Din]</code> <p>Graph feature of the previous virtual node, or <code>None</code> <code>M</code> is the number of graphs, <code>Din</code> is the input features. It is added to the result after the MLP, as a residual connection</p> required <code>edge_feat</code> <code>torch.Tensor[..., E, Din]</code> <p>Edge feature tensor, before convolution. <code>E</code> is the number of edges, <code>Din</code> is the input features</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>feat = torch.Tensor[..., N, Dout]</code>: Node feature tensor, after convolution and residual. <code>N</code> is the number of nodes, <code>Dout</code> is the output features of the layer and residual</p> <code>Tensor</code> <p><code>vn_feat = torch.Tensor[..., M, Dout]</code>: Graph feature tensor to be used at the next virtual node, or <code>None</code> <code>M</code> is the number of graphs, <code>Dout</code> is the output features</p> <code>Tensor</code> <p><code>edge_feat = torch.Tensor[..., N, Dout]</code>: Edge feature tensor, after convolution and residual - if edges are used, otherwise returned unchanged. <code>N</code> is the number of edges, <code>Dout</code> is the output features of the layer and residual</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg.parse_pooling_layer_pyg","title":"<code>parse_pooling_layer_pyg(in_dim, pooling, feat_type='node', **kwargs)</code>","text":"<p>Select the pooling layers from a list of strings, and put them in a Module that concatenates their outputs.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>The dimension at the input layer of the pooling</p> required <code>pooling</code> <code>Union[str, List[str]]</code> <p>The list of pooling layers to use. The accepted strings are:</p> <ul> <li>\"none\": No pooling</li> <li>\"sum\": Sum all the nodes for each graph</li> <li>\"mean\": Mean all the nodes for each graph</li> <li>\"logsum\": Mean all the nodes then multiply by log(num_nodes) for each graph</li> <li>\"max\": Max all the nodes for each graph</li> <li>\"min\": Min all the nodes for each graph</li> <li>\"std\": Standard deviation of all the nodes for each graph</li> </ul> required"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg.scatter_logsum_pool","title":"<code>scatter_logsum_pool(x, batch, dim=0, dim_size=None)</code>","text":"<p>Apply pooling over the nodes in the graph using a mean aggregation, but scaled by the log of the number of nodes. This gives the same expressive power as the sum, but helps deal with graphs that are significantly larger than others by using a logarithmic scale.</p> \\[r^{(i)} = \\frac{\\log N_i}{N_i}\\sum_{k=1}^{N_i} x^{(i)}_k\\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Node feature matrix :math:<code>\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}</code>.</p> required <code>batch</code> <code>LongTensor</code> <p>Batch vector :math:<code>\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N</code>, which assigns each node to a specific example.</p> required <code>size</code> <code>int</code> <p>Batch-size :math:<code>B</code>. Automatically calculated if not given. (default: :obj:<code>None</code>)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>the pooled features tensor</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.pooling_pyg.scatter_std_pool","title":"<code>scatter_std_pool(x, batch, dim=0, dim_size=None)</code>","text":"<p>Returns batch-wise graph-level-outputs by taking the channel-wise minimum across the node dimension, so that for a single graph :math:<code>\\mathcal{G}_i</code> its output is computed by</p> <p>.. math::     \\mathbf{r}i = \\mathrm{max}{n=1}^{N_i} \\, \\mathbf{x}_n</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Node feature matrix :math:<code>\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}</code>.</p> required <code>batch</code> <code>LongTensor</code> <p>Batch vector :math:<code>\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N</code>, which assigns each node to a specific example.</p> required <code>size</code> <code>int</code> <p>Batch-size :math:<code>B</code>. Automatically calculated if not given. (default: :obj:<code>None</code>)</p> required <p>Returns:</p> Type Description <p>the pooled features tensor</p>"},{"location":"api/graphium.nn/pyg_layers.html#utils","title":"Utils","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.utils","title":"<code>graphium.nn.pyg_layers.utils</code>","text":""},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.utils.GaussianLayer","title":"<code>GaussianLayer</code>","text":"<p>         Bases: <code>nn.Module</code></p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.utils.GaussianLayer.__init__","title":"<code>__init__(num_kernels=128, in_dim=3)</code>","text":"<pre><code>Gaussian kernel function that applied on the all-to-all 3D distances.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_kernels</code> <p>Number of gaussian kernel used.</p> <code>128</code>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.utils.PreprocessPositions","title":"<code>PreprocessPositions</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>Compute 3D attention bias and 3D node features according to the 3D position information.</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.utils.PreprocessPositions.__init__","title":"<code>__init__(num_heads, embed_dim, num_kernel, in_dim=3, num_layers=2, activation='gelu', first_normalization='none')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_heads</code> <p>Number of attention heads used in self-attention.</p> required <code>embed_dim</code> <p>Hidden dimension of node features.</p> required <code>num_kernel</code> <p>Number of gaussian kernels.</p> required <code>num_layers</code> <p>The number of layers in the MLP.</p> <code>2</code> <code>activation</code> <p>The activation function used in the MLP.</p> <code>'gelu'</code> <code>first_normalization</code> <p>The normalization function used before the gaussian kernel.</p> <code>'none'</code>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.utils.PreprocessPositions.forward","title":"<code>forward(batch, max_num_nodes_per_graph, on_ipu, positions_3d_key)</code>","text":"Inputs <p>batch:     Batch object. max_num_nodes_per_graph:     Maximum number of nodes per graph. on_ipu:     If model rus on IPU. positions_3d_key:     The key of the pyg graph object that contains the 3D positions.</p>"},{"location":"api/graphium.nn/pyg_layers.html#graphium.nn.pyg_layers.utils.triplets","title":"<code>triplets(edge_index, num_nodes)</code>","text":"<p>Generates triplets from the given edge indices.     A triplet is defined as a path of length two,     such that if node A is connected to node B,     and node B is connected to node C, then there is a triplet (A, B, C).</p> <p>Parameters:</p> Name Type Description Default <code>edge_index</code> <code>LongTensor</code> <p>The edge indices.</p> required <code>num_nodes</code> <code>int</code> <p>The number of nodes.</p> required <p>Returns:</p> Name Type Description <code>col</code> <code>Tensor</code> <p>The sink node indices of edges from the edge indices.</p> <code>row</code> <code>Tensor</code> <p>The source node indices of edges from the edge indices.</p> <code>idx_i</code> <code>Tensor</code> <p>The sink node indices of the triplets.</p> <code>idx_j</code> <code>Tensor</code> <p>The middle node indices of the triplets.</p> <code>idx_k</code> <code>Tensor</code> <p>The source node indices of the triplets.</p> <code>idx_kj</code> <code>Tensor</code> <p>The indices of edges those from the source node to the middle node.</p> <code>idx_ji</code> <code>Tensor</code> <p>The indices of edges those from the middle node to the sink node.</p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html","title":"Add new positional encoding","text":"<pre><code>pos_encoding_as_features:\n    pos_types:\n      deg_pos: #example, degree centrality\n        pos_type: degree\n        normalize: False\n</code></pre> <pre><code>pe_encoders:\n    out_dim: 64\n    pool: \"sum\" #choice of pooling across multiple pe encoders\n    last_norm: None #\"batch_norm\", \"layer_norm\"\n    encoders: \n      deg_pos: #same name from the previous cell\n        encoder_type: \"mlp\" #or you can specify your own specialized encoder\n        input_keys: [\"degree\"] #same as the pos_type configured before\n        output_keys: [\"feat\"] #node feature\n        hidden_dim: 64\n        num_layers: 1\n        dropout: 0.1\n        normalization: \"none\"   #\"batch_norm\" or \"layer_norm\"\n        first_normalization: \"layer_norm\"   #\"batch_norm\" or \"layer_norm\"\n</code></pre> In\u00a0[1]: Copied! <pre>from typing import Tuple, Union, Optional\n\nfrom scipy import sparse\nfrom scipy.sparse import spmatrix\nimport numpy as np\n\ndef compute_deg(adj: Union[np.ndarray, spmatrix], normalize: bool) -&gt; np.ndarray:\n\"\"\"\n    Compute the node degree positional encoding \n\n    Parameters:\n        adj: Adjacency matrix\n        normalize: indicate if the degree across all nodes are normalized to [0,1] or not\n    Returns:\n        2D array with shape (num_nodes, 1) specifying (outgoing) degree for each node\n    \"\"\"\n    \n    #first adj convert to scipy sparse matrix if not already\n    if type(adj) is np.ndarray:\n        adj = sparse.csr_matrix(adj)\n    \n    #https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.sum.html\n    degs = adj.sum(axis=0) #sum over each row\n    \n    if (normalize): #normalize the degree sequence to [0,1]\n        degs = degs / np.max(degs)\n    return degs\n</pre> from typing import Tuple, Union, Optional  from scipy import sparse from scipy.sparse import spmatrix import numpy as np  def compute_deg(adj: Union[np.ndarray, spmatrix], normalize: bool) -&gt; np.ndarray:     \"\"\"     Compute the node degree positional encoding       Parameters:         adj: Adjacency matrix         normalize: indicate if the degree across all nodes are normalized to [0,1] or not     Returns:         2D array with shape (num_nodes, 1) specifying (outgoing) degree for each node     \"\"\"          #first adj convert to scipy sparse matrix if not already     if type(adj) is np.ndarray:         adj = sparse.csr_matrix(adj)          #https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.sum.html     degs = adj.sum(axis=0) #sum over each row          if (normalize): #normalize the degree sequence to [0,1]         degs = degs / np.max(degs)     return degs In\u00a0[2]: Copied! <pre>adj = np.identity(5) #make an identity matrix\nnormalize = True\n\ndegs = compute_deg(adj, normalize=normalize)\n\ndegs\n</pre> adj = np.identity(5) #make an identity matrix normalize = True  degs = compute_deg(adj, normalize=normalize)  degs Out[2]: <pre>matrix([[1., 1., 1., 1., 1.]])</pre> <pre><code>encoders: \n  deg_pos: \n    encoder_type: \"mlp\" \n    input_keys: [\"degree\"] \n    output_keys: [\"feat\"] # node feature\n    hidden_dim: 64\n    num_layers: 1\n    dropout: 0.1\n    normalization: \"none\"   #\"batch_norm\" or \"layer_norm\"\n    first_normalization: \"layer_norm\"   #\"batch_norm\" or \"layer_norm\"\n</code></pre> <pre><code>encoders: \n  deg_pos: \n    encoder_type: \"deg_pos_encoder\" \n    input_keys: [\"degree\"] \n    output_keys: [\"feat\"] # node feature\n    hidden_dim: 64\n    #any other keys that might be used for initialization\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#add-new-positional-encoding","title":"Add new positional encoding\u00b6","text":"<p>One of the main advantage of this library is the ability to easily incorporate novel positional encodings on the node, edge and graph level. The positional encodings are computed and feed into respective encoders and then the hidden embeddings from all pe encoders are pooled (according to if they are node, edge, or graph level) and then feed into the GNN layers as features. The designs allow any combination of positional encodings to be used by modifying the configuration file. For more details on the data processing part, please visit the design page of the doc.</p> <p>Here is the workflow for computing and processing positional encoding in the library:</p> <ol> <li><p>edit related parts in the yaml configuration file</p> </li> <li><p>compute the raw positional encoding from the graph in <code>graphium/features/positional_encoding.py</code> (from the <code>graph positional encoder</code>)</p> </li> <li><p>feed the raw positional encoding into the respective (specialized) encoders in <code>graphium/nn/encoders</code>. For example, a simple <code>MLP positional encoder</code> can be found.</p> </li> <li><p>Output the hidden embeddings of pe from the encoders in their respective output keys: <code>feat</code>(node feature), <code>edge_feat</code>(edge feature), <code>graph_feat</code>(graph feature) and potentially other keys if needed such as <code>nodepair_feat</code></p> </li> <li><p>pool the hidden embeddings with same keys together: for example, all output with <code>feat</code> key will be pooled together</p> </li> <li><p>Construct the <code>PyG Batch</code>, batch of graphs, each contain the output keys seen above, ready for use in the GNN layers</p> </li> </ol> <p>Since this library is built using PyG, we recommend looking at their Docs and Tutorials for more info.</p> <p>We start by editing the configuration file first.</p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#table-of-content","title":"Table of content\u00b6","text":"<ol> <li>edit the config file</li> <li>compute the pe from mol</li> <li>add existing encoder</li> <li>add specialized encoder</li> <li>add the keys to spaces</li> </ol>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#edit-the-yaml-configuration-file","title":"Edit the yaml Configuration File\u00b6","text":""},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#computing-raw-pe","title":"Computing Raw PE\u00b6","text":"<p>We will use the degree of each node as a positional encoding in this tutorial. First start with an existing yaml configuration file, you can find them in <code>expts/configs</code></p> <p>We first look at where in the yaml file is the raw positional encodings computed. <code>deg_pos</code> is added as an example below. You can add relevant arguments for computing the positional encoding here as well such as <code>normalize</code> in the example.</p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#specifying-encoders-for-the-pe","title":"Specifying Encoders for the PE\u00b6","text":"<p>Now we want to specify arguments for the encoders associated with the pe</p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#compute-the-positional-encoding","title":"Compute the Positional Encoding\u00b6","text":"<p>Next, we want to compute the raw degree of each node from the molecule graph.</p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#add-function-to-compute-the-pe","title":"add function to compute the pe\u00b6","text":"<p>Go to graphium/features and add a new file <code>deg.py</code> to add the function to compute the pe.</p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#test-with-toy-matrix","title":"Test with toy matrix\u00b6","text":"<p>here we will test if our code compute the degrees of each node correctly</p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#add-to-positional_encodingpy","title":"add to positional_encoding.py\u00b6","text":"<p>To compute the new pe along with all existing pe, we need to add the function we wrote to <code>graphium/feature/positional_encoding.py</code>. Modify the <code>graph_positional_encoder</code> function by adding <code>pos_type == \"degree\"</code> logic</p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#add-existing-encoder","title":"Add Existing Encoder\u00b6","text":"<p>In order to pool over all the positional encodings, we need to add encoder to process the raw computed positional encoding and ensure the output dimension from all pe encoders are the same. When designing the encoder, you can either use an existing encoder or write a specialized encoder you made</p> <p>here we can simply specify <code>MLPEncoder</code> in the yaml file and the library will automatically feed the raw positional encoding to a mlp encoder based on the input arguments. Note that in this example, the encoder takes in the pe stored at the input key <code>degree</code> and then outputs to the output key <code>feat</code></p>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#add-specialized-encoder","title":"Add Specialized Encoder\u00b6","text":"<p>You can also add specialized encoder, such as <code>laplacian_pe</code> for the laplacian eigenvectors and eigenvalues. Here, we can add a new <code>deg_pos_encoder.py</code> in <code>graphium/nn/encoders</code>. As an example and template, please see the <code>MLPEncoder</code></p> <p>Note that all new encoders must inherent from <code>BaseEncoder</code> class and implement the following abstract methods</p> <ul> <li><p><code>forward</code>: the forward function of the encoder, how to process the input</p> </li> <li><p><code>parse_input_keys</code>: how to parse the input keys</p> </li> <li><p><code>parse_output_keys</code>: how to parse the output keys</p> </li> </ul>"},{"location":"tutorials/feature_processing/add_new_positional_encoding.html#add-the-keys-to-spaces","title":"Add the Keys to Spaces\u00b6","text":"<p>In order to directly find the correct encoders from the yaml file, we need to specify which key corresponding to what class.</p> <ul> <li>add our new <code>deg_pos_encoder</code> to <code>graphium/utils/spaces.py</code> in the <code>PE_ENCODERS_DICT</code></li> <li>add our new <code>deg_pos_encoder</code> to <code>graphium/nn/architectures/encoder_manager.py</code> in the <code>PE_ENCODERS_DICT</code></li> <li>add the import of our encoder to  <code>graphium/nn/encoders/__init__.py</code></li> </ul> <p>Now we can modify the yaml file to use our new encoder</p>"},{"location":"tutorials/feature_processing/choosing_parallelization.html","title":"Choosing the right parallelization","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport joblib\n\nimport numpy as np\nimport datamol as dm\nimport pandas as pd\n\nfrom pandarallel import pandarallel\n\npandarallel.initialize(progress_bar=True, nb_workers=joblib.cpu_count())\n</pre> %load_ext autoreload %autoreload 2  import joblib  import numpy as np import datamol as dm import pandas as pd  from pandarallel import pandarallel  pandarallel.initialize(progress_bar=True, nb_workers=joblib.cpu_count()) <pre>INFO: Pandarallel will run on 240 workers.\nINFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n</pre> In\u00a0[2]: Copied! <pre># download from https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\n# data = pd.read_csv(\"/home/hadim/250k_rndm_zinc_drugs_clean_3.csv\", usecols=[\"smiles\"])\n\n# download from https://storage.googleapis.com/graphium-public/datasets/QM9/norm_qm9.csv\ndata = pd.read_csv(\"https://storage.googleapis.com/graphium-public/datasets/QM9/norm_qm9.csv\", usecols=[\"smiles\"])\n</pre> # download from https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv # data = pd.read_csv(\"/home/hadim/250k_rndm_zinc_drugs_clean_3.csv\", usecols=[\"smiles\"])  # download from https://storage.googleapis.com/graphium-public/datasets/QM9/norm_qm9.csv data = pd.read_csv(\"https://storage.googleapis.com/graphium-public/datasets/QM9/norm_qm9.csv\", usecols=[\"smiles\"]) In\u00a0[3]: Copied! <pre>rows_number_list = [250_000]\nbatch_size_list = [10, 100, 1_000, 10_000]\n\n\ndef smiles_to_unique_mol_id(smiles):\n    try:\n        mol = dm.to_mol(mol=smiles)\n        mol_id = dm.unique_id(mol)\n    except:\n        mol_id = \"\"\n    if mol_id is None:\n        mol_id = \"\"\n    return mol_id\n\n\ndef smiles_to_unique_mol_id_batch(smiles_list):\n    mol_id_list = []\n    for smiles in smiles_list:\n        mol_id_list.append(smiles_to_unique_mol_id(smiles))\n    return mol_id_list\n</pre> rows_number_list = [250_000] batch_size_list = [10, 100, 1_000, 10_000]   def smiles_to_unique_mol_id(smiles):     try:         mol = dm.to_mol(mol=smiles)         mol_id = dm.unique_id(mol)     except:         mol_id = \"\"     if mol_id is None:         mol_id = \"\"     return mol_id   def smiles_to_unique_mol_id_batch(smiles_list):     mol_id_list = []     for smiles in smiles_list:         mol_id_list.append(smiles_to_unique_mol_id(smiles))     return mol_id_list In\u00a0[4]: Copied! <pre>benchmark = []\n</pre> benchmark = [] In\u00a0[5]: Copied! <pre>for n in rows_number_list:\n    df = data.iloc[:n]\n\n    with dm.utils.perf.watch_duration(log=False) as d:\n        out = dm.parallelized(\n            smiles_to_unique_mol_id,\n            df[\"smiles\"].values,\n            progress=True,\n            n_jobs=-1,\n            scheduler=\"processes\",\n        )\n\n    datum = {\n        \"batch\": False,\n        \"batch_size\": None,\n        \"scheduler\": \"loky_processes\",\n        \"duration_minutes\": d.duration_minutes,\n        \"duration_seconds\": d.duration,\n        \"n_rows\": len(df),\n    }\n    benchmark.append(datum)\n</pre> for n in rows_number_list:     df = data.iloc[:n]      with dm.utils.perf.watch_duration(log=False) as d:         out = dm.parallelized(             smiles_to_unique_mol_id,             df[\"smiles\"].values,             progress=True,             n_jobs=-1,             scheduler=\"processes\",         )      datum = {         \"batch\": False,         \"batch_size\": None,         \"scheduler\": \"loky_processes\",         \"duration_minutes\": d.duration_minutes,         \"duration_seconds\": d.duration,         \"n_rows\": len(df),     }     benchmark.append(datum) <pre>  0%|          | 0/133885 [00:00&lt;?, ?it/s]</pre> In\u00a0[6]: Copied! <pre>for batch_size in batch_size_list:\n    for n in rows_number_list:\n        df = data.iloc[:n]\n\n        with dm.utils.perf.watch_duration(log=False) as d:\n            out = dm.parallelized_with_batches(\n                smiles_to_unique_mol_id_batch,\n                df[\"smiles\"].values,\n                batch_size=batch_size,\n                progress=True,\n                n_jobs=-1,\n                scheduler=\"processes\",\n            )\n        assert len(out) == len(df), f\"{len(out)} != {len(df)}\"\n\n        datum = {\n            \"batch\": True,\n            \"batch_size\": batch_size,\n            \"scheduler\": \"loky_processes\",\n            \"duration_minutes\": d.duration_minutes,\n            \"duration_seconds\": d.duration,\n            \"n_rows\": len(df),\n        }\n        benchmark.append(datum)\n</pre> for batch_size in batch_size_list:     for n in rows_number_list:         df = data.iloc[:n]          with dm.utils.perf.watch_duration(log=False) as d:             out = dm.parallelized_with_batches(                 smiles_to_unique_mol_id_batch,                 df[\"smiles\"].values,                 batch_size=batch_size,                 progress=True,                 n_jobs=-1,                 scheduler=\"processes\",             )         assert len(out) == len(df), f\"{len(out)} != {len(df)}\"          datum = {             \"batch\": True,             \"batch_size\": batch_size,             \"scheduler\": \"loky_processes\",             \"duration_minutes\": d.duration_minutes,             \"duration_seconds\": d.duration,             \"n_rows\": len(df),         }         benchmark.append(datum) <pre>  0%|          | 0/13388 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1338 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/133 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre>for n in rows_number_list:\n    df = data.iloc[:n]\n\n    with dm.utils.perf.watch_duration(log=False) as d:\n        _ = df[\"smiles\"].parallel_apply(smiles_to_unique_mol_id)\n\n    datum = {\n        \"batch\": False,\n        \"batch_size\": None,\n        \"scheduler\": \"pandarallel\",\n        \"duration_minutes\": d.duration_minutes,\n        \"duration_seconds\": d.duration,\n        \"n_rows\": len(df),\n    }\n    benchmark.append(datum)\n</pre> for n in rows_number_list:     df = data.iloc[:n]      with dm.utils.perf.watch_duration(log=False) as d:         _ = df[\"smiles\"].parallel_apply(smiles_to_unique_mol_id)      datum = {         \"batch\": False,         \"batch_size\": None,         \"scheduler\": \"pandarallel\",         \"duration_minutes\": d.duration_minutes,         \"duration_seconds\": d.duration,         \"n_rows\": len(df),     }     benchmark.append(datum) <pre>VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=558), Label(value='0 / 558'))), HB\u2026</pre> In\u00a0[8]: Copied! <pre>b = pd.DataFrame(benchmark)\nb[\"duration_seconds_per_mol\"] = b[\"duration_seconds\"] / b[\"n_rows\"]\n\nb.sort_values(\"duration_seconds_per_mol\")\n</pre> b = pd.DataFrame(benchmark) b[\"duration_seconds_per_mol\"] = b[\"duration_seconds\"] / b[\"n_rows\"]  b.sort_values(\"duration_seconds_per_mol\") Out[8]: batch batch_size scheduler duration_minutes duration_seconds n_rows duration_seconds_per_mol 3 True 1000.0 loky_processes 0.015375 0.922496 133885 0.000007 2 True 100.0 loky_processes 0.037034 2.222021 133885 0.000017 4 True 10000.0 loky_processes 0.055083 3.304987 133885 0.000025 5 False NaN pandarallel 0.121338 7.280271 133885 0.000054 1 True 10.0 loky_processes 0.165935 9.956113 133885 0.000074 0 False NaN loky_processes 3.639053 218.343192 133885 0.001631 In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/feature_processing/choosing_parallelization.html#choosing-the-right-parallelization","title":"Choosing the right parallelization\u00b6","text":"<p>This tutorial is meant to help you benchmark different parallel processing methods for the processing of molecules into graphs. This will allow you to chose the one most suitable for your machine, since the benchmarks vary per machine.</p> <p>In general, we find that using <code>joblib</code> with the <code>loky</code> parallel processing and a batch size of <code>1000</code> is most beneficial. The logic is abstracted into <code>datamol.parallelized_with_batches</code></p>"},{"location":"tutorials/feature_processing/choosing_parallelization.html#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/feature_processing/choosing_parallelization.html#benchmarks","title":"Benchmarks\u00b6","text":""},{"location":"tutorials/feature_processing/choosing_parallelization.html#no-batch","title":"No batch\u00b6","text":""},{"location":"tutorials/feature_processing/choosing_parallelization.html#batch","title":"Batch\u00b6","text":""},{"location":"tutorials/feature_processing/choosing_parallelization.html#results","title":"Results\u00b6","text":""},{"location":"tutorials/feature_processing/csv_to_parquet.html","title":"Convert CSV to Parquet files","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport graphium\nfrom os.path import dirname, abspath\n\nMAIN_DIR = dirname(dirname(abspath(graphium.__file__)))\n\n# TODO create funciton to read parquet, test from GCP storage (put it in path, should support gs and path, explore function from Pandas instead of parquet \"pq\")\ndef _csv_to_parquet(csv_path, parquet_path):\n    df = pd.read_csv(csv_path)\n    df.to_parquet(parquet_path)\n\n_csv_to_parquet(MAIN_DIR + '/graphium/data/QM9/micro_qm9.csv', MAIN_DIR + '/graphium/data/QM9/micro_qm9.parquet')\n</pre> import pandas as pd import graphium from os.path import dirname, abspath  MAIN_DIR = dirname(dirname(abspath(graphium.__file__)))  # TODO create funciton to read parquet, test from GCP storage (put it in path, should support gs and path, explore function from Pandas instead of parquet \"pq\") def _csv_to_parquet(csv_path, parquet_path):     df = pd.read_csv(csv_path)     df.to_parquet(parquet_path)  _csv_to_parquet(MAIN_DIR + '/graphium/data/QM9/micro_qm9.csv', MAIN_DIR + '/graphium/data/QM9/micro_qm9.parquet') In\u00a0[2]: Copied! <pre># TODO create funciton to specify if you read parquet or csv\n    # TODO replace all location with call for _read_csv and make sure to read all files if path ends with \"*\"\n    # def read_table:\n</pre>     # TODO create funciton to specify if you read parquet or csv     # TODO replace all location with call for _read_csv and make sure to read all files if path ends with \"*\"     # def read_table: In\u00a0[2]: Copied! <pre>!pwd\n</pre> !pwd <pre>/nethome/andyh/graphium/docs/tutorials/feature_processing\r\n</pre> In\u00a0[3]: Copied! <pre>import graphium\nimport os\nfrom os.path import dirname, abspath\nMAIN_DIR = dirname(dirname(abspath(graphium.__file__)))\nos.chdir(MAIN_DIR) # No need for this file\n</pre> import graphium import os from os.path import dirname, abspath MAIN_DIR = dirname(dirname(abspath(graphium.__file__))) os.chdir(MAIN_DIR) # No need for this file In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/feature_processing/timing_parallel.html","title":"Timing parallel processing","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport joblib\n\nimport numpy as np\nimport datamol as dm\nimport pandas as pd\n\nfrom pandarallel import pandarallel\n\npandarallel.initialize(progress_bar=True, nb_workers=joblib.cpu_count())\n</pre> %load_ext autoreload %autoreload 2  import joblib  import numpy as np import datamol as dm import pandas as pd  from pandarallel import pandarallel  pandarallel.initialize(progress_bar=True, nb_workers=joblib.cpu_count()) <pre>INFO: Pandarallel will run on 240 workers.\nINFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n</pre> In\u00a0[2]: Copied! <pre># download from https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv\n# data = pd.read_csv(\"/home/hadim/250k_rndm_zinc_drugs_clean_3.csv\", usecols=[\"smiles\"])\n\n# download from https://storage.googleapis.com/graphium-public/datasets/QM9/norm_qm9.csv\ndata = pd.read_csv(\"https://storage.googleapis.com/graphium-public/datasets/QM9/norm_qm9.csv\", usecols=[\"smiles\"])\n</pre> # download from https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv # data = pd.read_csv(\"/home/hadim/250k_rndm_zinc_drugs_clean_3.csv\", usecols=[\"smiles\"])  # download from https://storage.googleapis.com/graphium-public/datasets/QM9/norm_qm9.csv data = pd.read_csv(\"https://storage.googleapis.com/graphium-public/datasets/QM9/norm_qm9.csv\", usecols=[\"smiles\"]) In\u00a0[3]: Copied! <pre>rows_number_list = [250_000]\nbatch_size_list = [10, 100, 1_000, 10_000]\n\n\ndef smiles_to_unique_mol_id(smiles):\n    try:\n        mol = dm.to_mol(mol=smiles)\n        mol_id = dm.unique_id(mol)\n    except:\n        mol_id = \"\"\n    if mol_id is None:\n        mol_id = \"\"\n    return mol_id\n\n\ndef smiles_to_unique_mol_id_batch(smiles_list):\n    mol_id_list = []\n    for smiles in smiles_list:\n        mol_id_list.append(smiles_to_unique_mol_id(smiles))\n    return mol_id_list\n</pre> rows_number_list = [250_000] batch_size_list = [10, 100, 1_000, 10_000]   def smiles_to_unique_mol_id(smiles):     try:         mol = dm.to_mol(mol=smiles)         mol_id = dm.unique_id(mol)     except:         mol_id = \"\"     if mol_id is None:         mol_id = \"\"     return mol_id   def smiles_to_unique_mol_id_batch(smiles_list):     mol_id_list = []     for smiles in smiles_list:         mol_id_list.append(smiles_to_unique_mol_id(smiles))     return mol_id_list In\u00a0[4]: Copied! <pre>benchmark = []\n</pre> benchmark = [] In\u00a0[5]: Copied! <pre>for n in rows_number_list:\n    df = data.iloc[:n]\n\n    with dm.utils.perf.watch_duration(log=False) as d:\n        out = dm.parallelized(\n            smiles_to_unique_mol_id,\n            df[\"smiles\"].values,\n            progress=True,\n            n_jobs=-1,\n            scheduler=\"processes\",\n        )\n\n    datum = {\n        \"batch\": False,\n        \"batch_size\": None,\n        \"scheduler\": \"loky_processes\",\n        \"duration_minutes\": d.duration_minutes,\n        \"duration_seconds\": d.duration,\n        \"n_rows\": len(df),\n    }\n    benchmark.append(datum)\n</pre> for n in rows_number_list:     df = data.iloc[:n]      with dm.utils.perf.watch_duration(log=False) as d:         out = dm.parallelized(             smiles_to_unique_mol_id,             df[\"smiles\"].values,             progress=True,             n_jobs=-1,             scheduler=\"processes\",         )      datum = {         \"batch\": False,         \"batch_size\": None,         \"scheduler\": \"loky_processes\",         \"duration_minutes\": d.duration_minutes,         \"duration_seconds\": d.duration,         \"n_rows\": len(df),     }     benchmark.append(datum) <pre>  0%|          | 0/133885 [00:00&lt;?, ?it/s]</pre> In\u00a0[5]: Copied! <pre>for batch_size in batch_size_list:\n    for n in rows_number_list:\n        df = data.iloc[:n]\n\n        with dm.utils.perf.watch_duration(log=False) as d:\n            out = dm.parallelized_with_batches(\n                smiles_to_unique_mol_id_batch,\n                df[\"smiles\"].values,\n                batch_size=batch_size,\n                progress=True,\n                n_jobs=-1,\n                scheduler=\"processes\",\n            )\n        assert len(out) == len(df), f\"{len(out)} != {len(df)}\"\n\n        datum = {\n            \"batch\": True,\n            \"batch_size\": batch_size,\n            \"scheduler\": \"loky_processes\",\n            \"duration_minutes\": d.duration_minutes,\n            \"duration_seconds\": d.duration,\n            \"n_rows\": len(df),\n        }\n        benchmark.append(datum)\n</pre> for batch_size in batch_size_list:     for n in rows_number_list:         df = data.iloc[:n]          with dm.utils.perf.watch_duration(log=False) as d:             out = dm.parallelized_with_batches(                 smiles_to_unique_mol_id_batch,                 df[\"smiles\"].values,                 batch_size=batch_size,                 progress=True,                 n_jobs=-1,                 scheduler=\"processes\",             )         assert len(out) == len(df), f\"{len(out)} != {len(df)}\"          datum = {             \"batch\": True,             \"batch_size\": batch_size,             \"scheduler\": \"loky_processes\",             \"duration_minutes\": d.duration_minutes,             \"duration_seconds\": d.duration,             \"n_rows\": len(df),         }         benchmark.append(datum) <pre>  0%|          | 0/13388 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/1338 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/133 [00:00&lt;?, ?it/s]</pre> <pre>  0%|          | 0/13 [00:00&lt;?, ?it/s]</pre> In\u00a0[7]: Copied! <pre>for n in rows_number_list:\n    df = data.iloc[:n]\n\n    with dm.utils.perf.watch_duration(log=False) as d:\n        _ = df[\"smiles\"].parallel_apply(smiles_to_unique_mol_id)\n\n    datum = {\n        \"batch\": False,\n        \"batch_size\": None,\n        \"scheduler\": \"pandarallel\",\n        \"duration_minutes\": d.duration_minutes,\n        \"duration_seconds\": d.duration,\n        \"n_rows\": len(df),\n    }\n    benchmark.append(datum)\n</pre> for n in rows_number_list:     df = data.iloc[:n]      with dm.utils.perf.watch_duration(log=False) as d:         _ = df[\"smiles\"].parallel_apply(smiles_to_unique_mol_id)      datum = {         \"batch\": False,         \"batch_size\": None,         \"scheduler\": \"pandarallel\",         \"duration_minutes\": d.duration_minutes,         \"duration_seconds\": d.duration,         \"n_rows\": len(df),     }     benchmark.append(datum) <pre>VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=558), Label(value='0 / 558'))), HB\u2026</pre> In\u00a0[8]: Copied! <pre>b = pd.DataFrame(benchmark)\nb[\"duration_seconds_per_mol\"] = b[\"duration_seconds\"] / b[\"n_rows\"]\n\nb.sort_values(\"duration_seconds_per_mol\")\n</pre> b = pd.DataFrame(benchmark) b[\"duration_seconds_per_mol\"] = b[\"duration_seconds\"] / b[\"n_rows\"]  b.sort_values(\"duration_seconds_per_mol\") Out[8]: batch batch_size scheduler duration_minutes duration_seconds n_rows duration_seconds_per_mol 3 True 1000.0 loky_processes 0.014199 0.851930 133885 0.000006 2 True 100.0 loky_processes 0.037132 2.227947 133885 0.000017 4 True 10000.0 loky_processes 0.047438 2.846266 133885 0.000021 5 False NaN pandarallel 0.118230 7.093791 133885 0.000053 1 True 10.0 loky_processes 0.222177 13.330603 133885 0.000100 0 False NaN loky_processes 4.002346 240.140754 133885 0.001794 In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/feature_processing/timing_parallel.html#timing-parallel-processing","title":"Timing parallel processing\u00b6","text":"<p>This tutorial is meant to help you benchmark different parallel processing methods for the processing of molecules into graphs. This will allow you to chose the one most suitable for your machine, since the benchmarks vary per machine.</p> <p>In general, we find that using <code>joblib</code> with the <code>loky</code> parallel processing and a batch size of <code>1000</code> is most beneficial. The logic is abstracted into <code>datamol.parallelized_with_batches</code></p>"},{"location":"tutorials/feature_processing/timing_parallel.html#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/feature_processing/timing_parallel.html#benchmarks","title":"Benchmarks\u00b6","text":""},{"location":"tutorials/feature_processing/timing_parallel.html#no-batch","title":"No batch\u00b6","text":""},{"location":"tutorials/feature_processing/timing_parallel.html#batch","title":"Batch\u00b6","text":""},{"location":"tutorials/feature_processing/timing_parallel.html#results","title":"Results\u00b6","text":""},{"location":"tutorials/gnn/add_new_gnn_layers.html","title":"Creating GNN layers","text":"In\u00a0[81]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_scatter import scatter\nfrom torch_geometric.typing import (\n    OptPairTensor,\n    OptTensor\n)\n\nfrom copy import deepcopy\nfrom typing import Callable, Union, Optional, List\nfrom graphium.nn.base_graph_layer import BaseGraphStructure\nfrom graphium.nn.base_layers import FCLayer\nfrom graphium.utils.decorators import classproperty\n\n_ = torch.manual_seed(42)\n</pre> %load_ext autoreload %autoreload 2  import torch from torch import Tensor from torch_geometric.data import Data, Batch from torch_geometric.nn.conv import MessagePassing from torch_scatter import scatter from torch_geometric.typing import (     OptPairTensor,     OptTensor )  from copy import deepcopy from typing import Callable, Union, Optional, List from graphium.nn.base_graph_layer import BaseGraphStructure from graphium.nn.base_layers import FCLayer from graphium.utils.decorators import classproperty  _ = torch.manual_seed(42) <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[82]: Copied! <pre>in_dim = 5          # Input node-feature dimensions\nout_dim = 11        # Desired output node-feature dimensions\nin_dim_edges = 13   # Input edge-feature dimensions\n\n\n# Let's create 2 simple pyg graphs. \n# start by specifying the edges with edge index\nedge_idx1 = torch.tensor([[0, 1, 2],\n                          [1, 2, 3]])\nedge_idx2 = torch.tensor([[2, 0, 0, 1],\n                          [0, 1, 2, 0]])\n\n# specify the node features, convention with variable x\nx1 = torch.randn(edge_idx1.max() + 1, in_dim, dtype=torch.float32)\nx2 = torch.randn(edge_idx2.max() + 1, in_dim, dtype=torch.float32)\n\n# specify the edge features in e\ne1 = torch.randn(edge_idx1.shape[-1], in_dim_edges, dtype=torch.float32)\ne2 = torch.randn(edge_idx2.shape[-1], in_dim_edges, dtype=torch.float32)\n\n# make the pyg graph objects with our constructed features\ng1 = Data(feat=x1, edge_index=edge_idx1, edge_feat=e1)\ng2 = Data(feat=x2, edge_index=edge_idx2, edge_feat=e2)\n\n# put the two graphs into a Batch graph\nbg = Batch.from_data_list([g1, g2])\n\n# The batched graph will show as a single graph with 7 nodes\nprint(bg)\n</pre> in_dim = 5          # Input node-feature dimensions out_dim = 11        # Desired output node-feature dimensions in_dim_edges = 13   # Input edge-feature dimensions   # Let's create 2 simple pyg graphs.  # start by specifying the edges with edge index edge_idx1 = torch.tensor([[0, 1, 2],                           [1, 2, 3]]) edge_idx2 = torch.tensor([[2, 0, 0, 1],                           [0, 1, 2, 0]])  # specify the node features, convention with variable x x1 = torch.randn(edge_idx1.max() + 1, in_dim, dtype=torch.float32) x2 = torch.randn(edge_idx2.max() + 1, in_dim, dtype=torch.float32)  # specify the edge features in e e1 = torch.randn(edge_idx1.shape[-1], in_dim_edges, dtype=torch.float32) e2 = torch.randn(edge_idx2.shape[-1], in_dim_edges, dtype=torch.float32)  # make the pyg graph objects with our constructed features g1 = Data(feat=x1, edge_index=edge_idx1, edge_feat=e1) g2 = Data(feat=x2, edge_index=edge_idx2, edge_feat=e2)  # put the two graphs into a Batch graph bg = Batch.from_data_list([g1, g2])  # The batched graph will show as a single graph with 7 nodes print(bg) <pre>DataBatch(edge_index=[2, 7], feat=[7, 5], edge_feat=[7, 13], batch=[7], ptr=[3])\n</pre> In\u00a0[83]: Copied! <pre># inherit from message passing class from pyg \n# inherit from BaseGraphStructure\n# this example is also based of : https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/simple_conv.html#SimpleConv.forward\n\nclass SimpleMeanLayer(MessagePassing, BaseGraphStructure):\n    def __init__(self, \n                 in_dim: int, \n                 out_dim: int, \n                 activation: Union[Callable, str] = \"relu\", \n                 dropout: float = 0.0, \n                 normalization: Union[str, Callable] = \"none\",\n                 aggr: str = \"mean\",\n                ):\nr\"\"\"\n        add documentation in the format shown here for this layer to be automatically added to the graphium api reference\n        the type information will also be automatically shown in the doc\n        Parameters:\n\n            in_dim:\n                Input feature dimensions of the layer\n\n            out_dim:\n                Output feature dimensions of the layer\n\n            activation:\n                activation function to use in the layer\n\n            dropout:\n                The ratio of units to dropout. Must be between 0 and 1\n\n            normalization:\n                Normalization to use. Choices:\n\n                - \"none\" or `None`: No normalization\n                - \"batch_norm\": Batch normalization\n                - \"layer_norm\": Layer normalization\n                - `Callable`: Any callable function\n            aggr:\n                what aggregation to use (\"add\", \"mean\" or \"max\")\n        \"\"\"\n        # Initialize the parent class\n        MessagePassing.__init__(self, node_dim=0, aggr=aggr)\n        BaseGraphStructure.__init__(self,\n                                    in_dim=in_dim, \n                                    out_dim=out_dim, \n                                    activation=activation,\n                                    dropout=dropout, \n                                    normalization=normalization)\n        \n        self.aggr = aggr\n        self._initialize_activation_dropout_norm()\n        # Create the mlp layer \n        # https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP\n        self.transform = FCLayer(in_dim=in_dim, out_dim=out_dim)\n            \n    # define the forward function \n    def forward(self, \n                batch: Union[Data, Batch],\n               ) -&gt; Union[Data, Batch]:\nr\"\"\"\n        similarly add documentation to the functions in the class\n        Parameters:\n            batch: pyg Batch graphs to pass through the layer\n        Returns:\n            batch: pyg Batch graphs\n        \"\"\"\n        \n        x = batch.feat\n        edge_index = batch.edge_index\n        \n        if isinstance(x, Tensor):\n            x: OptPairTensor = (x, x)\n\n        # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\n        out = self.propagate(edge_index, x=x)\n        out = self.transform(out)\n        out = self.apply_norm_activation_dropout(out, batch_idx=batch.batch)\n        batch.feat = out\n        return batch\n    \n    def message(self, x_j):\n\n        return x_j\n\n    # Finally, we define all the virtual properties according to how the class works with proper documentation of course\n    @classproperty\n    def layer_supports_edges(cls) -&gt; bool:\nr\"\"\"\n        Return a boolean specifying if the layer type supports edges or not.\n\n        Returns:\n\n            bool:\n                False for the current class\n        \"\"\"\n        return False\n\n    @property\n    def layer_inputs_edges(self) -&gt; bool:\nr\"\"\"\n        Returns:\n\n            bool:\n                Returns False\n        \"\"\"\n        return False\n            \n    @property\n    def layer_outputs_edges(self) -&gt; bool:\nr\"\"\"\n        Returns:\n\n            bool:\n                Always ``False`` for the current class\n        \"\"\"\n        return False\n\n    @property\n    def out_dim_factor(self) -&gt; int:\nr\"\"\"\n        Get the factor by which the output dimension is multiplied for\n        the next layer.\n\n        For standard layers, this will return ``1``.\n        Returns:\n\n            int:\n                Always ``1`` for the current class\n        \"\"\"\n        return 1\n</pre> # inherit from message passing class from pyg  # inherit from BaseGraphStructure # this example is also based of : https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/simple_conv.html#SimpleConv.forward  class SimpleMeanLayer(MessagePassing, BaseGraphStructure):     def __init__(self,                   in_dim: int,                   out_dim: int,                   activation: Union[Callable, str] = \"relu\",                   dropout: float = 0.0,                   normalization: Union[str, Callable] = \"none\",                  aggr: str = \"mean\",                 ):         r\"\"\"         add documentation in the format shown here for this layer to be automatically added to the graphium api reference         the type information will also be automatically shown in the doc                  Parameters:              in_dim:                 Input feature dimensions of the layer              out_dim:                 Output feature dimensions of the layer              activation:                 activation function to use in the layer              dropout:                 The ratio of units to dropout. Must be between 0 and 1              normalization:                 Normalization to use. Choices:                  - \"none\" or `None`: No normalization                 - \"batch_norm\": Batch normalization                 - \"layer_norm\": Layer normalization                 - `Callable`: Any callable function             aggr:                 what aggregation to use (\"add\", \"mean\" or \"max\")         \"\"\"         # Initialize the parent class         MessagePassing.__init__(self, node_dim=0, aggr=aggr)         BaseGraphStructure.__init__(self,                                     in_dim=in_dim,                                      out_dim=out_dim,                                      activation=activation,                                     dropout=dropout,                                      normalization=normalization)                  self.aggr = aggr         self._initialize_activation_dropout_norm()         # Create the mlp layer          # https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP         self.transform = FCLayer(in_dim=in_dim, out_dim=out_dim)                  # define the forward function      def forward(self,                  batch: Union[Data, Batch],                ) -&gt; Union[Data, Batch]:         r\"\"\"         similarly add documentation to the functions in the class                  Parameters:             batch: pyg Batch graphs to pass through the layer         Returns:             batch: pyg Batch graphs         \"\"\"                  x = batch.feat         edge_index = batch.edge_index                  if isinstance(x, Tensor):             x: OptPairTensor = (x, x)          # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)         out = self.propagate(edge_index, x=x)         out = self.transform(out)         out = self.apply_norm_activation_dropout(out, batch_idx=batch.batch)         batch.feat = out         return batch          def message(self, x_j):          return x_j      # Finally, we define all the virtual properties according to how the class works with proper documentation of course     @classproperty     def layer_supports_edges(cls) -&gt; bool:         r\"\"\"         Return a boolean specifying if the layer type supports edges or not.          Returns:              bool:                 False for the current class         \"\"\"         return False      @property     def layer_inputs_edges(self) -&gt; bool:         r\"\"\"         Returns:              bool:                 Returns False         \"\"\"         return False                  @property     def layer_outputs_edges(self) -&gt; bool:         r\"\"\"         Returns:              bool:                 Always ``False`` for the current class         \"\"\"         return False      @property     def out_dim_factor(self) -&gt; int:         r\"\"\"         Get the factor by which the output dimension is multiplied for         the next layer.          For standard layers, this will return ``1``.         Returns:              int:                 Always ``1`` for the current class         \"\"\"         return 1 In\u00a0[84]: Copied! <pre>graph = deepcopy(bg)\nprint(graph.feat.shape)\n\nlayer = SimpleMeanLayer(\n            in_dim=in_dim, \n            out_dim=out_dim, \n            activation=\"relu\", \n            dropout=.3, \n            normalization=\"batch_norm\")\n\ngraph = layer(graph)\nprint(graph.feat.shape)\n</pre> graph = deepcopy(bg) print(graph.feat.shape)  layer = SimpleMeanLayer(             in_dim=in_dim,              out_dim=out_dim,              activation=\"relu\",              dropout=.3,              normalization=\"batch_norm\")  graph = layer(graph) print(graph.feat.shape) <pre>torch.Size([7, 5])\ntorch.Size([7, 11])\n</pre> In\u00a0[85]: Copied! <pre># inherent from message passing class from pyg \n# inherent from BaseGraphStructure\n# adapting example from graphium/nn/pyg_layers/png_pyg.py\n\nclass ComplexMeanLayer(MessagePassing, BaseGraphStructure):\n    def __init__(self, \n                 in_dim: int, \n                 out_dim: int, \n                 in_dim_edges: int, \n                 activation: Union[Callable, str] = \"relu\", \n                 dropout: float = 0.0, \n                 normalization: Union[str, Callable] = \"none\",\n                 aggr: str = \"mean\",\n                ):\nr\"\"\"\n        add documentation in the format shown here for this layer to be automatically added to the graphium api reference\n        the type information will also be automatically shown in the doc\n        Parameters:\n\n            in_dim:\n                Input feature dimensions of the layer\n\n            out_dim:\n                Output feature dimensions of the layer\n            in_dim_edges:\n                Input edge feature dimension of the layer\n\n            activation:\n                activation function to use in the layer\n\n            dropout:\n                The ratio of units to dropout. Must be between 0 and 1\n\n            normalization:\n                Normalization to use. Choices:\n\n                - \"none\" or `None`: No normalization\n                - \"batch_norm\": Batch normalization\n                - \"layer_norm\": Layer normalization\n                - `Callable`: Any callable function\n            aggr:\n                what aggregation to use (\"add\", \"mean\" or \"max\")\n        \"\"\"\n        # Initialize the parent class\n        MessagePassing.__init__(self, node_dim=0, aggr=aggr)\n        BaseGraphStructure.__init__(self,\n                                    in_dim=in_dim, \n                                    out_dim=out_dim, \n                                    activation=activation,\n                                    dropout=dropout, \n                                    normalization=normalization)\n        \n        self.aggr = aggr\n        self._initialize_activation_dropout_norm()\n        # Create the mlp layer \n        # https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP\n        self.transform = FCLayer(in_dim=(in_dim + in_dim_edges), out_dim=out_dim)\n            \n    # define the forward function \n    def forward(self, \n                batch: Union[Data, Batch],\n               ) -&gt; Union[Data, Batch]:\nr\"\"\"\n        similarly add documentation to the functions in the class\n        Parameters:\n            batch: pyg Batch graphs to pass through the layer\n        Returns:\n            batch: pyg Batch graphs\n        \"\"\"        \n        x = batch.feat\n        edge_index = batch.edge_index\n        edge_feat = batch.edge_feat\n        \n        if isinstance(x, Tensor):\n            x: OptPairTensor = (x, x)\n\n        # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\n        out = self.propagate(edge_index, x=x, edge_feat=edge_feat, size=None)\n        out = self.transform(out)\n        out = self.apply_norm_activation_dropout(out, batch_idx=batch.batch)\n        batch.feat = out\n        return batch\n    \n    def message(self, x_j: Tensor, edge_feat: OptTensor) -&gt; Tensor:\nr\"\"\"\n        message function\n\n        Parameters:\n            x_i: node features\n            x_j: neighbour node features\n            edge_feat: edge features\n        Returns:\n            feat: the message\n        \"\"\"\n        feat = torch.cat([x_j, edge_feat], dim=-1)\n        return feat\n    \n    def aggregate(\n        self,\n        inputs: Tensor,\n        index: Tensor,\n        edge_index: Tensor,\n        dim_size: Optional[int] = None,\n    ) -&gt; Tensor:\nr\"\"\"\n        aggregate function\n        Parameters:\n            inputs: input features\n            index: index of the nodes\n            edge_index: edge index\n            dim_size: dimension size\n        Returns:\n            out: aggregated features\n        \"\"\"\n        out = scatter(inputs, index, 0, None, dim_size, reduce=self.aggr)\n        return out\n    \n\n    # Finally, we define all the virtual properties according to how the class works with proper documentation of course\n    @classproperty\n    def layer_supports_edges(cls) -&gt; bool:\nr\"\"\"\n        Return a boolean specifying if the layer type supports edges or not.\n\n        Returns:\n\n            bool:\n                False for the current class\n        \"\"\"\n        return True\n\n    @property\n    def layer_inputs_edges(self) -&gt; bool:\nr\"\"\"\n        Returns:\n\n            bool:\n                Returns True\n        \"\"\"\n        return True\n            \n    @property\n    def layer_outputs_edges(self) -&gt; bool:\nr\"\"\"\n        Returns:\n\n            bool:\n                Always ``True`` for the current class\n        \"\"\"\n        return True\n\n    @property\n    def out_dim_factor(self) -&gt; int:\nr\"\"\"\n        Get the factor by which the output dimension is multiplied for\n        the next layer.\n\n        For standard layers, this will return ``1``.\n        Returns:\n\n            int:\n                Always ``1`` for the current class\n        \"\"\"\n        return 1\n</pre> # inherent from message passing class from pyg  # inherent from BaseGraphStructure # adapting example from graphium/nn/pyg_layers/png_pyg.py  class ComplexMeanLayer(MessagePassing, BaseGraphStructure):     def __init__(self,                   in_dim: int,                   out_dim: int,                   in_dim_edges: int,                   activation: Union[Callable, str] = \"relu\",                   dropout: float = 0.0,                   normalization: Union[str, Callable] = \"none\",                  aggr: str = \"mean\",                 ):         r\"\"\"         add documentation in the format shown here for this layer to be automatically added to the graphium api reference         the type information will also be automatically shown in the doc                  Parameters:              in_dim:                 Input feature dimensions of the layer              out_dim:                 Output feature dimensions of the layer                          in_dim_edges:                 Input edge feature dimension of the layer              activation:                 activation function to use in the layer              dropout:                 The ratio of units to dropout. Must be between 0 and 1              normalization:                 Normalization to use. Choices:                  - \"none\" or `None`: No normalization                 - \"batch_norm\": Batch normalization                 - \"layer_norm\": Layer normalization                 - `Callable`: Any callable function             aggr:                 what aggregation to use (\"add\", \"mean\" or \"max\")         \"\"\"         # Initialize the parent class         MessagePassing.__init__(self, node_dim=0, aggr=aggr)         BaseGraphStructure.__init__(self,                                     in_dim=in_dim,                                      out_dim=out_dim,                                      activation=activation,                                     dropout=dropout,                                      normalization=normalization)                  self.aggr = aggr         self._initialize_activation_dropout_norm()         # Create the mlp layer          # https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP         self.transform = FCLayer(in_dim=(in_dim + in_dim_edges), out_dim=out_dim)                  # define the forward function      def forward(self,                  batch: Union[Data, Batch],                ) -&gt; Union[Data, Batch]:         r\"\"\"         similarly add documentation to the functions in the class                  Parameters:             batch: pyg Batch graphs to pass through the layer         Returns:             batch: pyg Batch graphs         \"\"\"                 x = batch.feat         edge_index = batch.edge_index         edge_feat = batch.edge_feat                  if isinstance(x, Tensor):             x: OptPairTensor = (x, x)          # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)         out = self.propagate(edge_index, x=x, edge_feat=edge_feat, size=None)         out = self.transform(out)         out = self.apply_norm_activation_dropout(out, batch_idx=batch.batch)         batch.feat = out         return batch          def message(self, x_j: Tensor, edge_feat: OptTensor) -&gt; Tensor:         r\"\"\"         message function          Parameters:             x_i: node features             x_j: neighbour node features             edge_feat: edge features         Returns:             feat: the message         \"\"\"         feat = torch.cat([x_j, edge_feat], dim=-1)         return feat          def aggregate(         self,         inputs: Tensor,         index: Tensor,         edge_index: Tensor,         dim_size: Optional[int] = None,     ) -&gt; Tensor:         r\"\"\"         aggregate function         Parameters:             inputs: input features             index: index of the nodes             edge_index: edge index             dim_size: dimension size         Returns:             out: aggregated features         \"\"\"         out = scatter(inputs, index, 0, None, dim_size, reduce=self.aggr)         return out           # Finally, we define all the virtual properties according to how the class works with proper documentation of course     @classproperty     def layer_supports_edges(cls) -&gt; bool:         r\"\"\"         Return a boolean specifying if the layer type supports edges or not.          Returns:              bool:                 False for the current class         \"\"\"         return True      @property     def layer_inputs_edges(self) -&gt; bool:         r\"\"\"         Returns:              bool:                 Returns True         \"\"\"         return True                  @property     def layer_outputs_edges(self) -&gt; bool:         r\"\"\"         Returns:              bool:                 Always ``True`` for the current class         \"\"\"         return True      @property     def out_dim_factor(self) -&gt; int:         r\"\"\"         Get the factor by which the output dimension is multiplied for         the next layer.          For standard layers, this will return ``1``.         Returns:              int:                 Always ``1`` for the current class         \"\"\"         return 1 In\u00a0[86]: Copied! <pre>graph = deepcopy(bg)\nprint(graph.feat.shape)\nprint(graph.edge_feat.shape)\n\nlayer = ComplexMeanLayer(\n            in_dim=in_dim, \n            out_dim=out_dim, \n            in_dim_edges=in_dim_edges,\n            activation=\"relu\", \n            dropout=.3, \n            normalization=\"batch_norm\")\n\ngraph = layer(graph)\nprint(graph.feat.shape)\n</pre> graph = deepcopy(bg) print(graph.feat.shape) print(graph.edge_feat.shape)  layer = ComplexMeanLayer(             in_dim=in_dim,              out_dim=out_dim,              in_dim_edges=in_dim_edges,             activation=\"relu\",              dropout=.3,              normalization=\"batch_norm\")  graph = layer(graph) print(graph.feat.shape) <pre>torch.Size([7, 5])\ntorch.Size([7, 13])\ntorch.Size([7, 11])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/gnn/add_new_gnn_layers.html#creating-gnn-layers","title":"Creating GNN layers\u00b6","text":"<p>One of the primary advantage of this library is the fact that GNN layers are independent from model architecture, thus allowing more flexibility with the code by easily swapping different layer types as hyper-parameters. However, this requires that the layers be implemented using the PyG library, and must be inherited from the class <code>BaseGraphStructure</code>, which standardizes the inputs, outputs and properties of the layers. Thus, the architecture can be handled independantly using the class <code>FeedForwardPyg</code> or other custom classes.</p> <p>We will first start by a simple layer that does not use edges, to a more complex layer that uses edges.</p> <p>Since these examples are built on top of PyG, we recommend looking at their Docs and Tutorials for more info.</p>"},{"location":"tutorials/gnn/add_new_gnn_layers.html#table-of-content","title":"Table of content\u00b6","text":"<ol> <li>Define test graph</li> <li>Create simple layer</li> <li>Test simple layer</li> <li>Create complex layer</li> <li>Test complex layer</li> </ol>"},{"location":"tutorials/gnn/add_new_gnn_layers.html#define-synthetic-test-graph","title":"Define synthetic test graph\u00b6","text":"<p>We define below a small batched graph on which we can test the created layers. You can also use synthetic graphs to define unit tests</p>"},{"location":"tutorials/gnn/add_new_gnn_layers.html#creating-a-simple-layer","title":"Creating a simple layer\u00b6","text":"<p>Here, we will show how to create a GNN layer that simples does a mean aggregation on the neighbouring features.</p> <p>First, for the layer to be fully compatible with the flexible architecture provided by <code>FeedForwardPyg</code>, it needs to inherit from the class <code>BaseGraphStructure</code>. This base-layer has multiple virtual methods that must be implemented in any class that inherits from it.</p> <p>The virtual methods are below</p> <ul> <li><code>layer_supports_edges</code>: We want to return <code>False</code> since our layer doesn't support edges</li> <li><code>layer_inputs_edges</code>: We want to return <code>False</code> since our layer doesn't input edges</li> <li><code>layer_outputs_edges</code>: We want to return <code>False</code> since our layer doesn't output edges</li> <li><code>out_dim_factor</code>: We want to return <code>1</code> since the output dimension does not depend on internal parameters.</li> </ul> <p>The example is given below</p>"},{"location":"tutorials/gnn/add_new_gnn_layers.html#test-our-simple-layer","title":"Test our simple layer\u00b6","text":"<p>Now, we are ready to test the <code>SimpleMeanLayer</code> on our constructed graphs. Note that in this example, we ignore the edge features since they are not supported.</p>"},{"location":"tutorials/gnn/add_new_gnn_layers.html#creating-a-complex-layer-with-edges","title":"Creating a complex layer with edges\u00b6","text":"<p>Here, we will show how to create a GNN layer that does a mean aggregation on the neighbouring features, concatenated to the edge features with their neighbours. In that case, only the node features will change, and the network will not update the edge features.</p> <p>The virtual methods will have different outputs</p> <ul> <li><code>layer_supports_edges</code>: We want to return <code>True</code> since our layer does support edges</li> <li><code>layer_inputs_edges</code>: We want to return <code>True</code> since our layer does input edges</li> <li><code>layer_outputs_edges</code>: We want to return <code>False</code> since our layer will not output new edges</li> <li><code>out_dim_factor</code>: We want to return <code>1</code> since the output dimension does not depend on internal parameters.</li> </ul> <p>The example is given below</p>"},{"location":"tutorials/gnn/add_new_gnn_layers.html#test-our-complex-layer","title":"Test our complex layer\u00b6","text":"<p>Now, we are ready to test the <code>ComplexMeanLayer</code> on our constructed graphs. Note that in this example, we utilize the edge features and node features</p>"},{"location":"tutorials/gnn/making_gnn_networks.html","title":"Making GNN Networks","text":"In\u00a0[114]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport torch\nfrom copy import deepcopy\n\nfrom torch_geometric.data import Data, Batch\n\nfrom graphium.nn.architectures import FullGraphMultiTaskNetwork\n\n_ = torch.manual_seed(42)\n</pre> %load_ext autoreload %autoreload 2  import torch from copy import deepcopy  from torch_geometric.data import Data, Batch  from graphium.nn.architectures import FullGraphMultiTaskNetwork  _ = torch.manual_seed(42) <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> <p>We will first create some simple batched graphs that will be used accross the examples. Here, <code>bg</code> is a batch containing 2 graphs with random node features.</p> In\u00a0[115]: Copied! <pre>in_dim = 5          # Input node-feature dimensions\nin_dim_edges = 13   # Input edge-feature dimensions\nout_dim = 11        # Desired output node-feature dimensions\n\n\n# Let's create 2 simple pyg graphs. \n# start by specifying the edges with edge index\nedge_idx1 = torch.tensor([[0, 1, 2],\n                          [1, 2, 3]])\nedge_idx2 = torch.tensor([[2, 0, 0, 1],\n                          [0, 1, 2, 0]])\n\n# specify the node features, convention with variable x\nx1 = torch.randn(edge_idx1.max() + 1, in_dim, dtype=torch.float32)\nx2 = torch.randn(edge_idx2.max() + 1, in_dim, dtype=torch.float32)\n\n# specify the edge features in e\ne1 = torch.randn(edge_idx1.shape[-1], in_dim_edges, dtype=torch.float32)\ne2 = torch.randn(edge_idx2.shape[-1], in_dim_edges, dtype=torch.float32)\n\n# make the pyg graph objects with our constructed features\ng1 = Data(feat=x1, edge_index=edge_idx1, edge_feat=e1)\ng2 = Data(feat=x2, edge_index=edge_idx2, edge_feat=e2)\n\n# put the two graphs into a Batch graph\nbg = Batch.from_data_list([g1, g2])\n\n# The batched graph will show as a single graph with 7 nodes\nprint(bg)\n</pre> in_dim = 5          # Input node-feature dimensions in_dim_edges = 13   # Input edge-feature dimensions out_dim = 11        # Desired output node-feature dimensions   # Let's create 2 simple pyg graphs.  # start by specifying the edges with edge index edge_idx1 = torch.tensor([[0, 1, 2],                           [1, 2, 3]]) edge_idx2 = torch.tensor([[2, 0, 0, 1],                           [0, 1, 2, 0]])  # specify the node features, convention with variable x x1 = torch.randn(edge_idx1.max() + 1, in_dim, dtype=torch.float32) x2 = torch.randn(edge_idx2.max() + 1, in_dim, dtype=torch.float32)  # specify the edge features in e e1 = torch.randn(edge_idx1.shape[-1], in_dim_edges, dtype=torch.float32) e2 = torch.randn(edge_idx2.shape[-1], in_dim_edges, dtype=torch.float32)  # make the pyg graph objects with our constructed features g1 = Data(feat=x1, edge_index=edge_idx1, edge_feat=e1) g2 = Data(feat=x2, edge_index=edge_idx2, edge_feat=e2)  # put the two graphs into a Batch graph bg = Batch.from_data_list([g1, g2])  # The batched graph will show as a single graph with 7 nodes print(bg)  <pre>DataBatch(edge_index=[2, 7], feat=[7, 5], edge_feat=[7, 13], batch=[7], ptr=[3])\n</pre> In\u00a0[116]: Copied! <pre>temp_dim_1 = 23\ntemp_dim_2 = 17\n\npre_nn_kwargs = {\n    \"in_dim\": in_dim,\n    \"out_dim\": temp_dim_1,\n    \"hidden_dims\": 4,\n    \"depth\": 2,\n    \"activation\": 'relu',\n    \"last_activation\": \"none\",\n    \"dropout\": 0.2\n}\n\ngnn_kwargs = {\n    \"in_dim\": temp_dim_1,\n    \"out_dim\": temp_dim_2,\n    \"hidden_dims\": 5,\n    \"depth\": 1,\n    \"activation\": 'gelu',\n    \"last_activation\": None,\n    \"dropout\": 0.1,\n    \"normalization\": 'layer_norm',\n    \"last_normalization\": 'layer_norm',\n    \"residual_type\": 'simple',\n    \"virtual_node\": None,\n    \"layer_type\": 'pyg:gcn',\n    \"layer_kwargs\": None\n}\n\ntask_heads_kwargs = {\n    \"graph-task-1\": {\n        \"task_level\": 'graph',\n        \"out_dim\": 3,\n        \"hidden_dims\": 32,\n        \"depth\": 2,\n        \"activation\": 'relu',\n        \"last_activation\": None,\n        \"dropout\": 0.1,\n        \"normalization\": None,\n        \"last_normalization\": None,\n        \"residual_type\": \"none\"\n    },\n    \"graph-task-2\": {\n        \"task_level\": 'graph',\n        \"out_dim\": 4,\n        \"hidden_dims\": 32,\n        \"depth\": 2,\n        \"activation\": 'relu',\n        \"last_activation\": None,\n        \"dropout\": 0.1,\n        \"normalization\": None,\n        \"last_normalization\": None,\n        \"residual_type\": \"none\"\n    },\n    \"node-task-1\": {\n        \"task_level\": 'node',\n        \"out_dim\": 2,\n        \"hidden_dims\": 32,\n        \"depth\": 2,\n        \"activation\": 'relu',\n        \"last_activation\": None,\n        \"dropout\": 0.1,\n        \"normalization\": None,\n        \"last_normalization\": None,\n        \"residual_type\": \"none\"\n    }\n}\n\ngraph_output_nn_kwargs = {\n    \"graph\": {\n        \"pooling\": ['sum'],\n        \"out_dim\": temp_dim_2,\n        \"hidden_dims\": temp_dim_2,\n        \"depth\": 1,\n        \"activation\": 'relu',\n        \"last_activation\": None,\n        \"dropout\": 0.1,\n        \"normalization\": None,\n        \"last_normalization\": None,\n        \"residual_type\": \"none\"\n    },\n    \"node\": {\n        \"pooling\": None,\n        \"out_dim\": temp_dim_2,\n        \"hidden_dims\": temp_dim_2,\n        \"depth\": 1,\n        \"activation\": 'relu',\n        \"last_activation\": None,\n        \"dropout\": 0.1,\n        \"normalization\": None,\n        \"last_normalization\": None,\n        \"residual_type\": \"none\"\n    }\n}\n    \n\ngnn_net = FullGraphMultiTaskNetwork(\n    gnn_kwargs=gnn_kwargs,\n    pre_nn_kwargs=pre_nn_kwargs, \n    task_heads_kwargs=task_heads_kwargs,\n    graph_output_nn_kwargs = graph_output_nn_kwargs\n)\n</pre> temp_dim_1 = 23 temp_dim_2 = 17  pre_nn_kwargs = {     \"in_dim\": in_dim,     \"out_dim\": temp_dim_1,     \"hidden_dims\": 4,     \"depth\": 2,     \"activation\": 'relu',     \"last_activation\": \"none\",     \"dropout\": 0.2 }  gnn_kwargs = {     \"in_dim\": temp_dim_1,     \"out_dim\": temp_dim_2,     \"hidden_dims\": 5,     \"depth\": 1,     \"activation\": 'gelu',     \"last_activation\": None,     \"dropout\": 0.1,     \"normalization\": 'layer_norm',     \"last_normalization\": 'layer_norm',     \"residual_type\": 'simple',     \"virtual_node\": None,     \"layer_type\": 'pyg:gcn',     \"layer_kwargs\": None }  task_heads_kwargs = {     \"graph-task-1\": {         \"task_level\": 'graph',         \"out_dim\": 3,         \"hidden_dims\": 32,         \"depth\": 2,         \"activation\": 'relu',         \"last_activation\": None,         \"dropout\": 0.1,         \"normalization\": None,         \"last_normalization\": None,         \"residual_type\": \"none\"     },     \"graph-task-2\": {         \"task_level\": 'graph',         \"out_dim\": 4,         \"hidden_dims\": 32,         \"depth\": 2,         \"activation\": 'relu',         \"last_activation\": None,         \"dropout\": 0.1,         \"normalization\": None,         \"last_normalization\": None,         \"residual_type\": \"none\"     },     \"node-task-1\": {         \"task_level\": 'node',         \"out_dim\": 2,         \"hidden_dims\": 32,         \"depth\": 2,         \"activation\": 'relu',         \"last_activation\": None,         \"dropout\": 0.1,         \"normalization\": None,         \"last_normalization\": None,         \"residual_type\": \"none\"     } }  graph_output_nn_kwargs = {     \"graph\": {         \"pooling\": ['sum'],         \"out_dim\": temp_dim_2,         \"hidden_dims\": temp_dim_2,         \"depth\": 1,         \"activation\": 'relu',         \"last_activation\": None,         \"dropout\": 0.1,         \"normalization\": None,         \"last_normalization\": None,         \"residual_type\": \"none\"     },     \"node\": {         \"pooling\": None,         \"out_dim\": temp_dim_2,         \"hidden_dims\": temp_dim_2,         \"depth\": 1,         \"activation\": 'relu',         \"last_activation\": None,         \"dropout\": 0.1,         \"normalization\": None,         \"last_normalization\": None,         \"residual_type\": \"none\"     } }       gnn_net = FullGraphMultiTaskNetwork(     gnn_kwargs=gnn_kwargs,     pre_nn_kwargs=pre_nn_kwargs,      task_heads_kwargs=task_heads_kwargs,     graph_output_nn_kwargs = graph_output_nn_kwargs ) In\u00a0[117]: Copied! <pre>graph = deepcopy(bg)\nprint(graph.feat.shape)\nprint(\"\\n\")\n\nprint(gnn_net)\nprint(\"\\n\")\n\nout = gnn_net(graph)\nfor task in out.keys():\n    print(task, out[task].shape)\n</pre> graph = deepcopy(bg) print(graph.feat.shape) print(\"\\n\")  print(gnn_net) print(\"\\n\")  out = gnn_net(graph) for task in out.keys():     print(task, out[task].shape) <pre>torch.Size([7, 5])\n\n\nFullGNN\n---------\n    pre-NN(depth=2, ResidualConnectionNone)\n        [FCLayer[5 -&gt; 4 -&gt; 23]\n    \n    GNN(depth=1, ResidualConnectionSimple(skip_steps=1))\n        GCNConvPyg[23 -&gt; 17]\n        \n    \n        Task heads:\n        graph-task-1: NN-graph-task-1(depth=2, ResidualConnectionNone)\n            [FCLayer[17 -&gt; 32 -&gt; 3]\n        graph-task-2: NN-graph-task-2(depth=2, ResidualConnectionNone)\n            [FCLayer[17 -&gt; 32 -&gt; 4]\n        node-task-1: NN-node-task-1(depth=2, ResidualConnectionNone)\n            [FCLayer[17 -&gt; 32 -&gt; 2]\n\n\ngraph-task-1 torch.Size([2, 3])\ngraph-task-2 torch.Size([2, 4])\nnode-task-1 torch.Size([7, 2])\n</pre> In\u00a0[118]: Copied! <pre>temp_dim_edges = 8\n\ngnn_edge_kwargs = {\n    \"in_dim\": temp_dim_1,\n    \"in_dim_edges\": temp_dim_edges,\n    \"out_dim\": temp_dim_2,\n    \"hidden_dims\": 5,\n    \"depth\": 1,\n    \"activation\": 'gelu',\n    \"last_activation\": None,\n    \"dropout\": 0.1,\n    \"normalization\": 'layer_norm',\n    \"last_normalization\": 'layer_norm',\n    \"residual_type\": 'simple',\n    \"virtual_node\": None,\n    \"layer_type\": 'pyg:gine',\n    \"layer_kwargs\": None\n}\n\npre_nn_edges_kwargs = {\n    \"in_dim\": in_dim_edges,\n    \"out_dim\": temp_dim_edges,\n    \"hidden_dims\": 4,\n    \"depth\": 2,\n    \"activation\": 'relu',\n    \"last_activation\": \"none\",\n    \"dropout\": 0.2\n}\n\n\ngnn_net_edges = FullGraphMultiTaskNetwork(\n    gnn_kwargs=gnn_edge_kwargs,\n    pre_nn_kwargs=pre_nn_kwargs,\n    pre_nn_edges_kwargs=pre_nn_edges_kwargs,\n    task_heads_kwargs=task_heads_kwargs,\n    graph_output_nn_kwargs = graph_output_nn_kwargs\n)\n</pre> temp_dim_edges = 8  gnn_edge_kwargs = {     \"in_dim\": temp_dim_1,     \"in_dim_edges\": temp_dim_edges,     \"out_dim\": temp_dim_2,     \"hidden_dims\": 5,     \"depth\": 1,     \"activation\": 'gelu',     \"last_activation\": None,     \"dropout\": 0.1,     \"normalization\": 'layer_norm',     \"last_normalization\": 'layer_norm',     \"residual_type\": 'simple',     \"virtual_node\": None,     \"layer_type\": 'pyg:gine',     \"layer_kwargs\": None }  pre_nn_edges_kwargs = {     \"in_dim\": in_dim_edges,     \"out_dim\": temp_dim_edges,     \"hidden_dims\": 4,     \"depth\": 2,     \"activation\": 'relu',     \"last_activation\": \"none\",     \"dropout\": 0.2 }   gnn_net_edges = FullGraphMultiTaskNetwork(     gnn_kwargs=gnn_edge_kwargs,     pre_nn_kwargs=pre_nn_kwargs,     pre_nn_edges_kwargs=pre_nn_edges_kwargs,     task_heads_kwargs=task_heads_kwargs,     graph_output_nn_kwargs = graph_output_nn_kwargs ) In\u00a0[119]: Copied! <pre>graph = deepcopy(bg)\nprint(graph.feat.shape)\nprint(graph.edge_feat.shape)\nprint(\"\\n\")\n\nprint(gnn_net)\nprint(\"\\n\")\n\nout = gnn_net_edges(graph)\nfor task in out.keys():\n    print(task, out[task].shape)\n</pre> graph = deepcopy(bg) print(graph.feat.shape) print(graph.edge_feat.shape) print(\"\\n\")  print(gnn_net) print(\"\\n\")  out = gnn_net_edges(graph) for task in out.keys():     print(task, out[task].shape) <pre>torch.Size([7, 5])\ntorch.Size([7, 13])\n\n\nFullGNN\n---------\n    pre-NN(depth=2, ResidualConnectionNone)\n        [FCLayer[5 -&gt; 4 -&gt; 23]\n    \n    GNN(depth=1, ResidualConnectionSimple(skip_steps=1))\n        GCNConvPyg[23 -&gt; 17]\n        \n    \n        Task heads:\n        graph-task-1: NN-graph-task-1(depth=2, ResidualConnectionNone)\n            [FCLayer[17 -&gt; 32 -&gt; 3]\n        graph-task-2: NN-graph-task-2(depth=2, ResidualConnectionNone)\n            [FCLayer[17 -&gt; 32 -&gt; 4]\n        node-task-1: NN-node-task-1(depth=2, ResidualConnectionNone)\n            [FCLayer[17 -&gt; 32 -&gt; 2]\n\n\ngraph-task-1 torch.Size([2, 3])\ngraph-task-2 torch.Size([2, 4])\nnode-task-1 torch.Size([7, 2])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/gnn/making_gnn_networks.html#making-gnn-networks","title":"Making GNN Networks\u00b6","text":"<p>In this example, you will learn how to easily build a full multi-task GNN using any kind of GNN layer. This tutorial uses the architecture defined by the class <code>FullGraphMultiTaskNetwork</code>.</p> <p><code>FullGraphMultiTaskNetwork</code> is an architecture that takes as input node features and (optionally) edge features. It applies a pre-MLP on both sets of features, then passes them into a main GNN network, and finally applies graph output NNs to produces the final outputs on possibly several task levels (graph, node, or edge-level).</p> <p>The network is very easy to built via a dictionnary of parameter that allow to customize each part of the network.</p>"},{"location":"tutorials/gnn/making_gnn_networks.html#building-a-network","title":"Building a network\u00b6","text":"<p>To build the network, we must define the arguments to pass at the different steps:</p> <ul> <li><p><code>pre_nn_kwargs</code>: The parameters used by a feed-forward neural network on the input node-features, before passing to the convolutional layers. See class <code>FeedForwardNN</code> for details on the required parameters. Will be ignored if set to <code>None</code>.</p> </li> <li><p><code>gnn_kwargs</code>: The parameters used by a feed-forward graph neural network on the features after it has passed through the pre-processing NN. See class <code>FeedForwardGraph</code> for details on the required parameters.</p> </li> <li><p><code>task_heads_kwargs</code>: The parameters used to specify the different task heads for possibly multiple tasks, each with a specified <code>task_level</code> (graph, node or edge).</p> </li> <li><p><code>graph_output_nn_kwargs</code>: The parameters used by the graph output NNs to process the features after the GNN layers. We need to to specify a NN for each <code>task_level</code> occuring in in the <code>task_heads_kwargs</code>.</p> </li> </ul>"},{"location":"tutorials/gnn/making_gnn_networks.html#applying-the-network","title":"Applying the network\u00b6","text":"<p>Once the network is defined, we only need to run the forward pass on the input graphs to get a prediction.</p> <p>The model outputs a dictionary of outputs, one for each task specified in <code>task_heads_kwargs</code>.</p>"},{"location":"tutorials/gnn/making_gnn_networks.html#building-a-network-using-additional-edge-features","title":"Building a network (using additional edge features)\u00b6","text":"<p>We can further use a GNN that uses additional edge features as inputs. For that, we only need to slightly modify the <code>gnn_kwargs</code> from above. Below, we define the new <code>gnn_edge_kwargs</code>, where we can set the <code>layer_type</code> to one of the GNN layers that support edge features (e.g., <code>pyg:gine</code> or <code>pyg:gps</code>) and add the additional parameter <code>in_dim_edges</code>. Optionally, we can configure a preprocessing NN for the the edge features via a dictionaly <code>pre_nn_edges_kwargs</code> analogous to <code>pre_nn_kwargs</code> above, or skip this step by setting it to <code>None</code>.</p>"},{"location":"tutorials/gnn/making_gnn_networks.html#applying-the-network-using-additional-edge-features","title":"Applying the network (using additional edge features)\u00b6","text":"<p>Again, we only need to run the forward pass on the input graphs to get a prediction.</p>"},{"location":"tutorials/gnn/using_gnn_layers.html","title":"Using GNN layers","text":"In\u00a0[53]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport torch\n\nimport torch_geometric as pyg\nfrom torch_geometric.data import Data, Batch\n\nfrom copy import deepcopy\n\nfrom graphium.nn.pyg_layers import (\n    GCNConvPyg,\n    GINConvPyg,\n    GatedGCNPyg,\n    GINEConvPyg,\n    GPSLayerPyg,\n    PNAMessagePassingPyg\n)\n\n_ = torch.manual_seed(42)\n</pre> %load_ext autoreload %autoreload 2  import torch  import torch_geometric as pyg from torch_geometric.data import Data, Batch  from copy import deepcopy  from graphium.nn.pyg_layers import (     GCNConvPyg,     GINConvPyg,     GatedGCNPyg,     GINEConvPyg,     GPSLayerPyg,     PNAMessagePassingPyg )  _ = torch.manual_seed(42) <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> <p>We will first create some simple batched graphs that will be used accross the examples. Here, <code>bg</code> is a batch containing 2 graphs with random node features.</p> In\u00a0[54]: Copied! <pre>in_dim = 5          # Input node-feature dimensions\nin_dim_edges = 13   # Input edge-feature dimensions\nout_dim = 11        # Desired output node-feature dimensions\nout_dim_edges = 15  # Desired output edge-feature dimensions\n\n\n# Let's create 2 simple pyg graphs. \n# start by specifying the edges with edge index\nedge_idx1 = torch.tensor([[0, 1, 2],\n                          [1, 2, 3]])\nedge_idx2 = torch.tensor([[2, 0, 0, 1],\n                          [0, 1, 2, 0]])\n\n# specify the node features, convention with variable x\nx1 = torch.randn(edge_idx1.max() + 1, in_dim, dtype=torch.float32)\nx2 = torch.randn(edge_idx2.max() + 1, in_dim, dtype=torch.float32)\n\n# specify the edge features in e\ne1 = torch.randn(edge_idx1.shape[-1], in_dim_edges, dtype=torch.float32)\ne2 = torch.randn(edge_idx2.shape[-1], in_dim_edges, dtype=torch.float32)\n\n# make the pyg graph objects with our constructed features\ng1 = Data(feat=x1, edge_index=edge_idx1, edge_feat=e1)\ng2 = Data(feat=x2, edge_index=edge_idx2, edge_feat=e2)\n\n# put the two graphs into a Batch graph\nbg = Batch.from_data_list([g1, g2])\n\n# The batched graph will show as a single graph with 7 nodes\nprint(bg)\n</pre> in_dim = 5          # Input node-feature dimensions in_dim_edges = 13   # Input edge-feature dimensions out_dim = 11        # Desired output node-feature dimensions out_dim_edges = 15  # Desired output edge-feature dimensions   # Let's create 2 simple pyg graphs.  # start by specifying the edges with edge index edge_idx1 = torch.tensor([[0, 1, 2],                           [1, 2, 3]]) edge_idx2 = torch.tensor([[2, 0, 0, 1],                           [0, 1, 2, 0]])  # specify the node features, convention with variable x x1 = torch.randn(edge_idx1.max() + 1, in_dim, dtype=torch.float32) x2 = torch.randn(edge_idx2.max() + 1, in_dim, dtype=torch.float32)  # specify the edge features in e e1 = torch.randn(edge_idx1.shape[-1], in_dim_edges, dtype=torch.float32) e2 = torch.randn(edge_idx2.shape[-1], in_dim_edges, dtype=torch.float32)  # make the pyg graph objects with our constructed features g1 = Data(feat=x1, edge_index=edge_idx1, edge_feat=e1) g2 = Data(feat=x2, edge_index=edge_idx2, edge_feat=e2)  # put the two graphs into a Batch graph bg = Batch.from_data_list([g1, g2])  # The batched graph will show as a single graph with 7 nodes print(bg) <pre>DataBatch(edge_index=[2, 7], feat=[7, 5], edge_feat=[7, 13], batch=[7], ptr=[3])\n</pre> In\u00a0[55]: Copied! <pre># The GCN method doesn't support edge features, so we ignore them\ngraph = deepcopy(bg)\nprint(graph.feat.shape)\n\n# We create the layer\nlayer = GCNConvPyg(\n            in_dim=in_dim, out_dim=out_dim, \n            activation=\"relu\", dropout=.3, normalization=\"batch_norm\")\n\n# We apply the forward loop on the node features\ngraph = layer(graph)\n\n# 7 is the number of nodes, 5 number of input features and 11 number of output features\nprint(layer)\nprint(graph.feat.shape)\n</pre> # The GCN method doesn't support edge features, so we ignore them graph = deepcopy(bg) print(graph.feat.shape)  # We create the layer layer = GCNConvPyg(             in_dim=in_dim, out_dim=out_dim,              activation=\"relu\", dropout=.3, normalization=\"batch_norm\")  # We apply the forward loop on the node features graph = layer(graph)  # 7 is the number of nodes, 5 number of input features and 11 number of output features print(layer) print(graph.feat.shape) <pre>torch.Size([7, 5])\nGCNConvPyg(5 -&gt; 11, activation=relu)\ntorch.Size([7, 11])\n</pre> In\u00a0[56]: Copied! <pre>graph = deepcopy(bg)\nprint(graph.feat.shape)\n\n# We create the layer\nlayer = GINConvPyg(\n            in_dim=in_dim, out_dim=out_dim, \n            activation=\"relu\", dropout=.3, normalization=\"batch_norm\")\n\n# We apply the forward loop on the node features\ngraph = layer(graph)\n\n# 7 is the number of nodes, 5 number of input features and 11 number of output features\nprint(layer)\nprint(graph.feat.shape)\n</pre> graph = deepcopy(bg) print(graph.feat.shape)  # We create the layer layer = GINConvPyg(             in_dim=in_dim, out_dim=out_dim,              activation=\"relu\", dropout=.3, normalization=\"batch_norm\")  # We apply the forward loop on the node features graph = layer(graph)  # 7 is the number of nodes, 5 number of input features and 11 number of output features print(layer) print(graph.feat.shape) <pre>torch.Size([7, 5])\nGINConvPyg(5 -&gt; 11, activation=relu)\ntorch.Size([7, 11])\n</pre> In\u00a0[57]: Copied! <pre># The GINE method uses edge features, so we have to pass the input dimension\ngraph = deepcopy(bg)\nprint(graph.feat.shape)\nprint(graph.edge_feat.shape)\n\n# We create the layer\nlayer = GINEConvPyg(\n            in_dim=in_dim, out_dim=out_dim,\n            in_dim_edges=in_dim_edges,\n            activation=\"relu\", dropout=.3, normalization=\"batch_norm\")\n\n# We apply the forward loop on the node features\ngraph = layer(graph)\n\n# 7 is the number of nodes, 5 number of input features and 11 number of output features\n# 7 is the number of edges, 13 number of input edge features\nprint(layer)\nprint(graph.feat.shape)\n</pre> # The GINE method uses edge features, so we have to pass the input dimension graph = deepcopy(bg) print(graph.feat.shape) print(graph.edge_feat.shape)  # We create the layer layer = GINEConvPyg(             in_dim=in_dim, out_dim=out_dim,             in_dim_edges=in_dim_edges,             activation=\"relu\", dropout=.3, normalization=\"batch_norm\")  # We apply the forward loop on the node features graph = layer(graph)  # 7 is the number of nodes, 5 number of input features and 11 number of output features # 7 is the number of edges, 13 number of input edge features print(layer) print(graph.feat.shape) <pre>torch.Size([7, 5])\ntorch.Size([7, 13])\nGINEConvPyg(5 -&gt; 11, activation=relu)\ntorch.Size([7, 11])\n</pre> In\u00a0[58]: Copied! <pre>graph = deepcopy(bg)\nprint(graph.feat.shape)\nprint(graph.edge_feat.shape)\n\n# We create the layer\nlayer = GPSLayerPyg(\n            in_dim=in_dim, out_dim=out_dim,\n            in_dim_edges=in_dim_edges,\n            mpnn_type = \"pyg:gine\", attn_type = \"full-attention\",\n            activation=\"relu\", dropout=.3, normalization=\"batch_norm\")\n\n# We apply the forward loop on the node features\ngraph = layer(graph)\n\n# 7 is the number of nodes, 5 number of input features and 11 number of output features\n# 7 is the number of edges, 13 number of input edge features\nprint(layer)\nprint(graph.feat.shape)\nprint(graph.edge_feat.shape)\n</pre> graph = deepcopy(bg) print(graph.feat.shape) print(graph.edge_feat.shape)  # We create the layer layer = GPSLayerPyg(             in_dim=in_dim, out_dim=out_dim,             in_dim_edges=in_dim_edges,             mpnn_type = \"pyg:gine\", attn_type = \"full-attention\",             activation=\"relu\", dropout=.3, normalization=\"batch_norm\")  # We apply the forward loop on the node features graph = layer(graph)  # 7 is the number of nodes, 5 number of input features and 11 number of output features # 7 is the number of edges, 13 number of input edge features print(layer) print(graph.feat.shape) print(graph.edge_feat.shape) <pre>torch.Size([7, 5])\ntorch.Size([7, 13])\nGPSLayerPyg(5 -&gt; 11, activation=relu)\ntorch.Size([7, 11])\ntorch.Size([7, 13])\n</pre> In\u00a0[59]: Copied! <pre>graph = deepcopy(bg)\nprint(graph.feat.shape)\nprint(graph.edge_feat.shape)\n\n# We create the layer\nlayer = GatedGCNPyg(\n            in_dim=in_dim, out_dim=out_dim,\n            in_dim_edges=in_dim_edges, out_dim_edges=out_dim_edges,\n            activation=\"relu\", dropout=.3, normalization=\"batch_norm\")\n\n# We apply the forward loop on the node features\ngraph = layer(graph)\n\n# 7 is the number of nodes, 5 number of input features and 11 number of output features\n# 7 is the number of edges, 13 number of input edge features and 15 the the number of output edge features\nprint(layer)\nprint(graph.feat.shape)\nprint(graph.edge_feat.shape)\n</pre> graph = deepcopy(bg) print(graph.feat.shape) print(graph.edge_feat.shape)  # We create the layer layer = GatedGCNPyg(             in_dim=in_dim, out_dim=out_dim,             in_dim_edges=in_dim_edges, out_dim_edges=out_dim_edges,             activation=\"relu\", dropout=.3, normalization=\"batch_norm\")  # We apply the forward loop on the node features graph = layer(graph)  # 7 is the number of nodes, 5 number of input features and 11 number of output features # 7 is the number of edges, 13 number of input edge features and 15 the the number of output edge features print(layer) print(graph.feat.shape) print(graph.edge_feat.shape) <pre>torch.Size([7, 5])\ntorch.Size([7, 13])\nGatedGCNPyg()\ntorch.Size([7, 11])\ntorch.Size([7, 15])\n</pre> In\u00a0[60]: Copied! <pre>graph = deepcopy(bg)\nprint(graph.feat.shape)\nprint(graph.edge_feat.shape)\n\n# We create the layer, and need to specify the aggregators and scalers\nlayer = PNAMessagePassingPyg(\n    in_dim=in_dim, out_dim=out_dim,\n    in_dim_edges=in_dim_edges,\n    aggregators=[\"mean\", \"max\", \"min\", \"std\"],\n    scalers=[\"identity\", \"amplification\", \"attenuation\"],\n    activation=\"relu\", dropout=.3, normalization=\"batch_norm\")\n\ngraph = layer(graph)\n\nprint(layer)\nprint(graph.feat.shape)\n</pre> graph = deepcopy(bg) print(graph.feat.shape) print(graph.edge_feat.shape)  # We create the layer, and need to specify the aggregators and scalers layer = PNAMessagePassingPyg(     in_dim=in_dim, out_dim=out_dim,     in_dim_edges=in_dim_edges,     aggregators=[\"mean\", \"max\", \"min\", \"std\"],     scalers=[\"identity\", \"amplification\", \"attenuation\"],     activation=\"relu\", dropout=.3, normalization=\"batch_norm\")  graph = layer(graph)  print(layer) print(graph.feat.shape) <pre>torch.Size([7, 5])\ntorch.Size([7, 13])\nPNAMessagePassingPyg()\ntorch.Size([7, 11])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/gnn/using_gnn_layers.html#using-gnn-layers","title":"Using GNN layers\u00b6","text":"<p>The current library implements multiple state-of-the-art graph neural networks. In this tutorial, you will learn how to use the GCN, GIN, GINE, GPS, Gated-GCN and PNA layers in a simple <code>forward</code> context.</p>"},{"location":"tutorials/gnn/using_gnn_layers.html#gcn-layer","title":"GCN Layer\u00b6","text":"<p>To use the GCN layer from the Kipf et al. paper, the steps are very simple. We create the layer with the desired attributes, and apply it to the graph.</p> <p>Kipf, Thomas N., and Max Welling. \"Semi-supervised classification with graph convolutional networks.\" arXiv preprint arXiv:1609.02907 (2016).</p>"},{"location":"tutorials/gnn/using_gnn_layers.html#gin-layer","title":"GIN Layer\u00b6","text":"<p>To use the GIN layer from the Xu et al. paper, the steps are identical to GCN.</p> <p>Xu, Keyulu, et al. \"How powerful are graph neural networks?.\" arXiv preprint arXiv:1810.00826 (2018).</p>"},{"location":"tutorials/gnn/using_gnn_layers.html#gine-layer","title":"GINE Layer\u00b6","text":"<p>To use the GINE layer from the Hu et al. paper, we also need to provide additional edge features as inputs.</p> <p>Hu, Weihua, et al. \"Strategies for Pre-training Graph Neural Networks.\" arXiv preprint arXiv:1905.12265 (2019).</p>"},{"location":"tutorials/gnn/using_gnn_layers.html#gps-layer","title":"GPS Layer\u00b6","text":"<p>To use the GPS layer from the Ramp\u00e1\u0161ek et al. paper, we also need to provide additional edge features as inputs. It is a hybrid approach using both a GNN and transformer in conjunction. Therefore, we further need to specify the GNN type and attention type used in the layer.</p> <p>Ramp\u00e1\u0161ek, Ladislav, et al. \"Recipe for a General, Powerful, Scalable Graph Transformer.\" arXiv preprint arXiv:2205.12454 (2022).</p>"},{"location":"tutorials/gnn/using_gnn_layers.html#gated-gcn-layer","title":"Gated-GCN Layer\u00b6","text":"<p>To use the Gated-GCN layer from the Bresson et al. paper, the steps are different since the layer not only requires edge features as inputs, but also outputs new edge features. Therefore, we have to further specify the number of output edge features</p> <p>Bresson, Xavier, and Thomas Laurent. \"Residual gated graph convnets.\" arXiv preprint arXiv:1711.07553 (2017).</p>"},{"location":"tutorials/gnn/using_gnn_layers.html#pna","title":"PNA\u00b6","text":"<p>PNA is a multi-aggregator method proposed by Corso et al.. It supports 2 types of aggregations, convolutional PNA-conv or message passing PNA-msgpass. Here, we provide the typically more powerful PNA-msgpass. It supports edges as inputs, but doesn't output edges. Here, we need to further specify the aggregators and scalers specific to this layer.</p> <p>Corso, Gabriele, et al. \"Principal Neighbourhood Aggregation for Graph Nets.\" arXiv preprint arXiv:2004.05718 (2020).</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html","title":"Multitask Molecular Modelling with Graphium on the IPU","text":"<p>Copyright (c) 2023 Graphcore Ltd. All rights reserved.</p> <p>Graphium is a high-performance deep learning library specializing in graph representation learning for diverse chemical tasks. Graphium integrates state-of-the-art Graph Neural Network (GNN) architectures and a user-friendly API, enabling the easy construction and training of custom GNN models.</p> <p>Graphium's distinctive feature is its robust featurization capabilities, enabling the extraction of comprehensive features from molecular structures for a range of applications, including property prediction, virtual screening, and drug discovery.</p> <p>Applicable to a wide array of chemical tasks like property prediction, toxicity evaluation, molecular generation, and graph regression, Graphium provides efficient tools and models, regardless of whether you're working with simple molecules or intricate chemical graphs.</p> <p>In this notebook, we'll illustrate Graphium's capabilities using the ToyMix dataset, an amalgamation of the QM9, Tox21, and ZINC12k datasets. We'll delve into the multitask scenario and display how Graphium trains models to concurrently predict multiple properties with chemical awareness.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install git+https://github.com/datamol-io/graphium.git\n!pip install -r requirements.txt\nfrom examples_utils import notebook_logging\n\n%load_ext gc_logger\n</pre> !pip install git+https://github.com/datamol-io/graphium.git !pip install -r requirements.txt from examples_utils import notebook_logging  %load_ext gc_logger In\u00a0[\u00a0]: Copied! <pre># General imports\nimport argparse\nimport os\nfrom os.path import dirname, abspath\nfrom loguru import logger\nfrom datetime import datetime\nfrom lightning.pytorch.utilities.model_summary import ModelSummary\n\n# Current project imports\nimport graphium\nfrom graphium.config._loader import (\n    load_datamodule,\n    load_metrics,\n    load_architecture,\n    load_predictor,\n    load_trainer,\n    load_accelerator,\n    load_yaml_config,\n)\nfrom graphium.utils.safe_run import SafeRun\nfrom graphium.utils.command_line_utils import update_config, get_anchors_and_aliases\n\nimport poptorch\nfrom tqdm.auto import tqdm\n\n# WandB\nimport wandb\n\n# Set up the working directory\nMAIN_DIR = dirname(dirname(abspath(graphium.__file__)))\n\npoptorch.setLogLevel(\"ERR\")\n</pre> # General imports import argparse import os from os.path import dirname, abspath from loguru import logger from datetime import datetime from lightning.pytorch.utilities.model_summary import ModelSummary  # Current project imports import graphium from graphium.config._loader import (     load_datamodule,     load_metrics,     load_architecture,     load_predictor,     load_trainer,     load_accelerator,     load_yaml_config, ) from graphium.utils.safe_run import SafeRun from graphium.utils.command_line_utils import update_config, get_anchors_and_aliases  import poptorch from tqdm.auto import tqdm  # WandB import wandb  # Set up the working directory MAIN_DIR = dirname(dirname(abspath(graphium.__file__)))  poptorch.setLogLevel(\"ERR\") In\u00a0[\u00a0]: Copied! <pre># And for compatibility with the Paperspace environment variables we will do the following:\ndataset_directory = os.getenv(\"DATASETS_DIR\")\n\nif not os.path.exists(dataset_directory + \"/data/neurips2023/small-dataset\"):\n    !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/qm9.csv.gz\n    !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/Tox21-7k-12-labels.csv.gz\n    !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/ZINC12k.csv.gz\n\n    !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/qm9_random_splits.pt\n    !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/Tox21_random_splits.pt\n    !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/ZINC12k_random_splits.pt\n    print(\"Datasets have been successfully downloaded.\")\nelse:\n    print(\"Datasets are already downloaded.\")\n</pre> # And for compatibility with the Paperspace environment variables we will do the following: dataset_directory = os.getenv(\"DATASETS_DIR\")  if not os.path.exists(dataset_directory + \"/data/neurips2023/small-dataset\"):     !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/qm9.csv.gz     !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/Tox21-7k-12-labels.csv.gz     !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/ZINC12k.csv.gz      !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/qm9_random_splits.pt     !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/Tox21_random_splits.pt     !wget -P $DATASETS_DIR/data/neurips2023/small-dataset/ https://storage.googleapis.com/graphium-public/datasets/neurips_2023/Small-dataset/ZINC12k_random_splits.pt     print(\"Datasets have been successfully downloaded.\") else:     print(\"Datasets are already downloaded.\") In\u00a0[\u00a0]: Copied! <pre>from IPython.core.display import Image, display\nfrom IPython.display import IFrame, display\n\ndisplay(Image(filename=\"ToyMix.png\", width=600))\n</pre> from IPython.core.display import Image, display from IPython.display import IFrame, display  display(Image(filename=\"ToyMix.png\", width=600)) <p>The representation of the datasets above shows that QM9 and ZINC are densely populated datasets with regression tasks and each label provided for each molecule. We also see that the Tox21 dataset is sparsely populated with binary labels. The relative sizes of these datasets is important. QM9 is approximately 10x larger than the other datasets, and so we would expect it to dominate the training.</p> <p>It is interesting to note the differences in the datasets, where the molecules are from fairly different domains in terms of their sizes and composition. Additionally, the quantum properties of molecules calculated using DFT is a very different type of property compared to results obtained from a wet lab, particularly in the content of complex systems.</p> <p>To get a sense of the dataset overlap, a Uniform Manifold Approximation and Projection (UMAP) is shown below. Here we can see the 2D projection of the molecule similarity, with the purple QM9 results being largely separate from the Tox21 and ZINC results.</p> <p>The question now is: how much can the large dataset of quantum properties be used to improve the prediction power of smaller datasets?</p> <p>This approach of using a larger dataset to improve predictions for smaller datasets has yielded impressive results in the world of large language models, but it is unknown whether the same can be done with molecules.</p> In\u00a0[\u00a0]: Copied! <pre>display(Image(filename=\"UMAP.png\", width=600))\n</pre> display(Image(filename=\"UMAP.png\", width=600)) In\u00a0[\u00a0]: Copied! <pre>display(Image(filename=\"flowchart.png\", width=900))\n</pre> display(Image(filename=\"flowchart.png\", width=900)) <p>The available configs are shown in the cell below, you can swap between them easily by uncommenting the desired line. The first set show the full multitask training on the ToyMix dataset for the three GNN architectures.</p> <p>The final two configs show the same GCN model trained on only the QM9 and Zinc datasets.</p> In\u00a0[\u00a0]: Copied! <pre>## Multi-Task Training - ToyMix Dataset\nCONFIG_FILE = {\n    \"small_gcn\": \"config_small_gcn.yaml\",\n    \"small_gcn_qm9\": \"config_small_gcn_qm9.yaml\",\n    \"small_gcn_zinc\": \"config_small_gcn_zinc.yaml\",\n    \"small_gin\": \"config_small_gin.yaml\",\n    \"small_gine\": \"config_small_gine.yaml\",\n}\n\ncfg = load_yaml_config(CONFIG_FILE[\"small_gcn\"], None)\n</pre> ## Multi-Task Training - ToyMix Dataset CONFIG_FILE = {     \"small_gcn\": \"config_small_gcn.yaml\",     \"small_gcn_qm9\": \"config_small_gcn_qm9.yaml\",     \"small_gcn_zinc\": \"config_small_gcn_zinc.yaml\",     \"small_gin\": \"config_small_gin.yaml\",     \"small_gine\": \"config_small_gine.yaml\", }  cfg = load_yaml_config(CONFIG_FILE[\"small_gcn\"], None) In\u00a0[\u00a0]: Copied! <pre>cfg[\"accelerator\"][\"ipu_config\"] = [\n    \"deviceIterations(5)\",\n    \"replicationFactor(4)\",  # &lt;------ 4x IPUs for a Pod-4, increase to 16 if using a Pod-16\n    \"TensorLocations.numIOTiles(128)\",\n    '_Popart.set(\"defaultBufferingDepth\", 128)',\n    \"Precision.enableStochasticRounding(True)\",\n]\n\n# For Paperspace we need to update the paths to the downloaded data / splits files\ncfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"qm9\"][\"df_path\"] = (\n    dataset_directory + \"/\"\n    + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"qm9\"][\"df_path\"]\n)\ncfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"qm9\"][\"splits_path\"] = (\n    dataset_directory + \"/\"\n    + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"qm9\"][\"splits_path\"]\n)\ncfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"tox21\"][\"df_path\"] = (\n    dataset_directory + \"/\"\n    + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"tox21\"][\"df_path\"]\n)\ncfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"tox21\"][\"df_path\"] = (\n    dataset_directory + \"/\"\n    + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"tox21\"][\"df_path\"]\n)\ncfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"zinc\"][\"splits_path\"] = (\n    dataset_directory + \"/\"\n    + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"zinc\"][\"splits_path\"]\n)\ncfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"zinc\"][\"splits_path\"] = (\n    dataset_directory + \"/\"\n    + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"zinc\"][\"splits_path\"]\n)\n</pre> cfg[\"accelerator\"][\"ipu_config\"] = [     \"deviceIterations(5)\",     \"replicationFactor(4)\",  # &lt;------ 4x IPUs for a Pod-4, increase to 16 if using a Pod-16     \"TensorLocations.numIOTiles(128)\",     '_Popart.set(\"defaultBufferingDepth\", 128)',     \"Precision.enableStochasticRounding(True)\", ]  # For Paperspace we need to update the paths to the downloaded data / splits files cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"qm9\"][\"df_path\"] = (     dataset_directory + \"/\"     + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"qm9\"][\"df_path\"] ) cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"qm9\"][\"splits_path\"] = (     dataset_directory + \"/\"     + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"qm9\"][\"splits_path\"] ) cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"tox21\"][\"df_path\"] = (     dataset_directory + \"/\"     + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"tox21\"][\"df_path\"] ) cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"tox21\"][\"df_path\"] = (     dataset_directory + \"/\"     + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"tox21\"][\"df_path\"] ) cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"zinc\"][\"splits_path\"] = (     dataset_directory + \"/\"     + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"zinc\"][\"splits_path\"] ) cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"zinc\"][\"splits_path\"] = (     dataset_directory + \"/\"     + cfg[\"datamodule\"][\"args\"][\"task_specific_args\"][\"zinc\"][\"splits_path\"] ) In\u00a0[\u00a0]: Copied! <pre>cfg[\"trainer\"][\"trainer\"][\"max_epochs\"] = 5\ncfg[\"predictor\"][\"torch_scheduler_kwargs\"][\"max_num_epochs\"] = cfg[\"trainer\"][\n    \"trainer\"\n][\"max_epochs\"]\ncfg[\"predictor\"][\"optim_kwargs\"][\"lr\"] = 4e-5\n</pre> cfg[\"trainer\"][\"trainer\"][\"max_epochs\"] = 5 cfg[\"predictor\"][\"torch_scheduler_kwargs\"][\"max_num_epochs\"] = cfg[\"trainer\"][     \"trainer\" ][\"max_epochs\"] cfg[\"predictor\"][\"optim_kwargs\"][\"lr\"] = 4e-5 In\u00a0[\u00a0]: Copied! <pre># Initialize the accelerator\ncfg, accelerator_type = load_accelerator(cfg)\nwandb.init(mode=\"disabled\")\n\n# Load and initialize the dataset\ndatamodule = load_datamodule(cfg, accelerator_type)\n\n# Initialize the network\nmodel_class, model_kwargs = load_architecture(\n    cfg,\n    in_dims=datamodule.in_dims,\n)\n</pre> # Initialize the accelerator cfg, accelerator_type = load_accelerator(cfg) wandb.init(mode=\"disabled\")  # Load and initialize the dataset datamodule = load_datamodule(cfg, accelerator_type)  # Initialize the network model_class, model_kwargs = load_architecture(     cfg,     in_dims=datamodule.in_dims, ) <p>First build the predictor - this is the model, look at the printed summary to see the layer types and number of parameters.</p> In\u00a0[\u00a0]: Copied! <pre>datamodule.prepare_data()\n\nmetrics = load_metrics(cfg)\nlogger.info(metrics)\n\npredictor = load_predictor(\n    cfg, model_class, model_kwargs, metrics, accelerator_type, datamodule.task_norms\n)\nlogger.info(predictor.model)\nlogger.info(ModelSummary(predictor, max_depth=4))\n</pre> datamodule.prepare_data()  metrics = load_metrics(cfg) logger.info(metrics)  predictor = load_predictor(     cfg, model_class, model_kwargs, metrics, accelerator_type, datamodule.task_norms ) logger.info(predictor.model) logger.info(ModelSummary(predictor, max_depth=4)) <p>Then build the trainer - which controls the actual training loop.</p> In\u00a0[\u00a0]: Copied! <pre>date_time_suffix = datetime.now().strftime(\"%d.%m.%Y_%H.%M.%S\")\nrun_name = \"main\"\n\ntrainer = load_trainer(cfg, run_name, accelerator_type, date_time_suffix)\n</pre> date_time_suffix = datetime.now().strftime(\"%d.%m.%Y_%H.%M.%S\") run_name = \"main\"  trainer = load_trainer(cfg, run_name, accelerator_type, date_time_suffix) In\u00a0[\u00a0]: Copied! <pre># Determine the max num nodes and edges in training and validation\nlogger.info(\"About to set the max nodes etc.\")\npredictor.set_max_nodes_edges_per_graph(datamodule, stages=[\"train\", \"val\"])\n\n# Run the model training\nwith SafeRun(\n    name=\"TRAINING\", raise_error=cfg[\"constants\"][\"raise_train_error\"], verbose=True\n):\n    trainer.fit(model=predictor, datamodule=datamodule)\n</pre> # Determine the max num nodes and edges in training and validation logger.info(\"About to set the max nodes etc.\") predictor.set_max_nodes_edges_per_graph(datamodule, stages=[\"train\", \"val\"])  # Run the model training with SafeRun(     name=\"TRAINING\", raise_error=cfg[\"constants\"][\"raise_train_error\"], verbose=True ):     trainer.fit(model=predictor, datamodule=datamodule) In\u00a0[\u00a0]: Copied! <pre># Determine the max num nodes and edges in training and validation\npredictor.set_max_nodes_edges_per_graph(datamodule, stages=[\"val\"])\n\n# Run the model validation\nwith SafeRun(\n    name=\"VALIDATING\", raise_error=cfg[\"constants\"][\"raise_train_error\"], verbose=True\n):\n    trainer.validate(model=predictor, datamodule=datamodule)\n</pre> # Determine the max num nodes and edges in training and validation predictor.set_max_nodes_edges_per_graph(datamodule, stages=[\"val\"])  # Run the model validation with SafeRun(     name=\"VALIDATING\", raise_error=cfg[\"constants\"][\"raise_train_error\"], verbose=True ):     trainer.validate(model=predictor, datamodule=datamodule) In\u00a0[\u00a0]: Copied! <pre># Run the model testing\nwith SafeRun(\n    name=\"TESTING\", raise_error=cfg[\"constants\"][\"raise_train_error\"], verbose=True\n):\n    trainer.test(\n        model=predictor,\n        datamodule=datamodule,\n    )\n</pre> # Run the model testing with SafeRun(     name=\"TESTING\", raise_error=cfg[\"constants\"][\"raise_train_error\"], verbose=True ):     trainer.test(         model=predictor,         datamodule=datamodule,     )"},{"location":"tutorials/model_training/running-multitask-ipu.html#multitask-molecular-modelling-with-graphium-on-the-ipu","title":"Multitask Molecular Modelling with Graphium on the IPU\u00b6","text":""},{"location":"tutorials/model_training/running-multitask-ipu.html#summary-table","title":"Summary table\u00b6","text":"Domain Tasks Model Datasets Workflow Number of IPUs Execution time Molecules Multitask GCN/GIN/GINE QM9, Zinc, Tox21 Training, Validation, Inference recommended: 4x (min: 1x, max: 16x) 20 mins"},{"location":"tutorials/model_training/running-multitask-ipu.html#learning-outcomes","title":"Learning outcomes\u00b6","text":"<p>In this notebook you will learn how to:</p> <ul> <li>Run multitask training for a variety of GNN models using the Graphium library on IPUs.</li> <li>Compare the results to the single task performance</li> <li>Learn how to modify the Graphium config files</li> </ul>"},{"location":"tutorials/model_training/running-multitask-ipu.html#links-to-other-resources","title":"Links to other resources\u00b6","text":"<p>For this notebook, it is best if you are already familiarity with GNNs and PyTorch Geometric. If you need a quick introduction, we suggest you refer to the Graphcore tutorials for using PyTorch Geometric on the IPU, which cover the basics of GNNs and PyTorch Geometric while introducing you to running models on the IPU.</p> <p></p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#dependencies","title":"Dependencies\u00b6","text":"<p>We install the following dependencies for this notebook:</p> <p>(This may take 5 - 10 minutes the first time.)</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#running-on-paperspace","title":"Running on Paperspace\u00b6","text":"<p>The Paperspace environment lets you run this notebook with no set up.  If a problem persists or you want to give us feedback on the content of this notebook, please reach out through our community of developers using Slack or raise a GitHub issue.</p> <p>Requirements:</p> <ul> <li>Python packages installed with <code>pip install -r ./requirements.txt</code></li> </ul>"},{"location":"tutorials/model_training/running-multitask-ipu.html#dataset-toymix-benchmark","title":"Dataset: ToyMix Benchmark\u00b6","text":"<p>The ToyMix dataset is an amalgamation of three widely recognized datasets in the field of machine learning for chemistry:</p> <ul> <li>QM9</li> <li>Tox21</li> <li>ZINC12K.</li> </ul> <p>Each of these datasets contributes different aspects, ensuring a comprehensive benchmarking tool.</p> <p>QM9, known in the realm of 3D GNNs, provides 19 graph-level quantum properties tied to the energy-minimized 3D conformation of molecules. Given that these molecules have at most nine heavy atoms, it's simpler and enables fast model iterations, much like the larger proposed quantum datasets, but on a smaller scale.</p> <p>The Tox21 dataset is notable among researchers in machine learning for drug discovery. It is a multi-label classification task with 12 labels, exhibiting a high degree of missing labels and imbalance towards the negative class. This mirrors the characteristics of larger datasets, specifically in terms of sparsity and imbalance, making it invaluable for learning to handle real-world, skewed datasets.</p> <p>Finally, the ZINC12K dataset is recognized in the study of GNN expressivity, an essential feature for large-scale data performance. It's included in ToyMix with the expectation that successful performance on this task will correlate well with success on larger datasets.</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#multitask-learning","title":"Multitask learning\u00b6","text":"<p>In multi-task learning, related tasks share information with each other during training, leading to a shared representation. This can often improve the model's performance on individual tasks, as information learned from one task can assist with others. Here the ToyMix dataset provides a sandbox to develop these models for molecular tasks.</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#download-the-dataset-and-splits","title":"Download the dataset and splits\u00b6","text":"<p>The ToyMix dataset requires downloading the three datasets. Here we will create the expected data directory and download both the raw dataset CSV files, and the split files to specify training, validation and test splits.</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#graphium-model-configuration","title":"Graphium model configuration\u00b6","text":"<p>The Graphium model is made up of the following components:</p> <ul> <li>Processing input features of the molecules</li> <li>A set of encoders used for positional and structural features</li> <li>Featurization to produce a PyG Graph dataset</li> <li>A stack of GNN layers - the backbone of the model - made up of MPNN and transformer layers</li> <li>The output node, graph and edge embeddings. These are processed with MLPs to yield multitask predictions.</li> </ul> <p>This is shown in the flowchart below.</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#updating-the-config","title":"Updating the config\u00b6","text":"<p>The Graphium library is designed to be used with a config file. This tutorial contains three examples (GCN, GIN, and GINE models) for training on the multitask ToyMix dataset.</p> <p>The configs are split into groups, with the key options in <code>accelerator</code>, <code>datamodule</code>, <code>archtecture</code>, <code>predictor</code>, <code>metrics</code> and <code>trainer</code>. It is recommended to have a look at the files to see how the model is structured.</p> <p>For some parameters, it makes more sense to set them in the notebook or via the command line arguments. You can also set these values by hand in the notebook, as shown below.</p> <p>(<code>--predictor.torch_scheduler_kwargs.max_num_epochs=200</code> will set the number of epochs).</p> <p>In the cell below, we can set the number of replicas to speed up training. The default value is 4, but if you are using a POD 16, we can increase the replica count for a significantly faster training.</p> <p>We can also set the number of epochs and learning rate. For the number of epochs, the <code>yaml</code> config file has been loaded into a <code>dict</code>, so we can't rely on the anchor and reference system in the YAML config file to keep parameters in sync, so this is done manually here.</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#key-hyper-parameters","title":"Key Hyper-Parameters:\u00b6","text":"<p>Shown here for simplicity - for testing you can change the values here / add other parameters When building a full training script it is recommended to use either the config file or the command line to set values.</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#train-the-model","title":"Train the model\u00b6","text":"<p>Now we can start training the model. The training is handled with PyTorch Lightning making it simple to set up the training loop.</p> <p>This will set up the training loop after calculating the maximum number of nodes and edges in the batch. We use fixed size batches on the IPU so we need to add some padding to account for variable molecule sizes before we compile the model.</p> <p>The training is displayed in a progress bar.</p> <p>Exercise for the reader: We currently only run training for 10 epochs to illustrate training the model and so you can see the validation and test output format. Try increasingthe number of epochs for better results. Using around 30 epochs should give results ~95% of the performance of the model trained for a full 300 epochs.</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#validation-and-test-dataset-splits","title":"Validation and test dataset splits\u00b6","text":"<p>Finally we can look at the validation and test dataset splits, where the trained model is loaded from the checkpoint and evaluated against the hold out sets.</p>"},{"location":"tutorials/model_training/running-multitask-ipu.html#conclusions-and-next-steps","title":"Conclusions and next steps\u00b6","text":"<p>We have demonstrated how you can use Graphium for multitask molecular modelling on the IPU. We used the ToyMix dataset, which combines the QM9, Tox21, and ZINC12k datasets to show how Graphium trains models to concurrently predict multiple properties with chemical awareness.</p> <p>Looking at the validation and test results we can see the impact of the larger QM9 dataset in improving the single task performance of the smaller datasets. The accuracy and MAE scores are much better than we would expect just from training on the small datasets, confirming the training benefit from a multitask setting even on a very short training run. To further verify this we can look at the same model trained for the full number of epochs on the multitask dataset and the individual datasets and compare the final validation results.</p> <p>You can try out the following:</p> <ul> <li>Increase the number of epochs for training to around 30. This should yield reasonably good results. If you train for 300 epochs, your results should match the benchmarking results.</li> <li>Change the config file to use a different model. GIN will achieve higher accuracy than GCN, and GINE will have a higher accuracy than GIN.</li> <li>Adapt the multitask config to only use a single dataset. You can use this to compare the results of the multitask performance with a single task performance. Rremember to remove the relevant sections from the dataset, the losses and the metrics. Example YAML config files showing how to do this for GCN with the QM9 (<code>config_small_gcn_qm9.yaml</code>) and ZINC12K (<code>config_small_gcn_zinc.yaml</code>) datasets are given.</li> <li>Explore more molecular featurisers in the <code>molfeat</code> notebook <code>pytorch_geometric_molfeat.ipynb</code></li> </ul>"},{"location":"tutorials/model_training/simple-molecular-model.html","title":"Building and training a simple model from configurations","text":"In\u00a0[1]: Copied! <pre>import yaml\nimport omegaconf\n</pre> import yaml import omegaconf In\u00a0[2]: Copied! <pre>def print_config_with_key(config, key):\n    new_config = {key: config[key]}\n    print(omegaconf.OmegaConf.to_yaml(new_config))\n</pre> def print_config_with_key(config, key):     new_config = {key: config[key]}     print(omegaconf.OmegaConf.to_yaml(new_config)) In\u00a0[3]: Copied! <pre># First, let's read the yaml configuration file\nwith open(\"../../../expts/configs/config_gps_10M_pcqm4m_mod.yaml\", \"r\") as file:\n    yaml_config = yaml.load(file, Loader=yaml.FullLoader)\n\nprint(\"Yaml file loaded\")\n</pre> # First, let's read the yaml configuration file with open(\"../../../expts/configs/config_gps_10M_pcqm4m_mod.yaml\", \"r\") as file:     yaml_config = yaml.load(file, Loader=yaml.FullLoader)  print(\"Yaml file loaded\") <pre>Yaml file loaded\n</pre> In\u00a0[4]: Copied! <pre>print_config_with_key(yaml_config, \"constants\")\n</pre> print_config_with_key(yaml_config, \"constants\") <pre>constants:\n  name: pcqm4mv2_mpnn_4layer\n  seed: 42\n  raise_train_error: true\n  accelerator:\n    type: gpu\n\n</pre> In\u00a0[5]: Copied! <pre>print_config_with_key(yaml_config, \"datamodule\")\n</pre> print_config_with_key(yaml_config, \"datamodule\") <pre>datamodule:\n  module_type: MultitaskFromSmilesDataModule\n  args:\n    task_specific_args:\n      homolumo:\n        df: null\n        task_level: graph\n        df_path: ~/scratch/data/graphium/data/PCQM4M/pcqm4mv2-20k.csv\n        smiles_col: cxsmiles\n        label_cols:\n        - homo_lumo_gap\n        split_val: 0.1\n        split_test: 0.1\n    prepare_dict_or_graph: pyg:graph\n    featurization_n_jobs: 30\n    featurization_progress: true\n    featurization_backend: loky\n    featurization:\n      mask_nan: 0\n      atom_property_list_onehot:\n      - atomic-number\n      - group\n      - period\n      - total-valence\n      atom_property_list_float:\n      - degree\n      - formal-charge\n      - radical-electron\n      - aromatic\n      - in-ring\n      edge_property_list:\n      - bond-type-onehot\n      - stereo\n      - in-ring\n      conformer_property_list:\n      - positions_3d\n      add_self_loop: false\n      explicit_H: false\n      use_bonds_weights: false\n      pos_encoding_as_features:\n        pos_types:\n          node_laplacian_eigvec:\n            pos_type: laplacian_eigvec\n            pos_level: node\n            num_pos: 8\n            normalization: none\n            disconnected_comp: true\n          node_laplacian_eigval:\n            pos_type: laplacian_eigval\n            pos_level: node\n            num_pos: 8\n            normalization: none\n            disconnected_comp: true\n          rw_return_probs:\n            pos_type: rw_return_probs\n            pos_level: node\n            ksteps:\n            - 4\n            - 8\n          nodepair_rw_transition_probs:\n            pos_type: rw_transition_probs\n            pos_level: edge\n            ksteps:\n            - 2\n            - 4\n          nodepair_rw_return_probs:\n            pos_type: rw_return_probs\n            pos_level: nodepair\n            ksteps:\n            - 4\n          electrostatic:\n            pos_type: electrostatic\n            pos_level: node\n          edge_commute:\n            pos_type: commute\n            pos_level: edge\n          nodepair_graphormer:\n            pos_type: graphormer\n            pos_level: nodepair\n    batch_size_training: 64\n    batch_size_inference: 16\n    num_workers: 0\n    persistent_workers: false\n\n</pre> In\u00a0[6]: Copied! <pre>print_config_with_key(yaml_config, \"architecture\")\n</pre> print_config_with_key(yaml_config, \"architecture\") <pre>architecture:\n  model_type: FullGraphMultiTaskNetwork\n  mup_base_path: null\n  pre_nn:\n    out_dim: 32\n    hidden_dims: 64\n    depth: 2\n    activation: relu\n    last_activation: none\n    dropout: 0.1\n    normalization: layer_norm\n    last_normalization: layer_norm\n    residual_type: none\n  pre_nn_edges:\n    out_dim: 16\n    hidden_dims: 32\n    depth: 2\n    activation: relu\n    last_activation: none\n    dropout: 0.1\n    normalization: layer_norm\n    last_normalization: layer_norm\n    residual_type: none\n  pe_encoders:\n    out_dim: 32\n    edge_out_dim: 16\n    pool: sum\n    last_norm: None\n    encoders:\n      emb_la_pos:\n        encoder_type: laplacian_pe\n        input_keys:\n        - laplacian_eigvec\n        - laplacian_eigval\n        output_keys:\n        - feat\n        hidden_dim: 32\n        model_type: DeepSet\n        num_layers: 2\n        num_layers_post: 1\n        dropout: 0.1\n        first_normalization: none\n      emb_rwse:\n        encoder_type: mlp\n        input_keys:\n        - rw_return_probs\n        output_keys:\n        - feat\n        hidden_dim: 32\n        num_layers: 2\n        dropout: 0.1\n        normalization: layer_norm\n        first_normalization: layer_norm\n      emb_electrostatic:\n        encoder_type: mlp\n        input_keys:\n        - electrostatic\n        output_keys:\n        - feat\n        hidden_dim: 32\n        num_layers: 1\n        dropout: 0.1\n        normalization: layer_norm\n        first_normalization: layer_norm\n      emb_edge_rwse:\n        encoder_type: mlp\n        input_keys:\n        - edge_rw_transition_probs\n        output_keys:\n        - edge_feat\n        hidden_dim: 32\n        num_layers: 1\n        dropout: 0.1\n        normalization: layer_norm\n      emb_edge_pes:\n        encoder_type: cat_mlp\n        input_keys:\n        - edge_rw_transition_probs\n        - edge_commute\n        output_keys:\n        - edge_feat\n        hidden_dim: 32\n        num_layers: 1\n        dropout: 0.1\n        normalization: layer_norm\n      gaussian_pos:\n        encoder_type: gaussian_kernel\n        input_keys:\n        - positions_3d\n        output_keys:\n        - feat\n        - nodepair_gaussian_bias_3d\n        num_heads: 2\n        num_layers: 2\n        embed_dim: 32\n        use_input_keys_prefix: false\n  gnn:\n    out_dim: 32\n    hidden_dims: 32\n    depth: 4\n    activation: gelu\n    last_activation: none\n    dropout: 0.0\n    normalization: layer_norm\n    last_normalization: layer_norm\n    residual_type: simple\n    pooling:\n    - sum\n    virtual_node: none\n    layer_type: pyg:gps\n    layer_kwargs:\n      node_residual: false\n      mpnn_type: pyg:mpnnplus\n      mpnn_kwargs:\n        in_dim: 32\n        out_dim: 32\n        in_dim_edges: 16\n        out_dim_edges: 16\n      attn_type: full-attention\n      attn_kwargs:\n        num_heads: 2\n      biased_attention_key: nodepair_gaussian_bias_3d\n  post_nn: null\n  task_heads:\n    homolumo:\n      out_dim: 1\n      hidden_dims: 256\n      depth: 2\n      activation: relu\n      last_activation: none\n      dropout: 0.1\n      normalization: layer_norm\n      last_normalization: none\n      residual_type: none\n\n</pre> In\u00a0[7]: Copied! <pre>print_config_with_key(yaml_config, \"predictor\")\n</pre> print_config_with_key(yaml_config, \"predictor\") <pre>predictor:\n  metrics_on_progress_bar:\n    homolumo:\n    - mae\n    - pearsonr\n  loss_fun:\n    homolumo: mse_ipu\n  random_seed: 42\n  optim_kwargs:\n    lr: 0.0004\n  torch_scheduler_kwargs:\n    module_type: WarmUpLinearLR\n    max_num_epochs: 5\n    warmup_epochs: 10\n    verbose: false\n  scheduler_kwargs: null\n  target_nan_mask: null\n  flag_kwargs:\n    n_steps: 0\n    alpha: 0.0\n\n</pre> In\u00a0[8]: Copied! <pre>print_config_with_key(yaml_config, \"metrics\")\n</pre> print_config_with_key(yaml_config, \"metrics\") <pre>metrics:\n  homolumo:\n  - name: mae\n    metric: mae_ipu\n    target_nan_mask: null\n    multitask_handling: flatten\n    threshold_kwargs: null\n  - name: pearsonr\n    metric: pearsonr_ipu\n    threshold_kwargs: null\n    target_nan_mask: null\n    multitask_handling: mean-per-label\n\n</pre> In\u00a0[9]: Copied! <pre>print_config_with_key(yaml_config, \"trainer\")\n</pre> print_config_with_key(yaml_config, \"trainer\") <pre>trainer:\n  logger:\n    save_dir: logs/PCQMv2\n    name: pcqm4mv2_mpnn_4layer\n    project: PCQMv2_mpnn\n  model_checkpoint:\n    dirpath: models_checkpoints/PCMQv2/\n    filename: pcqm4mv2_mpnn_4layer\n    save_top_k: 1\n    every_n_epochs: 100\n  trainer:\n    precision: 32\n    max_epochs: 5\n    min_epochs: 1\n    accumulate_grad_batches: 2\n    check_val_every_n_epoch: 20\n\n</pre> <p>$<code>python expts/main_run_multitask.py</code></p>"},{"location":"tutorials/model_training/simple-molecular-model.html#building-and-training-a-simple-model-from-configurations","title":"Building and training a simple model from configurations\u00b6","text":"<p>This tutorial will walk you through how to use a configuration file to define all the parameters of a model and of the trainer. This tutorial focuses on training from SMILES data in a CSV format.</p> <p>The work flow of testing your code on the entire pipeline is as follows:</p> <ol> <li>select a corresponding yaml file in the expts/main_run_multitask.py i.e. by <code>CONFIG_FILE = \"expts/configs/config_gps_10M_pcqm4m_mod.yaml\"</code></li> <li>modify the yaml config file</li> <li><code>python expts/main_run_multitask.py</code></li> </ol> <p>There are multiple examples of YAML files located in the folder <code>graphium/expts/configs</code> that one can refer to when training a new model. The file <code>config_gps_10M_pcqm4m_mod.yaml</code> shows an example of running the GPS model on the pcqm4m dataset.</p>"},{"location":"tutorials/model_training/simple-molecular-model.html#creating-the-yaml-file","title":"Creating the yaml file\u00b6","text":"<p>The first step is to create a YAML file containing all the required configurations, with an example given at <code>graphium/expts/config_gps_10M_pcqm4m_mod.yaml</code>. We will go through each part of the configurations.</p>"},{"location":"tutorials/model_training/simple-molecular-model.html#constants","title":"Constants\u00b6","text":"<p>First, we define the constants such as the random seed and whether the model should raise or ignore an error.</p>"},{"location":"tutorials/model_training/simple-molecular-model.html#datamodule","title":"Datamodule\u00b6","text":"<p>Here, we define all the parameters required by the datamodule to run correctly, such as the dataset path, whether to cache, the columns for the training, the molecular featurization to use, the train/val/test splits and the batch size.</p> <p>For more details, see class <code>MultitaskFromSmilesDataModule</code></p>"},{"location":"tutorials/model_training/simple-molecular-model.html#architecture","title":"Architecture\u00b6","text":"<p>The architecture is based on <code>FullGraphMultiTaskNetwork</code>. Here, we define all the layers for the model, including the layers for the pre-processing MLP (input layers <code>pre-nn</code> and <code>pre_nn_edges</code>), the positional encoder (<code>pe_encoders</code>), the post-processing MLP (output layers <code>post-nn</code>), and the main GNN (graph neural network <code>gnn</code>).</p> <p>You can find details in the following:</p> <ul> <li>info about the positional encoder in <code>graphium.nn.encoders</code></li> <li>info about the gnn layers in <code>graphium.nn.pyg_layers</code></li> <li>info about the architecture <code>FullGraphMultiTaskNetwork</code></li> <li>Main class for the GNN layers in <code>BaseGraphStructure</code></li> </ul> <p>The parameters allow to chose the feature size, the depth, the skip connections, the pooling and the virtual node. It also support different GNN layers such as <code>GatedGCNPyg</code>, <code>GINConvPyg</code>, <code>GINEConvPyg</code>, <code>GPSLayerPyg</code>, <code>MPNNPlusPyg</code>.</p>"},{"location":"tutorials/model_training/simple-molecular-model.html#predictor","title":"Predictor\u00b6","text":"<p>In the predictor, we define the loss functions, the metrics to track on the progress bar, and all the parameters necessary for the optimizer.</p>"},{"location":"tutorials/model_training/simple-molecular-model.html#metrics","title":"Metrics\u00b6","text":"<p>All the metrics can be defined there. If we want to use a classification metric, we can also define a threshold.</p> <p>See class <code>graphium.trainer.metrics.MetricWrapper</code> for more details.</p>"},{"location":"tutorials/model_training/simple-molecular-model.html#trainer","title":"Trainer\u00b6","text":"<p>Finally, the Trainer defines the parameters for the number of epochs to train, the checkpoints, and the patience.</p>"},{"location":"tutorials/model_training/simple-molecular-model.html#training-the-model","title":"Training the model\u00b6","text":"<p>Now that we defined all the configuration files, we want to train the model. The steps are fairly easy using the config loaders, and are given below.</p> <p>First make sure the dataset file is downloaded. Using <code>config_gps_10M_pcqm4m.yaml</code> as an example, if the file at <code>df_path</code> in the config is downloaded. In this case, we need to download <code>pcqm4mv2-20k.csv</code> into the specified directory <code>graphium/data/PCQM4M/pcqm4mv2-20k.csv</code></p>"}]}