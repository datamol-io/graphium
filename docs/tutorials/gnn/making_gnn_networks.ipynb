{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making GNN Networks\n",
    "\n",
    "In this example, you will learn how to easily build a full GNN network using any kind of GNN layer. This tutorial uses the architecture defined by the class `FullDGLNetwork`, which is compatible with any layer that inherits from `BaseDGLLayer`.\n",
    "\n",
    "`FullDGLNetwork` is an architecture that takes as input node features and (optionally) edge features. It applies a pre-MLP on both sets of features, then it passes it into a main GNN network, and finally applies a post-MLP to produce the final output (either node predictions or graph property predictions).\n",
    "\n",
    "The network is very easy to built via a dictionnary of parameter that allow to custom each part of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "from copy import deepcopy\n",
    "\n",
    "from graphium.nn.dgl_layers import PNAMessagePassingLayer\n",
    "from graphium.nn.architectures import FullDGLNetwork\n",
    "\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create some simple batched graphs that will be used accross the examples. Here, `bg` is a batch containing 2 graphs with random node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Graph(num_nodes=7, num_edges=14,\n      ndata_schemes={'feat': Scheme(shape=(5,), dtype=torch.float64)}\n      edata_schemes={'feat': Scheme(shape=(13,), dtype=torch.float64)})\n"
     ]
    }
   ],
   "source": [
    "in_dim = 5          # Input node-feature dimensions\n",
    "out_dim = 11        # Desired output node-feature dimensions\n",
    "in_dim_edges = 13   # Input edge-feature dimensions\n",
    "\n",
    "# Let's create 2 simple graphs. Here the tensors represent the connectivity between nodes\n",
    "g1 = dgl.graph((torch.tensor([0, 1, 2]), torch.tensor([1, 2, 3])))\n",
    "g2 = dgl.graph((torch.tensor([0, 0, 0, 1]), torch.tensor([0, 1, 2, 0])))\n",
    "\n",
    "# We add some node features to the graphs\n",
    "g1.ndata[\"feat\"] = torch.rand(g1.num_nodes(), in_dim, dtype=float)\n",
    "g2.ndata[\"feat\"] = torch.rand(g2.num_nodes(), in_dim, dtype=float)\n",
    "\n",
    "# We also add some edge features to the graphs\n",
    "g1.edata[\"feat\"] = torch.rand(g1.num_edges(), in_dim_edges, dtype=float)\n",
    "g2.edata[\"feat\"] = torch.rand(g2.num_edges(), in_dim_edges, dtype=float)\n",
    "\n",
    "# Finally we batch the graphs in a way compatible with the DGL library\n",
    "bg = dgl.batch([g1, g2])\n",
    "bg = dgl.add_self_loop(bg)\n",
    "\n",
    "# The batched graph will show as a single graph with 7 nodes\n",
    "print(bg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "To build the network, we must define the arguments to pass at the different steps:\n",
    "\n",
    "- `pre_nn_kwargs`: The parameters used by a feed-forward neural network on the input node-features, before passing to the convolutional layers. See class `FeedForwardNN` for details on the required parameters. Will be ignored if set to `None`.\n",
    "\n",
    "- `gnn_kwargs`: The parameters used by a feed-forward **graph** neural network on the features after it has passed through the pre-processing neural network. See class `FeedForwardDGL` for details on the required parameters.\n",
    "\n",
    "- `graph_output_nn_kwargs`: The parameters used by a feed-forward neural network on the features after the GNN layers. See class `FeedForwardNN` for details on the required parameters. Will be ignored if set to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dim_1 = 23\n",
    "temp_dim_2 = 17\n",
    "\n",
    "pre_nn_kwargs = {\n",
    "        \"in_dim\": in_dim,\n",
    "        \"out_dim\": temp_dim_1,\n",
    "        \"hidden_dims\": [4, 4, 4],\n",
    "        \"activation\": \"relu\",\n",
    "        \"last_activation\": \"none\",\n",
    "        \"batch_norm\": True,\n",
    "        \"dropout\": 0.2,    }\n",
    "\n",
    "graph_output_nn_kwargs = {\n",
    "        \"in_dim\": temp_dim_2,\n",
    "        \"out_dim\": out_dim,\n",
    "        \"hidden_dims\": [6, 6],\n",
    "        \"activation\": \"relu\",\n",
    "        \"last_activation\": \"sigmoid\",\n",
    "        \"batch_norm\": False,\n",
    "        \"dropout\": 0.,    }\n",
    "\n",
    "layer_kwargs = {\n",
    "    \"aggregators\": [\"mean\", \"max\", \"sum\"], \n",
    "    \"scalers\": [\"identity\", \"amplification\"],}\n",
    "\n",
    "gnn_kwargs = {\n",
    "    \"in_dim\": temp_dim_1,\n",
    "    \"out_dim\": temp_dim_2,\n",
    "    \"hidden_dims\": [5, 5, 5, 5, 5, 5],\n",
    "    \"residual_type\": \"densenet\",\n",
    "    \"residual_skip_steps\": 2,\n",
    "    \"layer_type\": PNAMessagePassingLayer,\n",
    "    \"pooling\": [\"sum\"],\n",
    "    \"activation\": \"relu\",\n",
    "    \"last_activation\": \"none\",\n",
    "    \"batch_norm\": False,\n",
    "    \"dropout\": 0.2,\n",
    "    \"in_dim_edges\": in_dim_edges,\n",
    "    \"layer_kwargs\": layer_kwargs,\n",
    "}\n",
    "\n",
    "gnn_net = FullDGLNetwork(\n",
    "    pre_nn_kwargs=pre_nn_kwargs, \n",
    "    gnn_kwargs=gnn_kwargs, \n",
    "    graph_output_nn_kwargs=graph_output_nn_kwargs).to(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the network\n",
    "\n",
    "Once the network is defined, we only need to run the forward pass on the input graphs to get a prediction.\n",
    "\n",
    "The network will handle the node and edge features depending on it's parameters and layer type.\n",
    "\n",
    "Chosing between graph property prediction and node property prediction depends on the parameter given by `gnn_kwargs[\"pooling\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([7, 5])\ntorch.Size([1, 11])\n\n\nDGL_GNN\n---------\n    pre-NN(depth=4, ResidualConnectionNone)\n        [FCLayer[5 -> 4 -> 4 -> 4 -> 23]\n    \n    GNN(depth=7, ResidualConnectionDenseNet(skip_steps=2))\n        PNAMessagePassingLayer[23 -> 5 -> 5 -> 5 -> 5 -> 5 -> 5 -> 17]\n        -> Pooling(['sum']) -> FCLayer(17 -> 17, activation=None)\n    \n    post-NN(depth=3, ResidualConnectionNone)\n        [FCLayer[17 -> 6 -> 6 -> 11]\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "h_in = graph.ndata[\"feat\"]\n",
    "\n",
    "h_out = gnn_net(graph)\n",
    "\n",
    "print(h_in.shape)\n",
    "print(h_out.shape)\n",
    "print(\"\\n\")\n",
    "print(gnn_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('graphium': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  },
  "interpreter": {
   "hash": "f4a99d018a205fcbcc0480c84566beaebcb91b08d0414b39a842df533e2a1d25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
