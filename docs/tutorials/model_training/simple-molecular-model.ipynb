{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training a simple model from configurations\n",
    "\n",
    "This tutorial will walk you through how to use a configuration file to define all the parameters of a model and of the trainer. This tutorial focuses on training from SMILES data in a CSV format.\n",
    "\n",
    "There are multiple examples of YAML files located in the folder `goli/expts` that one can refer to when training a new model. The file `config_ZINC_bench_gnn.yaml` shows an example of single task regression from a CSV file provided by goli. And the file `config_molpcba.yaml` shows an example of a multi-task classification on a dataset provided by OGB with some missing data.\n",
    "\n",
    "## Creating the yaml file\n",
    "\n",
    "The first step is to create a YAML file containing all the required configurations, with an example given at `goli/expts/config_micro_ZINC.yaml`. We will go through each part of the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_with_key(config, key):\n",
    "    new_config = {key: config[key]}\n",
    "    print(omegaconf.OmegaConf.to_yaml(new_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Yaml file loaded\n"
     ]
    }
   ],
   "source": [
    "# First, let's read the yaml configuration file\n",
    "with open(\"../../../expts/config_micro_ZINC.yaml\", \"r\") as file:\n",
    "    yaml_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "print(\"Yaml file loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "First, we define the constants such as the random seed and whether the model should raise or ignore an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "constants:\n  seed: 42\n  raise_train_error: true\n\n"
     ]
    }
   ],
   "source": [
    "print_config_with_key(yaml_config, \"constants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datamodule\n",
    "\n",
    "Here, we define all the parameters required by the datamodule to run correctly, such as the dataset path, whether to cache, the columns for the training, the molecular featurization to use, the train/val/test splits and the batch size.\n",
    "\n",
    "For more details, see class `goli.data.datamodule.DGLFromSmilesDataModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "datamodule:\n  module_type: DGLFromSmilesDataModule\n  args:\n    df_path: goli/data/micro_ZINC/micro_ZINC.csv\n    cache_data_path: goli/data/cache/micro_ZINC/full.cache\n    label_cols:\n    - score\n    smiles_col: SMILES\n    featurization_n_jobs: -1\n    featurization_progress: true\n    featurization:\n      atom_property_list_onehot:\n      - atomic-number\n      - valence\n      atom_property_list_float:\n      - mass\n      - electronegativity\n      - in-ring\n      edge_property_list:\n      - bond-type-onehot\n      - stereo\n      - in-ring\n      add_self_loop: false\n      explicit_H: false\n      use_bonds_weights: false\n      pos_encoding_as_features:\n        pos_type: laplacian_eigvec\n        num_pos: 3\n        normalization: none\n        disconnected_comp: true\n      pos_encoding_as_directions:\n        pos_type: laplacian_eigvec\n        num_pos: 3\n        normalization: none\n        disconnected_comp: true\n    split_val: 0.2\n    split_test: 0.2\n    split_seed: 42\n    splits_path: null\n    batch_size_train_val: 128\n    batch_size_test: 128\n    num_workers: 0\n    pin_memory: false\n    persistent_workers: false\n\n"
     ]
    }
   ],
   "source": [
    "print_config_with_key(yaml_config, \"datamodule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "In the architecture, we define all the layers for the model, including the layers for the pre-processing MLP (input layers `pre-nn`), the post-processing MLP (output layers `post-nn`), and the main GNN (graph neural network `gnn`).\n",
    "\n",
    "The parameters allow to chose the feature size, the depth, the skip connections, the pooling and the virtual node. It also support different GNN layers such as `gcn`, `gin`, `gat`, `gated-gcn`, `pna-conv` and `pna-msgpass`.\n",
    "\n",
    "For more details, see the following classes:\n",
    "\n",
    "-  `goli.nn.architecture.FullDGLNetwork`: Main class for the architecture\n",
    "-  `goli.nn.architecture.FeedForwardNN`: Main class for the inputs and outputs MLP\n",
    "-  `goli.nn.architecture.FeedForwardDGL`: Main class for the GNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "architecture:\n  model_type: fulldglnetwork\n  pre_nn:\n    out_dim: 32\n    hidden_dims: 32\n    depth: 1\n    activation: relu\n    last_activation: none\n    dropout: 0.1\n    normalization: batch_norm\n    last_normalization: batch_norm\n    residual_type: none\n  pre_nn_edges:\n    out_dim: 16\n    hidden_dims: 16\n    depth: 2\n    activation: relu\n    last_activation: none\n    dropout: 0.1\n    normalization: batch_norm\n    last_normalization: batch_norm\n    residual_type: none\n  gnn:\n    out_dim: 32\n    hidden_dims: 32\n    depth: 4\n    activation: relu\n    last_activation: none\n    dropout: 0.1\n    normalization: batch_norm\n    last_normalization: batch_norm\n    residual_type: simple\n    pooling:\n    - sum\n    - max\n    - dir1\n    virtual_node: sum\n    layer_type: dgn-msgpass\n    layer_kwargs:\n      aggregators:\n      - mean\n      - max\n      - dir1/dx_abs\n      - dir1/smooth\n      scalers:\n      - identity\n      - amplification\n      - attenuation\n  post_nn:\n    out_dim: 1\n    hidden_dims: 32\n    depth: 2\n    activation: relu\n    last_activation: none\n    dropout: 0.1\n    normalization: batch_norm\n    last_normalization: none\n    residual_type: none\n\n"
     ]
    }
   ],
   "source": [
    "print_config_with_key(yaml_config, \"architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor\n",
    "\n",
    "In the predictor, we define the loss functions, the metrics to track on the progress bar, and all the parameters necessary for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predictor:\n  metrics_on_progress_bar:\n  - mae\n  - pearsonr\n  - f1 > 3\n  - precision > 3\n  loss_fun: mse\n  random_seed: 42\n  optim_kwargs:\n    lr: 0.01\n    weight_decay: 1.0e-07\n  lr_reduce_on_plateau_kwargs:\n    factor: 0.5\n    patience: 7\n  scheduler_kwargs:\n    monitor: loss/val\n    frequency: 1\n  target_nan_mask: 0\n\n"
     ]
    }
   ],
   "source": [
    "print_config_with_key(yaml_config, \"predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "All the metrics can be defined there. If we want to use a classification metric, we can also define a threshold.\n",
    "\n",
    "See class `goli.trainer.metrics.MetricWrapper` for more details.\n",
    "\n",
    "See `goli.trainer.metrics.METRICS_CLASSIFICATION` and `goli.trainer.metrics.METRICS_REGRESSION` for a dictionnary of accepted metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "metrics:\n- name: mae\n  metric: mae\n  threshold_kwargs: null\n- name: pearsonr\n  metric: pearsonr\n  threshold_kwargs: null\n- name: f1 > 3\n  metric: f1\n  num_classes: 2\n  average: micro\n  threshold_kwargs:\n    operator: greater\n    threshold: 3\n    th_on_preds: true\n    th_on_target: true\n    target_to_int: true\n- name: f1 > 5\n  metric: f1\n  num_classes: 2\n  average: micro\n  threshold_kwargs:\n    operator: greater\n    threshold: 5\n    th_on_preds: true\n    th_on_target: true\n    target_to_int: true\n- name: precision > 3\n  metric: precision\n  class_reduction: micro\n  threshold_kwargs:\n    operator: greater\n    threshold: 3\n    th_on_preds: true\n    th_on_target: true\n    target_to_int: true\n\n"
     ]
    }
   ],
   "source": [
    "print_config_with_key(yaml_config, \"metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "Finally, the Trainer defines the parameters for the number of epochs to train, the checkpoints, and the patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainer:\n  logger:\n    save_dir: logs/micro_ZINC\n  early_stopping:\n    monitor: loss/val\n    min_delta: 0\n    patience: 10\n    mode: min\n  model_checkpoint:\n    dirpath: models_checkpoints/micro_ZINC/\n    filename: model\n    monitor: loss/val\n    mode: min\n    save_top_k: 1\n    every_n_val_epochs: 1\n  trainer:\n    max_epochs: 25\n    min_epochs: 5\n    gpus: 1\n\n"
     ]
    }
   ],
   "source": [
    "print_config_with_key(yaml_config, \"trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that we defined all the configuration files, we want to train the model. The steps are fairly easy using the config loaders, and are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n",
      "c:\\users\\domin\\documents\\gits\\goli_windows\\goli\\features\\spectral.py:43: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  eigvecs[comp, :] = this_eigvecs\n",
      "c:\\users\\domin\\documents\\gits\\goli_windows\\goli\\features\\spectral.py:44: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  eigvals_tile[comp, :] = this_eigvals\n",
      "2021-06-24 14:39:46.691 | WARNING  | goli.config._loader:load_trainer:126 - Number of GPUs selected is `1`, but will be ignored since no GPU are available on this device\n",
      "\n",
      "datamodule:\n",
      " name: DGLFromSmilesDataModule\n",
      "len: 1000\n",
      "train_size: null\n",
      "val_size: null\n",
      "test_size: null\n",
      "batch_size_train_val: 128\n",
      "batch_size_test: 128\n",
      "num_node_feats: 55\n",
      "num_node_feats_with_positional_encoding: 58\n",
      "num_edge_feats: 13\n",
      "num_labels: 1\n",
      "collate_fn: goli_collate_fn\n",
      "featurization:\n",
      "  atom_property_list_onehot:\n",
      "  - atomic-number\n",
      "  - valence\n",
      "  atom_property_list_float:\n",
      "  - mass\n",
      "  - electronegativity\n",
      "  - in-ring\n",
      "  edge_property_list:\n",
      "  - bond-type-onehot\n",
      "  - stereo\n",
      "  - in-ring\n",
      "  add_self_loop: false\n",
      "  explicit_H: false\n",
      "  use_bonds_weights: false\n",
      "  pos_encoding_as_features:\n",
      "    pos_type: laplacian_eigvec\n",
      "    num_pos: 3\n",
      "    normalization: none\n",
      "    disconnected_comp: true\n",
      "  pos_encoding_as_directions:\n",
      "    pos_type: laplacian_eigvec\n",
      "    num_pos: 3\n",
      "    normalization: none\n",
      "    disconnected_comp: true\n",
      " \n",
      "\n",
      "{'mae': mean_absolute_error, 'pearsonr': pearsonr, 'f1 > 3': f1(>3), 'f1 > 5': f1(>5), 'precision > 3': precision(>3)}\n",
      "DGL_GNN\n",
      "---------\n",
      "    pre-NN(depth=1, ResidualConnectionNone)\n",
      "        [FCLayer[58 -> 32]\n",
      "    \n",
      "    pre-NN-edges(depth=2, ResidualConnectionNone)\n",
      "        [FCLayer[13 -> 16 -> 16]\n",
      "    \n",
      "    GNN(depth=4, ResidualConnectionSimple(skip_steps=1))\n",
      "        DGNMessagePassingLayer[32 -> 32 -> 32 -> 32 -> 32]\n",
      "        -> Pooling(['sum', 'max', 'dir1']) -> FCLayer(96 -> 32, activation=None)\n",
      "    \n",
      "    post-NN(depth=2, ResidualConnectionNone)\n",
      "        [FCLayer[32 -> 32 -> 1]\n",
      "   | Name                              | Type                     | Params\n",
      "--------------------------------------------------------------------------------\n",
      "0  | model                             | FullDGLNetwork           | 74.4 K\n",
      "1  | model.pre_nn                      | FeedForwardNN            | 2.0 K \n",
      "2  | model.pre_nn.activation           | ReLU                     | 0     \n",
      "3  | model.pre_nn.residual_layer       | ResidualConnectionNone   | 0     \n",
      "4  | model.pre_nn.layers               | ModuleList               | 2.0 K \n",
      "5  | model.pre_nn.layers.0             | FCLayer                  | 2.0 K \n",
      "6  | model.pre_nn_edges                | FeedForwardNN            | 560   \n",
      "7  | model.pre_nn_edges.activation     | ReLU                     | 0     \n",
      "8  | model.pre_nn_edges.residual_layer | ResidualConnectionNone   | 0     \n",
      "9  | model.pre_nn_edges.layers         | ModuleList               | 560   \n",
      "10 | model.pre_nn_edges.layers.0       | FCLayer                  | 256   \n",
      "11 | model.pre_nn_edges.layers.1       | FCLayer                  | 304   \n",
      "12 | model.gnn                         | FeedForwardDGL           | 70.8 K\n",
      "13 | model.gnn.activation              | ReLU                     | 0     \n",
      "14 | model.gnn.layers                  | ModuleList               | 64.3 K\n",
      "15 | model.gnn.layers.0                | DGNMessagePassingLayer   | 16.1 K\n",
      "16 | model.gnn.layers.1                | DGNMessagePassingLayer   | 16.1 K\n",
      "17 | model.gnn.layers.2                | DGNMessagePassingLayer   | 16.1 K\n",
      "18 | model.gnn.layers.3                | DGNMessagePassingLayer   | 16.1 K\n",
      "19 | model.gnn.virtual_node_layers     | ModuleList               | 3.4 K \n",
      "20 | model.gnn.virtual_node_layers.0   | VirtualNode              | 1.1 K \n",
      "21 | model.gnn.virtual_node_layers.1   | VirtualNode              | 1.1 K \n",
      "22 | model.gnn.virtual_node_layers.2   | VirtualNode              | 1.1 K \n",
      "23 | model.gnn.residual_layer          | ResidualConnectionSimple | 0     \n",
      "24 | model.gnn.global_pool_layer       | ModuleListConcat         | 0     \n",
      "25 | model.gnn.global_pool_layer.0     | SumPooling               | 0     \n",
      "26 | model.gnn.global_pool_layer.1     | MaxPooling               | 0     \n",
      "27 | model.gnn.global_pool_layer.2     | DirPooling               | 0     \n",
      "28 | model.gnn.out_linear              | FCLayer                  | 3.2 K \n",
      "29 | model.gnn.out_linear.linear       | Linear                   | 3.1 K \n",
      "30 | model.gnn.out_linear.normalization         | BatchNorm1d              | 64    \n",
      "31 | model.gnn.out_linear.dropout      | Dropout                  | 0     \n",
      "32 | model.post_nn                     | FeedForwardNN            | 1.2 K \n",
      "33 | model.post_nn.activation          | ReLU                     | 0     \n",
      "34 | model.post_nn.residual_layer      | ResidualConnectionNone   | 0     \n",
      "35 | model.post_nn.layers              | ModuleList               | 1.2 K \n",
      "36 | model.post_nn.layers.0            | FCLayer                  | 1.1 K \n",
      "37 | model.post_nn.layers.1            | FCLayer                  | 33    \n",
      "38 | loss_fun                          | MSELoss                  | 0     \n",
      "--------------------------------------------------------------------------------\n",
      "74.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "74.4 K    Total params\n",
      "0.298     Total estimated model params size (MB)\n",
      "C:\\Users\\Domin\\miniconda3\\envs\\goli\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: Checkpoint directory models_checkpoints/micro_ZINC/ exists and is not empty.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2021-06-24 14:39:46.714 | INFO     | goli.data.datamodule:_load_from_cache:567 - Try reloading the data module from goli/data/cache/micro_ZINC/full.cache.\n",
      "2021-06-24 14:40:42.701 | INFO     | goli.data.datamodule:_load_from_cache:605 - Datamodule correctly reloaded from cache.\n",
      "C:\\Users\\Domin\\miniconda3\\envs\\goli\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: RuntimeWarning: Found unsupported keys in the lr scheduler dict: ['mode']\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name     | Type           | Params\n",
      "--------------------------------------------\n",
      "0 | model    | FullDGLNetwork | 74.4 K\n",
      "1 | loss_fun | MSELoss        | 0     \n",
      "--------------------------------------------\n",
      "74.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "74.4 K    Total params\n",
      "0.298     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\Domin\\miniconda3\\envs\\goli\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s] C:\\Users\\Domin\\miniconda3\\envs\\goli\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  71%|███████▏  | 5/7 [00:02<00:00,  2.17it/s, loss=3.97, v_num=18, loss/val=3.110, mae/val=1.390, pearsonr/val=-2.68e-6, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 7/7 [00:02<00:00,  2.67it/s, loss=3.97, v_num=18, loss/val=3.110, mae/val=1.390, pearsonr/val=-2.68e-6, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 0: 100%|██████████| 7/7 [00:02<00:00,  2.47it/s, loss=3.97, v_num=18, loss/val=5.480, mae/val=1.920, pearsonr/val=-.244, f1 > 3/val=0.000, precision > 3/val=nan.0]   \n",
      "Epoch 1:  86%|████████▌ | 6/7 [00:02<00:00,  2.63it/s, loss=3.62, v_num=18, loss/val=5.480, mae/val=1.920, pearsonr/val=-.244, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  4.17it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 7/7 [00:02<00:00,  2.53it/s, loss=3.62, v_num=18, loss/val=2.980, mae/val=1.370, pearsonr/val=0.137, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 2:  86%|████████▌ | 6/7 [00:02<00:00,  2.40it/s, loss=3.32, v_num=18, loss/val=2.980, mae/val=1.370, pearsonr/val=0.137, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  2.59it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 7/7 [00:03<00:00,  2.15it/s, loss=3.32, v_num=18, loss/val=3.850, mae/val=1.490, pearsonr/val=0.457, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 3:  86%|████████▌ | 6/7 [00:02<00:00,  2.52it/s, loss=3.09, v_num=18, loss/val=3.850, mae/val=1.490, pearsonr/val=0.457, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.81it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 7/7 [00:02<00:00,  2.46it/s, loss=3.09, v_num=18, loss/val=7.770, mae/val=2.380, pearsonr/val=0.548, f1 > 3/val=0.0476, precision > 3/val=nan.0]\n",
      "Epoch 4:  86%|████████▌ | 6/7 [00:02<00:00,  2.82it/s, loss=2.56, v_num=18, loss/val=7.770, mae/val=2.380, pearsonr/val=0.548, f1 > 3/val=0.0476, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 7/7 [00:02<00:00,  2.70it/s, loss=2.56, v_num=18, loss/val=2.720, mae/val=1.250, pearsonr/val=0.693, f1 > 3/val=0.167, precision > 3/val=nan.0] \n",
      "Epoch 5:  86%|████████▌ | 6/7 [00:02<00:00,  2.62it/s, loss=2.14, v_num=18, loss/val=2.720, mae/val=1.250, pearsonr/val=0.693, f1 > 3/val=0.167, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.45it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 7/7 [00:02<00:00,  2.49it/s, loss=2.14, v_num=18, loss/val=2.130, mae/val=1.140, pearsonr/val=0.753, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 6:  86%|████████▌ | 6/7 [00:02<00:00,  2.64it/s, loss=1.78, v_num=18, loss/val=2.130, mae/val=1.140, pearsonr/val=0.753, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  4.03it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 7/7 [00:02<00:00,  2.56it/s, loss=1.78, v_num=18, loss/val=1.780, mae/val=1.120, pearsonr/val=0.828, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 7:  86%|████████▌ | 6/7 [00:02<00:00,  2.37it/s, loss=1.47, v_num=18, loss/val=1.780, mae/val=1.120, pearsonr/val=0.828, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  4.04it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s, loss=1.47, v_num=18, loss/val=2.880, mae/val=1.510, pearsonr/val=0.861, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 8:  86%|████████▌ | 6/7 [00:02<00:00,  2.61it/s, loss=1.23, v_num=18, loss/val=2.880, mae/val=1.510, pearsonr/val=0.861, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.62it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 7/7 [00:02<00:00,  2.51it/s, loss=1.23, v_num=18, loss/val=6.910, mae/val=2.470, pearsonr/val=0.842, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 9:  86%|████████▌ | 6/7 [00:02<00:00,  2.34it/s, loss=1.05, v_num=18, loss/val=6.910, mae/val=2.470, pearsonr/val=0.842, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.22it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 7/7 [00:03<00:00,  2.24it/s, loss=1.05, v_num=18, loss/val=5.230, mae/val=2.150, pearsonr/val=0.890, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 10:  86%|████████▌ | 6/7 [00:02<00:00,  2.13it/s, loss=0.918, v_num=18, loss/val=5.230, mae/val=2.150, pearsonr/val=0.890, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.35it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 7/7 [00:03<00:00,  2.10it/s, loss=0.918, v_num=18, loss/val=9.860, mae/val=3.030, pearsonr/val=0.869, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 11:  86%|████████▌ | 6/7 [00:02<00:00,  2.45it/s, loss=0.805, v_num=18, loss/val=9.860, mae/val=3.030, pearsonr/val=0.869, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.83it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 7/7 [00:02<00:00,  2.39it/s, loss=0.805, v_num=18, loss/val=4.080, mae/val=1.900, pearsonr/val=0.899, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 12:  86%|████████▌ | 6/7 [00:02<00:00,  2.36it/s, loss=0.736, v_num=18, loss/val=4.080, mae/val=1.900, pearsonr/val=0.899, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.51it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s, loss=0.736, v_num=18, loss/val=1.540, mae/val=1.100, pearsonr/val=0.934, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 13:  86%|████████▌ | 6/7 [00:02<00:00,  2.37it/s, loss=0.694, v_num=18, loss/val=1.540, mae/val=1.100, pearsonr/val=0.934, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.66it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 7/7 [00:03<00:00,  2.30it/s, loss=0.694, v_num=18, loss/val=1.140, mae/val=0.928, pearsonr/val=0.927, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 14:  86%|████████▌ | 6/7 [00:02<00:00,  2.55it/s, loss=0.662, v_num=18, loss/val=1.140, mae/val=0.928, pearsonr/val=0.927, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.63it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 7/7 [00:02<00:00,  2.45it/s, loss=0.662, v_num=18, loss/val=0.830, mae/val=0.711, pearsonr/val=0.908, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 15:  86%|████████▌ | 6/7 [00:02<00:00,  2.61it/s, loss=0.658, v_num=18, loss/val=0.830, mae/val=0.711, pearsonr/val=0.908, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  4.09it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 7/7 [00:02<00:00,  2.54it/s, loss=0.658, v_num=18, loss/val=0.476, mae/val=0.527, pearsonr/val=0.930, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 16:  86%|████████▌ | 6/7 [00:02<00:00,  2.63it/s, loss=0.64, v_num=18, loss/val=0.476, mae/val=0.527, pearsonr/val=0.930, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.60it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 7/7 [00:02<00:00,  2.51it/s, loss=0.64, v_num=18, loss/val=0.780, mae/val=0.727, pearsonr/val=0.924, f1 > 3/val=0.400, precision > 3/val=nan.0]\n",
      "Epoch 17:  86%|████████▌ | 6/7 [00:02<00:00,  2.51it/s, loss=0.594, v_num=18, loss/val=0.780, mae/val=0.727, pearsonr/val=0.924, f1 > 3/val=0.400, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.75it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 7/7 [00:02<00:00,  2.43it/s, loss=0.594, v_num=18, loss/val=0.593, mae/val=0.624, pearsonr/val=0.934, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 18:  86%|████████▌ | 6/7 [00:02<00:00,  2.48it/s, loss=0.559, v_num=18, loss/val=0.593, mae/val=0.624, pearsonr/val=0.934, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.58it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 7/7 [00:02<00:00,  2.40it/s, loss=0.559, v_num=18, loss/val=0.846, mae/val=0.773, pearsonr/val=0.930, f1 > 3/val=0.333, precision > 3/val=nan.0]\n",
      "Epoch 19:  86%|████████▌ | 6/7 [00:02<00:00,  2.25it/s, loss=0.515, v_num=18, loss/val=0.846, mae/val=0.773, pearsonr/val=0.930, f1 > 3/val=0.333, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.82it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 7/7 [00:03<00:00,  2.24it/s, loss=0.515, v_num=18, loss/val=0.513, mae/val=0.572, pearsonr/val=0.928, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 20:  86%|████████▌ | 6/7 [00:02<00:00,  2.60it/s, loss=0.491, v_num=18, loss/val=0.513, mae/val=0.572, pearsonr/val=0.928, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.61it/s]\u001b[A\n",
      "Epoch 20: 100%|██████████| 7/7 [00:02<00:00,  2.49it/s, loss=0.491, v_num=18, loss/val=0.559, mae/val=0.580, pearsonr/val=0.931, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 21:  86%|████████▌ | 6/7 [00:02<00:00,  2.66it/s, loss=0.485, v_num=18, loss/val=0.559, mae/val=0.580, pearsonr/val=0.931, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.96it/s]\u001b[A\n",
      "Epoch 21: 100%|██████████| 7/7 [00:02<00:00,  2.57it/s, loss=0.485, v_num=18, loss/val=0.380, mae/val=0.473, pearsonr/val=0.939, f1 > 3/val=0.667, precision > 3/val=nan.0]\n",
      "Epoch 22:  86%|████████▌ | 6/7 [00:02<00:00,  2.65it/s, loss=0.471, v_num=18, loss/val=0.380, mae/val=0.473, pearsonr/val=0.939, f1 > 3/val=0.667, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.69it/s]\u001b[A\n",
      "Epoch 22: 100%|██████████| 7/7 [00:02<00:00,  2.55it/s, loss=0.471, v_num=18, loss/val=0.340, mae/val=0.433, pearsonr/val=0.942, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 23:  86%|████████▌ | 6/7 [00:02<00:00,  2.48it/s, loss=0.467, v_num=18, loss/val=0.340, mae/val=0.433, pearsonr/val=0.942, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.96it/s]\u001b[A\n",
      "Epoch 23: 100%|██████████| 7/7 [00:02<00:00,  2.43it/s, loss=0.467, v_num=18, loss/val=0.423, mae/val=0.501, pearsonr/val=0.938, f1 > 3/val=0.667, precision > 3/val=nan.0]\n",
      "Epoch 24:  86%|████████▌ | 6/7 [00:02<00:00,  2.67it/s, loss=0.473, v_num=18, loss/val=0.423, mae/val=0.501, pearsonr/val=0.938, f1 > 3/val=0.667, precision > 3/val=nan.0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:  50%|█████     | 1/2 [00:00<00:00,  3.97it/s]\u001b[A\n",
      "Epoch 24: 100%|██████████| 7/7 [00:02<00:00,  2.59it/s, loss=0.473, v_num=18, loss/val=0.326, mae/val=0.430, pearsonr/val=0.946, f1 > 3/val=0.000, precision > 3/val=nan.0]\n",
      "Epoch 24: 100%|██████████| 7/7 [00:02<00:00,  2.53it/s, loss=0.473, v_num=18, loss/val=0.326, mae/val=0.430, pearsonr/val=0.946, f1 > 3/val=0.000, precision > 3/val=nan.0]\n"
     ]
    }
   ],
   "source": [
    "from os.path import dirname, abspath\n",
    "from copy import deepcopy\n",
    "\n",
    "import goli\n",
    "from goli.config._loader import (load_datamodule, load_metrics, load_architecture, load_predictor, load_trainer)\n",
    "\n",
    "MAIN_DIR = dirname(dirname(abspath(goli.__file__)))\n",
    "os.chdir(MAIN_DIR)\n",
    "\n",
    "cfg = dict(deepcopy(yaml_config))\n",
    "\n",
    "# Load and initialize the dataset\n",
    "datamodule = load_datamodule(cfg)\n",
    "print(\"\\ndatamodule:\\n\", datamodule, \"\\n\")\n",
    "\n",
    "# Initialize the network\n",
    "model_class, model_kwargs = load_architecture(\n",
    "    cfg,\n",
    "    in_dim_nodes=datamodule.num_node_feats_with_positional_encoding,\n",
    "    in_dim_edges=datamodule.num_edge_feats,\n",
    ")\n",
    "\n",
    "# Load and print the metrics\n",
    "metrics = load_metrics(cfg)\n",
    "print(metrics)\n",
    "\n",
    "# Load the predictor, print the model, and print a summary of the number of parameters\n",
    "predictor = load_predictor(cfg, model_class, model_kwargs, metrics)\n",
    "print(predictor.model)\n",
    "print(predictor.summarize(mode=4, to_print=False))\n",
    "\n",
    "# Load the trainer, and start the training\n",
    "trainer = load_trainer(cfg)\n",
    "trainer.fit(model=predictor, datamodule=datamodule)"
   ]
  },
  {
   "source": [
    "## Testing the model\n",
    "Once the model is trained, we can use the same datamodule to get the results on the test set. Here, `ckpt_path` refers to the checkpoint path where the model at the best validation step was saved. Thus, the results on the test set represent the early stopping.\n",
    "\n",
    "All the metrics that were computed on the validation set are then computed on the test set, printed, and saved into the `metrics.yaml` file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 0it [00:00, ?it/s]C:\\Users\\Domin\\miniconda3\\envs\\goli\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Testing: 100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'MSELoss/test': 0.6393043398857117,\n",
      " 'f1 > 3/test': 0.0,\n",
      " 'f1 > 5/test': 0.0,\n",
      " 'loss/test': 0.6393043398857117,\n",
      " 'mae/test': 0.5762802362442017,\n",
      " 'mean_pred/test': -0.48094016313552856,\n",
      " 'mean_target/test': -0.6447566151618958,\n",
      " 'pearsonr/test': 0.9359009265899658,\n",
      " 'precision > 3/test': nan,\n",
      " 'std_pred/test': 1.7718788385391235,\n",
      " 'std_target/test': 2.1336562633514404}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'mean_pred/test': -0.48094016313552856,\n",
       "  'std_pred/test': 1.7718788385391235,\n",
       "  'mean_target/test': -0.6447566151618958,\n",
       "  'std_target/test': 2.1336562633514404,\n",
       "  'mae/test': 0.5762802362442017,\n",
       "  'pearsonr/test': 0.9359009265899658,\n",
       "  'f1 > 3/test': 0.0,\n",
       "  'f1 > 5/test': 0.0,\n",
       "  'precision > 3/test': nan,\n",
       "  'MSELoss/test': 0.6393043398857117,\n",
       "  'loss/test': 0.6393043398857117}]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ckpt_path = trainer.checkpoint_callbacks[0].best_model_path\n",
    "trainer.test(model=predictor, datamodule=datamodule, ckpt_path=ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('goli': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  },
  "interpreter": {
   "hash": "f4a99d018a205fcbcc0480c84566beaebcb91b08d0414b39a842df533e2a1d25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}