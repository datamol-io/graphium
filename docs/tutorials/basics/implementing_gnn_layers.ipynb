{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('goli': conda)"
  },
  "interpreter": {
   "hash": "f4a99d018a205fcbcc0480c84566beaebcb91b08d0414b39a842df533e2a1d25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Creating GNN layers\n",
    "\n",
    "One of the primary advantage of the current library is the fact that GNN layers are independant from model architecture, thus allowing more flexibility with the code by easily swapping different layer types as hyper-parameters. However, this requires that the layers be implemented using the DGL library, and must be inherited from the class `BaseDGLLayer`, which standardizes the inputs, outputs and properties of the layers. Thus, the architecture can be handled independantly using the class `FeedForwardDGL`, or any similar custom class.\n",
    "\n",
    "We will first start by a simple layer that does not use edges, to a more complex layer that uses edges.\n",
    "\n",
    "Since these examples are built on top of DGL, we recommend looking at their [library](https://docs.dgl.ai/en/0.5.x/index.html) for more info. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "from copy import deepcopy\n",
    "\n",
    "from goli.nn.dgl_layers import BaseDGLLayer\n",
    "from goli.nn.base_layers import FCLayer\n",
    "from goli.utils.decorators import classproperty\n",
    "\n",
    "\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "source": [
    "## Pre-defining test variables\n",
    "\n",
    "We define below a small batched graph on which we can test the created layers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Graph(num_nodes=7, num_edges=14,\n      ndata_schemes={'h': Scheme(shape=(5,), dtype=torch.float64)}\n      edata_schemes={'e': Scheme(shape=(13,), dtype=torch.float64)})\n"
     ]
    }
   ],
   "source": [
    "in_dim = 5          # Input node-feature dimensions\n",
    "out_dim = 11        # Desired output node-feature dimensions\n",
    "in_dim_edges = 13   # Input edge-feature dimensions\n",
    "\n",
    "# Let's create 2 simple graphs. Here the tensors represent the connectivity between nodes\n",
    "g1 = dgl.graph((torch.tensor([0, 1, 2]), torch.tensor([1, 2, 3])))\n",
    "g2 = dgl.graph((torch.tensor([0, 0, 0, 1]), torch.tensor([0, 1, 2, 0])))\n",
    "\n",
    "# We add some node features to the graphs\n",
    "g1.ndata[\"h\"] = torch.rand(g1.num_nodes(), in_dim, dtype=float)\n",
    "g2.ndata[\"h\"] = torch.rand(g2.num_nodes(), in_dim, dtype=float)\n",
    "\n",
    "# We also add some edge features to the graphs\n",
    "g1.edata[\"e\"] = torch.rand(g1.num_edges(), in_dim_edges, dtype=float)\n",
    "g2.edata[\"e\"] = torch.rand(g2.num_edges(), in_dim_edges, dtype=float)\n",
    "\n",
    "# Finally we batch the graphs in a way compatible with the DGL library\n",
    "bg = dgl.batch([g1, g2])\n",
    "bg = dgl.add_self_loop(bg)\n",
    "\n",
    "# The batched graph will show as a single graph with 7 nodes\n",
    "print(bg)"
   ]
  },
  {
   "source": [
    "## Creating a simple layer\n",
    "\n",
    "Here, we will show how to create a GNN layer that does a mean aggregation on the neighbouring features.\n",
    "\n",
    "First, for the layer to be fully compatible with the flexible architecture provided by `FeedForwardDGL`, it needs to inherit from the class `BaseDGLLayer`. This base-layer has multiple virtual methods that must be implemented in any class that inherits from it.\n",
    "\n",
    "The virtual methods are below\n",
    "\n",
    "- `layer_supports_edges`: We want to return `False` since our layer doesn't support edges\n",
    "- `layer_inputs_edges`: We want to return `False` since our layer doesn't input edges\n",
    "- `layer_outputs_edges`: We want to return `False` since our layer doesn't output edges\n",
    "- `layer_outdim_factor`: We want to return `1` since the output dimension does not depend on internal parameters.\n",
    "\n",
    "The example is given below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMeanLayer(BaseDGLLayer):\n",
    "    def __init__(self, in_dim, out_dim, activation, dropout, normalization):\n",
    "        # Initialize the parent class\n",
    "        super().__init__(   in_dim=in_dim, out_dim=out_dim, activation=activation,\n",
    "                            dropout=dropout, normalization=normalization)\n",
    "\n",
    "        # Create the layer with learned parameters\n",
    "        self.layer = FCLayer(in_dim=in_dim, out_dim=out_dim)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # We first apply the mean aggregation\n",
    "        g.ndata[\"h\"] = h\n",
    "        g.update_all(message_func=dgl.function.copy_u(\"h\", \"m\"), \n",
    "                    reduce_func=dgl.function.mean(\"m\", \"h\"))\n",
    "\n",
    "        # Then we apply the FCLayer, and the non-linearities\n",
    "        h = g.ndata[\"h\"]\n",
    "        h = self.layer(h)\n",
    "        h = self.apply_norm_activation_dropout(h)\n",
    "        return h\n",
    "\n",
    "    # Finally, we define all the virtual properties according to how\n",
    "    # the class works\n",
    "    @classproperty\n",
    "    def layer_supports_edges(cls):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def layer_inputs_edges(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def layer_outputs_edges(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def out_dim_factor(self):\n",
    "        return 1   "
   ]
  },
  {
   "source": [
    "Now, we are ready to test the `SimpleMeanLayer` on some DGL graphs. Note that in this example, we **ignore** the edge features since they are not supported."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([7, 5])\ntorch.Size([7, 11])\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "h_in = graph.ndata[\"h\"]\n",
    "layer = SimpleMeanLayer(\n",
    "            in_dim=in_dim, out_dim=out_dim, \n",
    "            activation=\"relu\", dropout=.3, normalization=\"batch_norm\").to(float)\n",
    "h_out = layer(graph, h_in)\n",
    "\n",
    "print(h_in.shape)\n",
    "print(h_out.shape)"
   ]
  },
  {
   "source": [
    "## Creating a complex layer with edges\n",
    "\n",
    "Here, we will show how to create a GNN layer that does a mean aggregation on the neighbouring features, concatenated to the edge features with their neighbours. In that case, only the node features will change, and the network will not update the edge features.\n",
    "\n",
    "The virtual methods will have different outputs\n",
    "\n",
    "- `layer_supports_edges`: We want to return `True` since our layer does support edges\n",
    "- `layer_inputs_edges`: We want to return `True` since our layer does input edges\n",
    "- `layer_outputs_edges`: We want to return `False` since our layer will not output new edges\n",
    "- `layer_outdim_factor`: We want to return `1` since the output dimension does not depend on internal parameters.\n",
    "\n",
    "The example is given below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexMeanLayer(BaseDGLLayer):\n",
    "    def __init__(self, in_dim, out_dim, in_dim_edges, activation, dropout, normalization):\n",
    "        # Initialize the parent class\n",
    "        super().__init__(   in_dim=in_dim, out_dim=out_dim, activation=activation,\n",
    "                            dropout=dropout, normalization=normalization)\n",
    "\n",
    "        # Create the layer with learned parameters. Note the addition\n",
    "        self.layer = FCLayer(in_dim=in_dim + in_dim_edges, out_dim=out_dim)\n",
    "\n",
    "    def cat_nodes_edges(self, edges):\n",
    "        # Create a message \"m\" by concatenating \"h\" and \"e\" for each pair of nodes\n",
    "        nodes_edges = torch.cat([edges.src[\"h\"], edges.data[\"e\"]], dim=-1)\n",
    "        return {\"m\": nodes_edges}\n",
    "\n",
    "    def get_edges_messages(self, edges): # Simply return the messages on the edges\n",
    "        return {\"m\": edges.data[\"m\"]}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "\n",
    "        # We first concatenate both the node and edge features on the edges\n",
    "        g.ndata[\"h\"] = h\n",
    "        g.edata[\"e\"] = e\n",
    "        g.apply_edges(self.cat_nodes_edges)\n",
    "\n",
    "        # Then we apply the mean aggregation to generate a message \"m\"\n",
    "        g.update_all(message_func=self.get_edges_messages, \n",
    "                    reduce_func=dgl.function.mean(\"m\", \"h\"))\n",
    "\n",
    "        # Finally we apply the FCLayer, and the non-linearities\n",
    "        h = g.ndata[\"h\"]\n",
    "        h = self.layer(h)\n",
    "        h = self.apply_norm_activation_dropout(h)\n",
    "        return h\n",
    "\n",
    "    # Finally, we define all the virtual properties according to how\n",
    "    # the class works\n",
    "    @classproperty\n",
    "    def layer_supports_edges(cls):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def layer_inputs_edges(self):\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def layer_outputs_edges(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def out_dim_factor(self):\n",
    "        return 1   "
   ]
  },
  {
   "source": [
    "Now, we are ready to test the `ComplexMeanLayer` on some DGL graphs. Note that in this example, we **use** the edge features since they are mandatory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([7, 5])\ntorch.Size([7, 11])\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "h_in = graph.ndata[\"h\"]\n",
    "e_in = graph.edata[\"e\"]\n",
    "layer = ComplexMeanLayer(\n",
    "            in_dim=in_dim, out_dim=out_dim, in_dim_edges=in_dim_edges,\n",
    "            activation=\"relu\", dropout=.3, normalization=\"batch_norm\").to(float)\n",
    "h_out = layer(graph, h_in, e_in)\n",
    "\n",
    "print(h_in.shape)\n",
    "print(h_out.shape)"
   ]
  }
 ]
}