# @package _global_

architecture:
  model_type: FullGraphMultiTaskNetwork
  mup_base_path: null
  pre_nn:   # Set as null to avoid a pre-nn network
    out_dim: 64
    hidden_dims: 256
    depth: 2
    activation: relu
    last_activation: none
    dropout: &dropout 0.1
    normalization: ${constants.norm}
    last_normalization: ${constants.norm}
    residual_type: none

  pre_nn_edges: null

  pe_encoders:
    out_dim: 32
    pool: "sum" #"mean" "max"
    last_norm: None #"batch_norm", "layer_norm"
    encoders: #la_pos |  rw_pos
      la_pos:  # Set as null to avoid a pre-nn network
        encoder_type: "laplacian_pe"
        input_keys: ["laplacian_eigvec", "laplacian_eigval"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 32
        model_type: 'DeepSet' #'Transformer' or 'DeepSet'
        num_layers: 2
        num_layers_post: 1 # Num. layers to apply after pooling
        dropout: 0.1
        first_normalization: "none" #"batch_norm" or "layer_norm"
      rw_pos:
        encoder_type: "mlp"
        input_keys: ["rw_return_probs"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 32
        num_layers: 2
        dropout: 0.1
        normalization: ${constants.norm} #"batch_norm" or "layer_norm"
        first_normalization: ${constants.norm} #"batch_norm" or "layer_norm"

  gnn:  # Set as null to avoid a post-nn network
    in_dim: 64 # or otherwise the correct value
    out_dim: &gnn_dim 768
    hidden_dims: *gnn_dim
    depth: 4
    activation: gelu
    last_activation: none
    dropout: 0.1
    normalization: ${constants.norm}
    last_normalization: ${constants.norm}
    residual_type: simple
    virtual_node: 'none'

  graph_output_nn:
    graph:
      pooling: [sum]
      out_dim: 1024
      hidden_dims: *gnn_dim
      depth: 2
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: ${constants.norm}
      last_normalization: "none"
      residual_type: none
    node:
      pooling: [sum]
      out_dim: 64
      hidden_dims: *gnn_dim
      depth: 2
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: ${constants.norm}
      last_normalization: "none"
      residual_type: none

datamodule:
  module_type: "MultitaskFromSmilesDataModule"
  args:
    prepare_dict_or_graph: pyg:graph
    featurization_n_jobs: 20
    featurization_progress: True
    featurization_backend: "loky"
    processed_graph_data_path: ${constants.datacache_path}
    dataloading_from: "disk"
    num_workers: 20 # -1 to use all
    persistent_workers: True
    featurization:
      atom_property_list_onehot: [atomic-number, group, period, total-valence]
      atom_property_list_float: [degree, formal-charge, radical-electron, aromatic, in-ring]
      edge_property_list: [bond-type-onehot, stereo, in-ring]
      add_self_loop: False
      explicit_H: False # if H is included
      use_bonds_weights: False
      pos_encoding_as_features: # encoder dropout 0.18
        pos_types:
          lap_eigvec:
            pos_level: node
            pos_type: laplacian_eigvec
            num_pos: 8
            normalization: "none" # nomrlization already applied on the eigen vectors
            disconnected_comp: True # if eigen values/vector for disconnected graph are included
          lap_eigval:
            pos_level: node
            pos_type: laplacian_eigval
            num_pos: 8
            normalization: "none" # nomrlization already applied on the eigen vectors
            disconnected_comp: True # if eigen values/vector for disconnected graph are included
          rw_pos: # use same name as pe_encoder
            pos_level: node
            pos_type: rw_return_probs
            ksteps: 16