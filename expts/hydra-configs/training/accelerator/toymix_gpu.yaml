# @package _global_

accelerator:
  float32_matmul_precision: medium

datamodule:
  args:
    batch_size_training: 2000
    batch_size_inference: 2000
    featurization_n_jobs: 6
    num_workers: 6

predictor:
  optim_kwargs: {}
  metrics_every_n_train_steps: 300
  torch_scheduler_kwargs:
    max_num_epochs: ${constants.max_epochs}

trainer:
  trainer:
    # accumulate_grad_batches: 1
    max_epochs: ${constants.max_epochs}