# @package _global_

predictor:
  random_seed: ${constants.seed}
  optim_kwargs:
    lr: 4.e-5 # warmup can be scheduled using torch_scheduler_kwargs
    # weight_decay: 1.e-7
  torch_scheduler_kwargs:
    module_type: WarmUpLinearLR
    max_num_epochs: ${constants.max_epochs}
    warmup_epochs: 10
    verbose: False
  scheduler_kwargs: null
  target_nan_mask: null
  multitask_handling: flatten # flatten, mean-per-label

trainer:
  seed: ${constants.seed}
  model_checkpoint:
    filename: ${constants.name}
    save_last: True
  trainer:
    precision: 32
    max_epochs: ${constants.max_epochs}
    min_epochs: 1
    check_val_every_n_epoch: 10