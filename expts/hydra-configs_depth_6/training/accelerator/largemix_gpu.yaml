# @package _global_

accelerator:
  float32_matmul_precision: medium

datamodule:
  args:
    batch_size_training: 960
    batch_size_inference: 960
    featurization_n_jobs: 6
    num_workers: 6

predictor:
  metrics_every_n_train_steps: 1000
  torch_scheduler_kwargs:
    max_num_epochs: ${constants.max_epochs}

trainer:
  trainer:
    precision: 16-mixed
    # accumulate_grad_batches: 2
    max_epochs: ${constants.max_epochs}