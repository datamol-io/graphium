# GPS++ model with the PCQMv2 dataset on IPU.
constants:
  name: &name pcqm4mv2_gpspp_4layer
  seed: &seed 42
  raise_train_error: true   # Whether the code should raise an error if it crashes during training
  accelerator:
    type: ipu  # cpu or ipu or gpu

datamodule:
  module_type: "MultitaskFromSmilesDataModule"
  # module_type: "FakeDataModule"  # Option to use generated data
  args: # Matches that in the test_multitask_datamodule.py case.
    task_specific_args:   # To be replaced by a new class "DatasetParams"
      homolumo:
        df: null
        task_level: "graph"
        df_path: graphium/data/PCQM4Mv2/pcqm4mv2.csv #graphium/data/PCQM4Mv2/pcqm4mv2.csv
        # wget https://storage.googleapis.com/datasets-public-research/PCQM4M/cxsmiles/pcqm4mv2.csv
        # or set path as https://storage.googleapis.com/datasets-public-research/PCQM4M/cxsmiles/pcqm4mv2.csv directly
        smiles_col: "cxsmiles"
        label_cols: ["homo_lumo_gap"]
        # sample_size: 80000 # use sample_size for test
        splits_path: graphium/data/PCQM4Mv2/split_dict_v2.pt  # Download with `wget https://storage.googleapis.com/datasets-public-research/PCQM4M/cxsmiles/split_dict_v2.pt`
        # graphium/data/PCQM4Mv2/split_dict.pt
        # graphium/data/PCQM4Mv2/pcqm4m_split.csv
        split_names: ["train", "valid", "test-dev"]
        label_normalization:
          method: "normal"
          min_clipping: 0
          max_clipping: 50

    # Featurization
    prepare_dict_or_graph: pyg:graph
    featurization_n_jobs: 20
    featurization_progress: True
    featurization_backend: "loky"
    processed_graph_data_path: "/tmp/graphium_data/PCQM4Mv2/"
    featurization:
    # OGB: ['atomic_num', 'degree', 'possible_formal_charge', 'possible_numH' (total-valence),
    # 'possible_number_radical_e', 'possible_is_aromatic', 'possible_is_in_ring',
    # 'num_chiral_centers (not included yet)']
      atom_property_list_onehot: [atomic-number, group, period, total-valence]
      atom_property_list_float: [degree, formal-charge, radical-electron, aromatic, in-ring]
      conformer_property_list: [positions_3d] # 3D_bias
      # OGB: ['possible_bond_type', 'possible_bond_stereo', 'possible_is_in_ring']
      edge_property_list: [bond-type-onehot, stereo, in-ring]
      add_self_loop: False
      explicit_H: False # if H is included
      use_bonds_weights: False
      pos_encoding_as_features: # encoder dropout 0.18
        pos_types:
          la_pos: &pos_enc
            pos_type: laplacian_eigvec_eigval #laplacian_eigvec
            num_pos: 8
            normalization: "none" # nomrlization already applied on the eigen vectors
            disconnected_comp: True # if eigen values/vector for disconnected graph are included
          rw_pos: # use same name as pe_encoder
            pos_type: rwse
            ksteps: 16


    # Data handling-related
    batch_size_training: 16
    batch_size_inference: 16
    # cache_data_path: .
    num_workers: 20 # -1 to use all
    persistent_workers: False # if use persistent worker at the start of each epoch.
    # Using persistent_workers false might make the start of each epoch very long.
    featurization_backend: "loky"

    ipu_dataloader_training_opts:
      mode: async
      max_num_nodes_per_graph: 20 # train max nodes: 20, max_edges: 54
      max_num_edges_per_graph: 60

    ipu_dataloader_inference_opts:
      mode: async
      max_num_nodes_per_graph: 16 # valid max nodes: 51, max_edges: 118
      max_num_edges_per_graph: 120
      # test-dev max nodes: 50, max_edges: 116
      # test-challenge max nodes: 51, max_edges: 106

architecture:
  model_type: FullGraphMultiTaskNetwork
  mup_base_path: null
  pre_nn:   # Set as null to avoid a pre-nn network
    out_dim: 256
    hidden_dims: 1024
    depth: 2
    activation: relu
    last_activation: none
    dropout: &dropout 0.18
    normalization: &normalization layer_norm
    last_normalization: *normalization
    residual_type: none

  pre_nn_edges:   # Set as null to avoid a pre-nn network
    out_dim: 128
    hidden_dims: 512
    depth: 2
    activation: relu
    last_activation: none
    dropout: *dropout
    normalization: *normalization
    last_normalization: *normalization
    residual_type: none

  pe_encoders:
    out_dim: 32
    pool: "sum" #"mean" "max"
    last_norm: None #"batch_norm", "layer_norm"
    encoders: #la_pos |  rw_pos
      la_pos:  # Set as null to avoid a pre-nn network
        encoder_type: "laplacian_pe"
        input_keys: ["eigvecs", "eigvals"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 32
        model_type: 'DeepSet' #'Transformer' or 'DeepSet'
        num_layers: 2
        num_layers_post: 1 # Num. layers to apply after pooling
        dropout: 0.18
        first_normalization: "none" #"batch_norm" or "layer_norm"
      rw_pos:
        encoder_type: "mlp"
        input_keys: ["rwse"]
        output_keys: ["feat"]
        hidden_dim: 64
        out_dim: 32
        num_layers: 2
        dropout: 0.18
        normalization: "layer_norm" #"batch_norm" or "layer_norm"
        first_normalization: "layer_norm" #"batch_norm" or "layer_norm"
      gaussian_pos: # 3D_bias
        encoder_type: "gaussian_kernel"
        input_keys: ["positions_3d"]
        output_keys: ["feat", "graph_gaussian_bias_3d"]
        num_heads: 32
        num_layers: 1 #2
        embed_dim: 32
        out_dim: 32 # need num of gaussian kernels 128
        # but currently it checks pe_out_dim == pe_out_dim in encoder_manager.py, line 128
        use_input_keys_prefix: False


  gnn:  # Set as null to avoid a post-nn network
    out_dim: 256
    hidden_dims: 256
    depth: 4
    activation: gelu
    last_activation: none
    dropout: 0.1
    normalization: "layer_norm"
    last_normalization: *normalization
    residual_type: simple
    pooling: [sum]
    virtual_node: 'none'
    layer_type: 'pyg:gps' #pyg:gine #'pyg:gps' # pyg:gated-gcn, pyg:gine,pyg:gps
    layer_kwargs:  # Parameters for the model itself. You could define dropout_attn: 0.1
      node_residual: false
      mpnn_type: 'pyg:mpnnplus'
      mpnn_kwargs:
        in_dim: 256
        out_dim: 256
        in_dim_edges: 128
        out_dim_edges: 128
      attn_type: "full-attention" # "full-attention", "none"
      precision: &precision 16
      biased_attention_key: "graph_gaussian_bias_3d" # 3D_bias
      attn_kwargs:
        num_heads: 32
      droppath_rate_attn: 0.0
      droppath_rate_ffn: 0.0


  post_nn: null

  task_heads:
    homolumo:
      out_dim: 1
      hidden_dims: 256
      depth: 2                          # Not needed if we have hidden_dims
      activation: relu
      last_activation: none
      dropout: *dropout
      normalization: *normalization
      last_normalization: "none"
      residual_type: none

#Task-specific
predictor:
  metrics_on_progress_bar:
    homolumo: ["mae", "pearsonr"]
  loss_fun:
    homolumo: mae_ipu
  random_seed: *seed
  optim_kwargs:
    lr: 4.e-4 # warmup can be scheduled using torch_scheduler_kwargs
    # weight_decay: 1.e-7
    # loss_scaling: 1024
  torch_scheduler_kwargs:
    module_type: WarmUpLinearLR
    max_num_epochs: &max_epochs 100
    warmup_epochs: 10
    verbose: False
  scheduler_kwargs:
  #  monitor: &monitor homolumo/mae/train
  #  mode: min
  #  frequency: 1
  target_nan_mask: null # null: no mask, 0: 0 mask, ignore: ignore nan values from loss
  flag_kwargs:
    n_steps: 0 # 1
    alpha: 0.0 # 0.01

# Task-specific
metrics:
  homolumo:
    - name: mae
      metric: mae_ipu
      target_nan_mask: null
      multitask_handling: flatten
      threshold_kwargs: null
    - name: pearsonr
      metric: pearsonr_ipu
      threshold_kwargs: null
      target_nan_mask: null
      multitask_handling: mean-per-label
