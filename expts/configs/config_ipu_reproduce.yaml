constants:
  name: &name ipu-reproduce
  seed: &seed 42
  raise_train_error: true   # Whether the code should raise an error if it crashes during training
  accelerator:
    type: ipu


datamodule:
  module_type: "MultitaskIPUFromSmilesDataModule"
  args: # Matches that in the test_multitask_datamodule.py case.
    task_specific_args:   # To be replaced by a new class "DatasetParams"
      pc-assays:
        df_path:  goli/data/mega-pubchem/mini.csv.gz
        # df_path: goli/data/mega-pubchem/mega-pubchem-L1000-VCAP.csv.gz
        # df_path: gs://goli-private/datasets/goli-pubchem-L1000/mega-pubchem-L1000-VCAP.csv.gz
        smiles_col: "SMILES"
        label_cols: "assayID-*"
        split_val: 0.08
        split_test: 0.
        split_seed: *seed
        splits_path: null                 # This may not always be provided
        sample_size: 20000 # 715233       # This may not always be provided
        idx_col: null                     # This may not always be provided
        weights_col: null                 # This may not always be provided
        weights_type: null                # This may not always be provided

    # Featurization
    prepare_dict_or_graph: pyg:graph
    featurization_n_jobs: -1
    featurization_progress: True
    featurization:
      atom_property_list_onehot: [atomic-number] #[atomic-number, valence]
      atom_property_list_float: [mass] # [mass, electronegativity, in-ring, hybridization, chirality, aromatic, degree, formal-charge, single-bond, double-bond, radical-electron, vdw-radius, covalent-radius, metal]
      edge_property_list: [bond-type-float] # [bond-type-onehot, bond-type-float, stereo, in-ring, conjugated, estimated-bond-length]
      add_self_loop: False
      explicit_H: False
      use_bonds_weights: False
      max_num_atoms: 100
      # pos_encoding_as_features:
      #   pos_types:
      #     la_pos: &pos_enc  #use same name as pe_encoder
      #       pos_type: laplacian_eigvec_eigval #laplacian_eigvec
      #       num_pos: 3
      #       normalization: "none"
      #       disconnected_comp: True
      #     rw_pos: #use same name as pe_encoder
      #       pos_type: rwse
      #       ksteps: 16
      # pos_encoding_as_directions: *pos_enc # Only for DGN or directional pooling

    # Data handling-related
    batch_size_train_val: 10
    batch_size_test: 10
    # cache_data_path: null

    num_workers: 0
    persistent_workers: False

    ipu_dataloader_training_opts:
      mode: async
      max_num_nodes: &max_num_nodes 325
      max_num_edges: &max_num_edges 650

    ipu_dataloader_inference_opts:
      mode: async
      max_num_nodes: *max_num_nodes
      max_num_edges: *max_num_edges

architecture:
  model_type: FullGraphMultiTaskNetwork
  pre_nn:   # Set as null to avoid a pre-nn network
    out_dim: &hidden_dim 672
    hidden_dims: *hidden_dim
    depth: 3
    activation: relu
    last_activation: none
    dropout: &dropout_mlp 0.1
    last_dropout: *dropout_mlp
    normalization: &normalization batch_norm
    last_normalization: *normalization
    residual_type: simple

  pre_nn_edges:   # Set as null to avoid a pre-nn network
    out_dim: 32
    hidden_dims: 32
    depth: 2
    activation: relu
    last_activation: none
    dropout: *dropout_mlp
    last_dropout: *dropout_mlp
    normalization: *normalization
    last_normalization: *normalization
    residual_type: simple

  # pe_encoders:
  #   out_dim: 64
  #   pool: "sum" #"mean" "max"
  #   last_norm: None #"batch_norm", "layer_norm"
  #   encoders: #la_pos |  rw_pos
  #     la_pos:  # Set as null to avoid a pre-nn network
  #       encoder_type: "laplacian_pe"
  #       on_keys: ["eigvecs", "eigvals"]
  #       hidden_dim: 64
  #       model_type: 'DeepSet' #'Transformer' or 'DeepSet'
  #       num_layers: 1
  #       num_layers_post: 0 # Num. layers to apply after pooling
  #       dropout: 0.1
  #       first_normalization: "none" #"batch_norm" or "layer_norm"
  #     rw_pos:
  #       encoder_type: "mlp"
  #       on_keys: ["rwse"]
  #       out_level: "node"
  #       hidden_dim: 64
  #       num_layers: 1
  #       dropout: 0.1
  #       normalization: "none" #"batch_norm" or "layer_norm"
  #       first_normalization: "none" #"batch_norm" or "layer_norm"

  gnn:  # Set as null to avoid a post-nn network
    out_dim: &out_mlp_dim 1200
    hidden_dims: *hidden_dim
    depth: 8
    activation: relu
    last_activation: none
    dropout: &dropout_gnn 0.07
    last_dropout: *dropout_gnn
    normalization: *normalization
    last_normalization: *normalization
    residual_type: simple
    pooling: ['logsum', 'max', 'mean']
    # pooling: ['mean']
    virtual_node: 'none'
    layer_type: "pyg:pna-msgpass" # 'pyg:gine'
    layer_kwargs:
      aggregators: [mean, max, sum]
      scalers: [identity]

  post_nn:
    out_dim: *out_mlp_dim
    hidden_dims: *out_mlp_dim
    depth: 2
    activation: relu
    last_activation: relu
    dropout: *dropout_mlp
    last_dropout: 0.
    normalization: *normalization
    last_normalization: "none"
    residual_type: simple


  task_heads:
    - task_name: "pc-assays"
      out_dim: 561 # 2306
      hidden_dims: *out_mlp_dim
      depth: 1                          # Not needed if we have hidden_dims
      activation: relu
      last_activation: sigmoid
      dropout: *dropout_mlp
      normalization: *normalization
      last_normalization: "none"
      residual_type: none

#Task-specific
predictor:
  metrics_on_progress_bar:
    pc-assays: [] # ["mae", "averageprecision", "auroc"]
  metrics_on_training_set:
    pc-assays: [] # ["mae", "averageprecision", "auroc"]
  loss_fun:
    pc-assays: bce_ipu
  random_seed: *seed
  optim_kwargs:
    lr: 2.e-4
    # weight_decay: 1.e-7
  torch_scheduler_kwargs:
    #module_type: ReduceLROnPlateau
    #factor: 0.5
    #patience: 7
  scheduler_kwargs: null
  #  monitor: &monitor loss/val
  #  mode: min
  #  frequency: 1
  target_nan_mask: null # null: no mask, 0: 0 mask, ignore-flatten, ignore-mean-label
  flag_kwargs:
    n_steps: 0 #1
    alpha: 0.0 #0.01


metrics:
  pc-assays:
    - name: mae
      metric: mae
      threshold_kwargs: null
      target_nan_mask: 0

    - name: averageprecision
      metric: averageprecision
      target_nan_mask: 0
      threshold_kwargs:
        threshold: null
        th_on_preds: False
        th_on_target: False
        target_to_int: True

    - name: auroc
      metric: auroc
      target_nan_mask: 0
      threshold_kwargs:
        threshold: null
        th_on_preds: False
        th_on_target: False
        target_to_int: True

    # - name: f1 > 0.5
    #   metric: f1
    #   num_classes: 2
    #   average: macro
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: &threshold_1
    #     operator: greater
    #     threshold: 0.5
    #     th_on_preds: True
    #     th_on_target: False
    #     target_to_int: True

    # - name: f1 > 0.25
    #   metric: f1
    #   num_classes: 2
    #   average: macro
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: &threshold_2
    #     operator: greater
    #     threshold: 0.25
    #     th_on_preds: True
    #     th_on_target: False
    #     target_to_int: True

    # - name: f1 > 0.75
    #   metric: f1
    #   num_classes: 2
    #   average: macro
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: &threshold_3
    #     operator: greater
    #     threshold: 0.75
    #     th_on_preds: True
    #     th_on_target: False
    #     target_to_int: True

    # - name: recall > 0.5
    #   metric: recall
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: *threshold_1

    # - name: recall > 0.25
    #   metric: recall
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: *threshold_2

    # - name: recall > 0.75
    #   metric: recall
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: *threshold_3

    # - name: precision > 0.5
    #   metric: precision
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: *threshold_1

    # - name: precision > 0.25
    #   metric: precision
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: *threshold_2

    # - name: precision > 0.25
    #   metric: precision
    #   target_nan_mask: ignore-mean-label
    #   threshold_kwargs: *threshold_3

trainer:
  logger:
    save_dir: logs/ipu-reproduce
    name: *name
  #early_stopping:
  #  monitor: *monitor
  #  min_delta: 0
  #  patience: 10
  #  mode: &mode min
  model_checkpoint:
    dirpath: models_checkpoints/ipu-reproduce/
    filename: *name
    #monitor: *monitor
    #mode: *mode
    save_top_k: 1
    every_n_epochs: 1
  trainer:
    precision: 16
    max_epochs: 1000
    min_epochs: 100
